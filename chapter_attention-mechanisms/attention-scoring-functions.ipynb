{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cdfe535",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 注意力评分函数\n",
    ":label:`sec_attention-scoring-functions`\n",
    "\n",
    " :numref:`sec_nadaraya-watson`使用了高斯核来对查询和键之间的关系建模。\n",
    " :eqref:`eq_nadaraya-watson-gaussian`中的\n",
    "高斯核指数部分可以视为*注意力评分函数*（attention scoring function），\n",
    "简称*评分函数*（scoring function），\n",
    "然后把这个函数的输出结果输入到softmax函数中进行运算。\n",
    "通过上述步骤，将得到与键对应的值的概率分布（即注意力权重）。\n",
    "最后，注意力汇聚的输出就是基于这些注意力权重的值的加权和。\n",
    "\n",
    "从宏观来看，上述算法可以用来实现\n",
    " :numref:`fig_qkv`中的注意力机制框架。\n",
    " :numref:`fig_attention_output`说明了\n",
    "如何将注意力汇聚的输出计算成为值的加权和，\n",
    "其中$a$表示注意力评分函数。\n",
    "由于注意力权重是概率分布，\n",
    "因此加权和其本质上是加权平均值。\n",
    "\n",
    "![计算注意力汇聚的输出为值的加权和](../img/attention-output.svg)\n",
    ":label:`fig_attention_output`\n",
    "\n",
    "用数学语言描述，假设有一个查询\n",
    "$\\mathbf{q} \\in \\mathbb{R}^q$和\n",
    "$m$个“键－值”对\n",
    "$(\\mathbf{k}_1, \\mathbf{v}_1), \\ldots, (\\mathbf{k}_m, \\mathbf{v}_m)$，\n",
    "其中$\\mathbf{k}_i \\in \\mathbb{R}^k$，$\\mathbf{v}_i \\in \\mathbb{R}^v$。\n",
    "注意力汇聚函数$f$就被表示成值的加权和：\n",
    "\n",
    "$$f(\\mathbf{q}, (\\mathbf{k}_1, \\mathbf{v}_1), \\ldots, (\\mathbf{k}_m, \\mathbf{v}_m)) = \\sum_{i=1}^m \\alpha(\\mathbf{q}, \\mathbf{k}_i) \\mathbf{v}_i \\in \\mathbb{R}^v,$$\n",
    ":eqlabel:`eq_attn-pooling`\n",
    "\n",
    "其中查询$\\mathbf{q}$和键$\\mathbf{k}_i$的注意力权重（标量）\n",
    "是通过注意力评分函数$a$将两个向量映射成标量，\n",
    "再经过softmax运算得到的：\n",
    "\n",
    "$$\\alpha(\\mathbf{q}, \\mathbf{k}_i) = \\mathrm{softmax}(a(\\mathbf{q}, \\mathbf{k}_i)) = \\frac{\\exp(a(\\mathbf{q}, \\mathbf{k}_i))}{\\sum_{j=1}^m \\exp(a(\\mathbf{q}, \\mathbf{k}_j))} \\in \\mathbb{R}.$$\n",
    ":eqlabel:`eq_attn-scoring-alpha`\n",
    "\n",
    "正如上图所示，选择不同的注意力评分函数$a$会导致不同的注意力汇聚操作。\n",
    "本节将介绍两个流行的评分函数，稍后将用他们来实现更复杂的注意力机制。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77bfe8f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:58:57.046810Z",
     "iopub.status.busy": "2023-08-18T06:58:57.045981Z",
     "iopub.status.idle": "2023-08-18T06:59:00.174680Z",
     "shell.execute_reply": "2023-08-18T06:59:00.173514Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "# from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937acec4",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "## [**掩蔽softmax操作**]\n",
    "\n",
    "正如上面提到的，softmax操作用于输出一个概率分布作为注意力权重。\n",
    "在某些情况下，并非所有的值都应该被纳入到注意力汇聚中。\n",
    "例如，为了在 :numref:`sec_machine_translation`中高效处理小批量数据集，\n",
    "某些文本序列被填充了没有意义的特殊词元。\n",
    "为了仅将有意义的词元作为值来获取注意力汇聚，\n",
    "可以指定一个有效序列长度（即词元的个数），\n",
    "以便在计算softmax时过滤掉超出指定范围的位置。\n",
    "下面的`masked_softmax`函数\n",
    "实现了这样的*掩蔽softmax操作*（masked softmax operation），\n",
    "其中任何超出有效长度的位置都被掩蔽并置为0。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e109644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#@save\n",
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"在序列中屏蔽不相关的项\"\"\"\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                        device=X.device)[None, :] < valid_len[:, None]\n",
    "    X[~mask] = value\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3be54330",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:59:00.180834Z",
     "iopub.status.busy": "2023-08-18T06:59:00.179926Z",
     "iopub.status.idle": "2023-08-18T06:59:00.189306Z",
     "shell.execute_reply": "2023-08-18T06:59:00.188185Z"
    },
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def masked_softmax(X, valid_lens):\n",
    "    \"\"\"通过在最后一个轴上掩蔽元素来执行softmax操作\"\"\"\n",
    "    # X:3D张量，valid_lens:1D或2D张量\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        # 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0\n",
    "        X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens,\n",
    "                              value=-1e6)\n",
    "        return nn.functional.softmax(X.reshape(shape), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8ddc66",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "为了[**演示此函数是如何工作**]的，\n",
    "考虑由两个$2 \\times 4$矩阵表示的样本，\n",
    "这两个样本的有效长度分别为$2$和$3$。\n",
    "经过掩蔽softmax操作，超出有效长度的值都被掩蔽为0。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f3b0b7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:59:00.194266Z",
     "iopub.status.busy": "2023-08-18T06:59:00.193661Z",
     "iopub.status.idle": "2023-08-18T06:59:00.238157Z",
     "shell.execute_reply": "2023-08-18T06:59:00.237124Z"
    },
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6821, 0.3179, 0.0000, 0.0000],\n",
       "         [0.6656, 0.3344, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.3406, 0.2927, 0.3667, 0.0000],\n",
       "         [0.2336, 0.4594, 0.3070, 0.0000]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7be477",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "同样，也可以使用二维张量，为矩阵样本中的每一行指定有效长度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0296eee3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:59:00.245983Z",
     "iopub.status.busy": "2023-08-18T06:59:00.244094Z",
     "iopub.status.idle": "2023-08-18T06:59:00.257237Z",
     "shell.execute_reply": "2023-08-18T06:59:00.256163Z"
    },
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2665, 0.2904, 0.4431, 0.0000]],\n",
       "\n",
       "        [[0.5846, 0.4154, 0.0000, 0.0000],\n",
       "         [0.2042, 0.3795, 0.2300, 0.1863]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_softmax(torch.rand(2, 2, 4), torch.tensor([[1, 3], [2, 4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f184f421",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "## [**加性注意力**]\n",
    ":label:`subsec_additive-attention`\n",
    "\n",
    "一般来说，当查询和键是不同长度的矢量时，可以使用加性注意力作为评分函数。\n",
    "给定查询$\\mathbf{q} \\in \\mathbb{R}^q$和\n",
    "键$\\mathbf{k} \\in \\mathbb{R}^k$，\n",
    "*加性注意力*（additive attention）的评分函数为\n",
    "\n",
    "$$a(\\mathbf q, \\mathbf k) = \\mathbf w_v^\\top \\text{tanh}(\\mathbf W_q\\mathbf q + \\mathbf W_k \\mathbf k) \\in \\mathbb{R},$$\n",
    ":eqlabel:`eq_additive-attn`\n",
    "\n",
    "其中可学习的参数是$\\mathbf W_q\\in\\mathbb R^{h\\times q}$、\n",
    "$\\mathbf W_k\\in\\mathbb R^{h\\times k}$和\n",
    "$\\mathbf w_v\\in\\mathbb R^{h}$。\n",
    "如 :eqref:`eq_additive-attn`所示，\n",
    "将查询和键连结起来后输入到一个多层感知机（MLP）中，\n",
    "感知机包含一个隐藏层，其隐藏单元数是一个超参数$h$。\n",
    "通过使用$\\tanh$作为激活函数，并且禁用偏置项。\n",
    "\n",
    "下面来实现加性注意力。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c67a2f61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:59:00.265207Z",
     "iopub.status.busy": "2023-08-18T06:59:00.263273Z",
     "iopub.status.idle": "2023-08-18T06:59:00.277358Z",
     "shell.execute_reply": "2023-08-18T06:59:00.276229Z"
    },
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"加性注意力\"\"\"\n",
    "    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):\n",
    "        super(AdditiveAttention, self).__init__(**kwargs)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)\n",
    "        self.w_v = nn.Linear(num_hiddens, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
    "        # 在维度扩展后，\n",
    "        # queries的形状：(batch_size，查询的个数，1，num_hidden)\n",
    "        # key的形状：(batch_size，1，“键－值”对的个数，num_hiddens)\n",
    "        # 使用广播方式进行求和\n",
    "        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n",
    "        features = torch.tanh(features)\n",
    "        # self.w_v仅有一个输出，因此从形状中移除最后那个维度。\n",
    "        # scores的形状：(batch_size，查询的个数，“键-值”对的个数)\n",
    "        scores = self.w_v(features).squeeze(-1)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        # values的形状：(batch_size，“键－值”对的个数，值的维度)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c134134a",
   "metadata": {
    "origin_pos": 25
   },
   "source": [
    "用一个小例子来[**演示上面的`AdditiveAttention`类**]，\n",
    "其中查询、键和值的形状为（批量大小，步数或词元序列长度，特征大小），\n",
    "实际输出为$(2,1,20)$、$(2,10,2)$和$(2,10,4)$。\n",
    "注意力汇聚输出的形状为（批量大小，查询的步数，值的维度）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "764a05d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:59:00.284992Z",
     "iopub.status.busy": "2023-08-18T06:59:00.283120Z",
     "iopub.status.idle": "2023-08-18T06:59:00.309249Z",
     "shell.execute_reply": "2023-08-18T06:59:00.308150Z"
    },
    "origin_pos": 27,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
       "\n",
       "        [[10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=<BmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries, keys = torch.normal(0, 1, (2, 1, 20)), torch.ones((2, 10, 2))\n",
    "# values的小批量，两个值矩阵是相同的\n",
    "values = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat(\n",
    "    2, 1, 1)\n",
    "valid_lens = torch.tensor([2, 6])\n",
    "\n",
    "attention = AdditiveAttention(key_size=2, query_size=20, num_hiddens=8,\n",
    "                              dropout=0.1)\n",
    "attention.eval()\n",
    "attention(queries, keys, values, valid_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8ed50",
   "metadata": {
    "origin_pos": 30
   },
   "source": [
    "尽管加性注意力包含了可学习的参数，但由于本例子中每个键都是相同的，\n",
    "所以[**注意力权重**]是均匀的，由指定的有效长度决定。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4995c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5),\n",
    "                  cmap='Reds'):\n",
    "    \"\"\"显示矩阵热图\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # 设置高分辨率显示\n",
    "    import matplotlib\n",
    "    matplotlib.rcParams['figure.dpi'] = 100\n",
    "    matplotlib.rcParams['savefig.dpi'] = 100\n",
    "    \n",
    "    num_rows, num_cols = matrices.shape[0], matrices.shape[1]\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize,\n",
    "                             sharex=True, sharey=True, squeeze=False)\n",
    "    \n",
    "    for i, (row_axes, row_matrices) in enumerate(zip(axes, matrices)):\n",
    "        for j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)):\n",
    "            pcm = ax.imshow(matrix.detach().numpy(), cmap=cmap)\n",
    "            if i == num_rows - 1:\n",
    "                ax.set_xlabel(xlabel)\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(ylabel)\n",
    "            if titles:\n",
    "                ax.set_title(titles[j])\n",
    "    \n",
    "    fig.colorbar(pcm, ax=axes, shrink=0.6)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cc35bc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:59:00.316794Z",
     "iopub.status.busy": "2023-08-18T06:59:00.315016Z",
     "iopub.status.idle": "2023-08-18T06:59:00.531804Z",
     "shell.execute_reply": "2023-08-18T06:59:00.530673Z"
    },
    "origin_pos": 31,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 250x250 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)),\n",
    "                  xlabel='Keys', ylabel='Queries')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efae8fbc",
   "metadata": {
    "origin_pos": 32
   },
   "source": [
    "## [**缩放点积注意力**]\n",
    "\n",
    "使用点积可以得到计算效率更高的评分函数，\n",
    "但是点积操作要求查询和键具有相同的长度$d$。\n",
    "假设查询和键的所有元素都是独立的随机变量，\n",
    "并且都满足零均值和单位方差，\n",
    "那么两个向量的点积的均值为$0$，方差为$d$。\n",
    "为确保无论向量长度如何，\n",
    "点积的方差在不考虑向量长度的情况下仍然是$1$，\n",
    "我们再将点积除以$\\sqrt{d}$，\n",
    "则*缩放点积注意力*（scaled dot-product attention）评分函数为：\n",
    "\n",
    "$$a(\\mathbf q, \\mathbf k) = \\mathbf{q}^\\top \\mathbf{k}  /\\sqrt{d}.$$\n",
    "\n",
    "在实践中，我们通常从小批量的角度来考虑提高效率，\n",
    "例如基于$n$个查询和$m$个键－值对计算注意力，\n",
    "其中查询和键的长度为$d$，值的长度为$v$。\n",
    "查询$\\mathbf Q\\in\\mathbb R^{n\\times d}$、\n",
    "键$\\mathbf K\\in\\mathbb R^{m\\times d}$和\n",
    "值$\\mathbf V\\in\\mathbb R^{m\\times v}$的缩放点积注意力是：\n",
    "\n",
    "$$ \\mathrm{softmax}\\left(\\frac{\\mathbf Q \\mathbf K^\\top }{\\sqrt{d}}\\right) \\mathbf V \\in \\mathbb{R}^{n\\times v}.$$\n",
    ":eqlabel:`eq_softmax_QK_V`\n",
    "\n",
    "下面的缩放点积注意力的实现使用了暂退法进行模型正则化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "974ab3dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:59:00.537439Z",
     "iopub.status.busy": "2023-08-18T06:59:00.536367Z",
     "iopub.status.idle": "2023-08-18T06:59:00.545728Z",
     "shell.execute_reply": "2023-08-18T06:59:00.544563Z"
    },
    "origin_pos": 34,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"缩放点积注意力\"\"\"\n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # queries的形状：(batch_size，查询的个数，d)\n",
    "    # keys的形状：(batch_size，“键－值”对的个数，d)\n",
    "    # values的形状：(batch_size，“键－值”对的个数，值的维度)\n",
    "    # valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        d = queries.shape[-1]\n",
    "        # 设置transpose_b=True为了交换keys的最后两个维度\n",
    "        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bb92bb",
   "metadata": {
    "origin_pos": 37
   },
   "source": [
    "为了[**演示上述的`DotProductAttention`类**]，\n",
    "我们使用与先前加性注意力例子中相同的键、值和有效长度。\n",
    "对于点积操作，我们令查询的特征维度与键的特征维度大小相同。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4eb4277",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:59:00.550788Z",
     "iopub.status.busy": "2023-08-18T06:59:00.549812Z",
     "iopub.status.idle": "2023-08-18T06:59:00.562245Z",
     "shell.execute_reply": "2023-08-18T06:59:00.561174Z"
    },
    "origin_pos": 39,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
       "\n",
       "        [[10.0000, 11.0000, 12.0000, 13.0000]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = torch.normal(0, 1, (2, 1, 2))\n",
    "attention = DotProductAttention(dropout=0.5)\n",
    "attention.eval()\n",
    "attention(queries, keys, values, valid_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b6a04e",
   "metadata": {
    "origin_pos": 42
   },
   "source": [
    "与加性注意力演示相同，由于键包含的是相同的元素，\n",
    "而这些元素无法通过任何查询进行区分，因此获得了[**均匀的注意力权重**]。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76040da6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:59:00.567093Z",
     "iopub.status.busy": "2023-08-18T06:59:00.566474Z",
     "iopub.status.idle": "2023-08-18T06:59:00.804899Z",
     "shell.execute_reply": "2023-08-18T06:59:00.803678Z"
    },
    "origin_pos": 43,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 250x250 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)),\n",
    "                  xlabel='Keys', ylabel='Queries')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911f8588",
   "metadata": {
    "origin_pos": 44
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 将注意力汇聚的输出计算可以作为值的加权平均，选择不同的注意力评分函数会带来不同的注意力汇聚操作。\n",
    "* 当查询和键是不同长度的矢量时，可以使用可加性注意力评分函数。当它们的长度相同时，使用缩放的“点－积”注意力评分函数的计算效率更高。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 修改小例子中的键，并且可视化注意力权重。可加性注意力和缩放的“点－积”注意力是否仍然产生相同的结果？为什么？\n",
    "1. 只使用矩阵乘法，能否为具有不同矢量长度的查询和键设计新的评分函数？\n",
    "1. 当查询和键具有相同的矢量长度时，矢量求和作为评分函数是否比“点－积”更好？为什么？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8affdbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "修改后的键矩阵示例:\n",
      "Keys shape: torch.Size([2, 10, 2])\n",
      "Keys sample:\n",
      " tensor([[ 0.4266,  0.3636],\n",
      "        [-1.1653, -1.1240],\n",
      "        [ 1.0533,  1.7060],\n",
      "        [-0.7358,  0.3239],\n",
      "        [-1.8131,  0.7535],\n",
      "        [ 0.0593, -0.3463],\n",
      "        [ 0.5549,  0.8203],\n",
      "        [-0.5514,  0.0579],\n",
      "        [-0.5009, -0.1603],\n",
      "        [ 0.6995, -1.1508]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [2, 20] but got: [2, 2].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     76\u001b[39m attention_dot = DotProductAttention(dropout=\u001b[32m0.1\u001b[39m)\n\u001b[32m     77\u001b[39m attention_dot.eval()\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m output_dot = \u001b[43mattention_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys_modified\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m加性注意力输出:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     81\u001b[39m \u001b[38;5;28mprint\u001b[39m(output_add)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\d2l-zh\\pytorch\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\d2l-zh\\pytorch\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mDotProductAttention.forward\u001b[39m\u001b[34m(self, queries, keys, values, valid_lens)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, queries, keys, values, valid_lens=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     56\u001b[39m     d = queries.shape[-\u001b[32m1\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     scores = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m / math.sqrt(d)\n\u001b[32m     58\u001b[39m     \u001b[38;5;28mself\u001b[39m.attention_weights = masked_softmax(scores, valid_lens)\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.bmm(\u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28mself\u001b[39m.attention_weights), values)\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected size for first two dimensions of batch2 tensor to be: [2, 20] but got: [2, 2]."
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 定义必要的函数和类\n",
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"在序列中屏蔽不相关的项\"\"\"\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                        device=X.device)[None, :] < valid_len[:, None]\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "def masked_softmax(X, valid_lens):\n",
    "    \"\"\"通过在最后一个轴上掩蔽元素来执行softmax操作\"\"\"\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens,\n",
    "                          value=-1e6)\n",
    "        return nn.functional.softmax(X.reshape(shape), dim=-1)\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"加性注意力\"\"\"\n",
    "    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):\n",
    "        super(AdditiveAttention, self).__init__(**kwargs)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)\n",
    "        self.w_v = nn.Linear(num_hiddens, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
    "        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n",
    "        features = torch.tanh(features)\n",
    "        scores = self.w_v(features).squeeze(-1)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
    "\n",
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"缩放点积注意力\"\"\"\n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        d = queries.shape[-1]\n",
    "        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
    "\n",
    "# 创建修改后的键矩阵（使用不同的值而不是全1）\n",
    "torch.manual_seed(42)  # 设置随机种子以确保结果可重现\n",
    "queries, keys_original = torch.normal(0, 1, (2, 1, 20)), torch.ones((2, 10, 2))\n",
    "# 修改键矩阵，使用随机值而不是全1\n",
    "keys_modified = torch.normal(0, 1, (2, 10, 2))\n",
    "values = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat(2, 1, 1)\n",
    "valid_lens = torch.tensor([2, 6])\n",
    "\n",
    "print(\"原始键矩阵示例 (全为1):\")\n",
    "print(\"Keys shape:\", keys_original.shape)\n",
    "print(\"Keys sample:\\n\", keys_original[0])\n",
    "\n",
    "print(\"\\n修改后的键矩阵示例 (随机值):\")\n",
    "print(\"Keys shape:\", keys_modified.shape)\n",
    "print(\"Keys sample:\\n\", keys_modified[0])\n",
    "\n",
    "# 为了点积注意力，我们需要查询和键具有相同的维度\n",
    "# 重新生成查询，使其与键的维度匹配\n",
    "queries_dot = torch.normal(0, 1, (2, 1, 2))  # 与键的维度匹配\n",
    "\n",
    "# 使用加性注意力 (可以处理不同维度的查询和键)\n",
    "attention_add = AdditiveAttention(key_size=2, query_size=20, num_hiddens=8, dropout=0.1)\n",
    "attention_add.eval()\n",
    "output_add = attention_add(queries, keys_modified, values, valid_lens)\n",
    "\n",
    "# 使用缩放点积注意力 (需要查询和键具有相同维度)\n",
    "attention_dot = DotProductAttention(dropout=0.1)\n",
    "attention_dot.eval()\n",
    "output_dot = attention_dot(queries_dot, keys_modified, values, valid_lens)\n",
    "\n",
    "print(\"\\n加性注意力输出:\")\n",
    "print(output_add)\n",
    "print(\"\\n缩放点积注意力输出:\")\n",
    "print(output_dot)\n",
    "\n",
    "# 可视化注意力权重\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 加性注意力权重\n",
    "add_weights = attention_add.attention_weights.reshape(2, 1, 10)\n",
    "im1 = axes[0, 0].imshow(add_weights[0].detach().numpy(), cmap='Reds')\n",
    "axes[0, 0].set_title('Additive Attention Weights (Sample 1)')\n",
    "axes[0, 0].set_xlabel('Keys')\n",
    "axes[0, 0].set_ylabel('Queries')\n",
    "plt.colorbar(im1, ax=axes[0, 0])\n",
    "\n",
    "im2 = axes[0, 1].imshow(add_weights[1].detach().numpy(), cmap='Reds')\n",
    "axes[0, 1].set_title('Additive Attention Weights (Sample 2)')\n",
    "axes[0, 1].set_xlabel('Keys')\n",
    "axes[0, 1].set_ylabel('Queries')\n",
    "plt.colorbar(im2, ax=axes[0, 1])\n",
    "\n",
    "# 缩放点积注意力权重\n",
    "dot_weights = attention_dot.attention_weights.reshape(2, 1, 10)\n",
    "im3 = axes[1, 0].imshow(dot_weights[0].detach().numpy(), cmap='Reds')\n",
    "axes[1, 0].set_title('Dot Product Attention Weights (Sample 1)')\n",
    "axes[1, 0].set_xlabel('Keys')\n",
    "axes[1, 0].set_ylabel('Queries')\n",
    "plt.colorbar(im3, ax=axes[1, 0])\n",
    "\n",
    "im4 = axes[1, 1].imshow(dot_weights[1].detach().numpy(), cmap='Reds')\n",
    "axes[1, 1].set_title('Dot Product Attention Weights (Sample 2)')\n",
    "axes[1, 1].set_xlabel('Keys')\n",
    "axes[1, 1].set_ylabel('Queries')\n",
    "plt.colorbar(im4, ax=axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 打印数值结果进行比较\n",
    "print(\"\\n加性注意力权重 (第一个样本):\")\n",
    "print(add_weights[0].detach().numpy())\n",
    "print(\"\\n缩放点积注意力权重 (第一个样本):\")\n",
    "print(dot_weights[0].detach().numpy())\n",
    "\n",
    "print(\"\\n加性注意力权重 (第二个样本):\")\n",
    "print(add_weights[1].detach().numpy())\n",
    "print(\"\\n缩放点积注意力权重 (第二个样本):\")\n",
    "print(dot_weights[1].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625a83ae",
   "metadata": {
    "origin_pos": 46,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/5752)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
