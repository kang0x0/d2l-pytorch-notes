{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4c46021",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 循环神经网络的简洁实现\n",
    ":label:`sec_rnn-concise`\n",
    "\n",
    "虽然 :numref:`sec_rnn_scratch`\n",
    "对了解循环神经网络的实现方式具有指导意义，但并不方便。\n",
    "本节将展示如何使用深度学习框架的高级API提供的函数更有效地实现相同的语言模型。\n",
    "我们仍然从读取时光机器数据集开始。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1787a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import os\n",
    "import collections\n",
    "import random\n",
    "import torch\n",
    "\n",
    "class Vocab:  #@save\n",
    "    \"\"\"文本词表\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        # 按出现频率排序\n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        # 未知词元的索引为0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # 未知词元的索引为0\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs\n",
    "    \n",
    "def count_corpus(tokens):  #@save\n",
    "    \"\"\"统计词元的频率\"\"\"\n",
    "    # 这里的tokens是1D列表或2D列表\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # 将词元列表展平成一个列表\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)\n",
    "\n",
    "def download_time_machine(data_dir='../data', filename='timemachine.txt'):\n",
    "    \"\"\"下载时间机器文本文件\"\"\"\n",
    "    url = 'http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt'\n",
    "    \n",
    "    # 创建数据目录（如果不存在）\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    # 完整文件路径\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "    \n",
    "    # 如果文件不存在，则下载\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f'Downloading {filename} from {url}...')\n",
    "        response = requests.get(url)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f'File saved to {file_path}')\n",
    "    else:\n",
    "        print(f'File already exists at {file_path}')\n",
    "    \n",
    "    return file_path\n",
    "\n",
    "def read_time_machine():\n",
    "    \"\"\"将时间机器数据集加载到文本行的列表中\"\"\"\n",
    "    # 下载文件并获取路径\n",
    "    file_path = download_time_machine()\n",
    "    \n",
    "    # 读取文件内容\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # 处理每一行：只保留字母，转为小写，去除首尾空格\n",
    "    processed_lines = [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "    \n",
    "    return processed_lines\n",
    "\n",
    "def tokenize(lines, token='word'):  #@save\n",
    "    \"\"\"将文本行拆分为单词或字符词元\"\"\"\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('错误：未知词元类型：' + token)\n",
    "\n",
    "def load_corpus_time_machine(max_tokens=-1):  #@save\n",
    "    \"\"\"返回时光机器数据集的词元索引列表和词表\"\"\"\n",
    "    lines = read_time_machine()\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    vocab = Vocab(tokens)\n",
    "    # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，\n",
    "    # 所以将所有文本行展平到一个列表中\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab\n",
    "\n",
    "def seq_data_iter_random(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"使用随机抽样生成一个小批量子序列\"\"\"\n",
    "    # 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1\n",
    "    corpus = corpus[random.randint(0, num_steps - 1):]\n",
    "    # 减去1，是因为我们需要考虑标签\n",
    "    num_subseqs = (len(corpus) - 1) // num_steps\n",
    "    # 长度为num_steps的子序列的起始索引\n",
    "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
    "    # 在随机抽样的迭代过程中，\n",
    "    # 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻\n",
    "    random.shuffle(initial_indices)\n",
    "\n",
    "    def data(pos):\n",
    "        # 返回从pos位置开始的长度为num_steps的序列\n",
    "        return corpus[pos: pos + num_steps]\n",
    "\n",
    "    num_batches = num_subseqs // batch_size\n",
    "    for i in range(0, batch_size * num_batches, batch_size):\n",
    "        # 在这里，initial_indices包含子序列的随机起始索引\n",
    "        initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
    "        X = [data(j) for j in initial_indices_per_batch]\n",
    "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
    "        yield torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"使用顺序分区生成一个小批量子序列\"\"\"\n",
    "    # 从随机偏移量开始划分序列\n",
    "    offset = random.randint(0, num_steps)\n",
    "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
    "    Xs = torch.tensor(corpus[offset: offset + num_tokens])\n",
    "    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
    "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
    "    num_batches = Xs.shape[1] // num_steps\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\n",
    "        X = Xs[:, i: i + num_steps]\n",
    "        Y = Ys[:, i: i + num_steps]\n",
    "        yield X, Y\n",
    "\n",
    "class SeqDataLoader:  #@save\n",
    "    \"\"\"加载序列数据的迭代器\"\"\"\n",
    "    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
    "        if use_random_iter:\n",
    "            self.data_iter_fn = seq_data_iter_random\n",
    "        else:\n",
    "            self.data_iter_fn = seq_data_iter_sequential\n",
    "        self.corpus, self.vocab = load_corpus_time_machine(max_tokens)\n",
    "        self.batch_size, self.num_steps = batch_size, num_steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)\n",
    "\n",
    "def load_data_time_machine(batch_size, num_steps,  #@save\n",
    "                           use_random_iter=False, max_tokens=10000):\n",
    "    \"\"\"返回时光机器数据集的迭代器和词表\"\"\"\n",
    "    data_iter = SeqDataLoader(\n",
    "        batch_size, num_steps, use_random_iter, max_tokens)\n",
    "    return data_iter, data_iter.vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e38d82e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:22:38.029441Z",
     "iopub.status.busy": "2023-08-18T07:22:38.028754Z",
     "iopub.status.idle": "2023-08-18T07:22:42.082845Z",
     "shell.execute_reply": "2023-08-18T07:22:42.081933Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists at ../data\\timemachine.txt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "# from d2l import torch as d2l\n",
    "\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8deb24",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "## [**定义模型**]\n",
    "\n",
    "高级API提供了循环神经网络的实现。\n",
    "我们构造一个具有256个隐藏单元的单隐藏层的循环神经网络层`rnn_layer`。\n",
    "事实上，我们还没有讨论多层循环神经网络的意义（这将在 :numref:`sec_deep_rnn`中介绍）。\n",
    "现在仅需要将多层理解为一层循环神经网络的输出被用作下一层循环神经网络的输入就足够了。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37dae103",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:22:42.087317Z",
     "iopub.status.busy": "2023-08-18T07:22:42.086622Z",
     "iopub.status.idle": "2023-08-18T07:22:42.117225Z",
     "shell.execute_reply": "2023-08-18T07:22:42.116310Z"
    },
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "num_hiddens = 256\n",
    "rnn_layer = nn.RNN(len(vocab), num_hiddens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e36240",
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "我们(**使用张量来初始化隐状态**)，它的形状是（隐藏层数，批量大小，隐藏单元数）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1922fe18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:22:42.122208Z",
     "iopub.status.busy": "2023-08-18T07:22:42.121722Z",
     "iopub.status.idle": "2023-08-18T07:22:42.128343Z",
     "shell.execute_reply": "2023-08-18T07:22:42.127617Z"
    },
    "origin_pos": 13,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 256])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = torch.zeros((1, batch_size, num_hiddens))\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170be239",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "[**通过一个隐状态和一个输入，我们就可以用更新后的隐状态计算输出。**]\n",
    "需要强调的是，`rnn_layer`的“输出”（`Y`）不涉及输出层的计算：\n",
    "它是指每个时间步的隐状态，这些隐状态可以用作后续输出层的输入。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cecfe4c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:22:42.132231Z",
     "iopub.status.busy": "2023-08-18T07:22:42.131762Z",
     "iopub.status.idle": "2023-08-18T07:22:42.149849Z",
     "shell.execute_reply": "2023-08-18T07:22:42.148795Z"
    },
    "origin_pos": 19,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([35, 32, 256]), torch.Size([1, 32, 256]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(size=(num_steps, batch_size, len(vocab)))\n",
    "Y, state_new = rnn_layer(X, state)\n",
    "Y.shape, state_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c919b48",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "与 :numref:`sec_rnn_scratch`类似，\n",
    "[**我们为一个完整的循环神经网络模型定义了一个`RNNModel`类**]。\n",
    "注意，`rnn_layer`只包含隐藏的循环层，我们还需要创建一个单独的输出层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "300de81f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:22:42.155516Z",
     "iopub.status.busy": "2023-08-18T07:22:42.154604Z",
     "iopub.status.idle": "2023-08-18T07:22:42.171447Z",
     "shell.execute_reply": "2023-08-18T07:22:42.170040Z"
    },
    "origin_pos": 24,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"循环神经网络模型\"\"\"\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1\n",
    "        if not self.rnn.bidirectional:\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "        else:\n",
    "            self.num_directions = 2\n",
    "            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        # 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)\n",
    "        # 它的输出形状是(时间步数*批量大小,词表大小)。\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # nn.GRU以张量作为隐状态\n",
    "            return  torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens),\n",
    "                                device=device)\n",
    "        else:\n",
    "            # nn.LSTM以元组作为隐状态\n",
    "            return (torch.zeros((\n",
    "                self.num_directions * self.rnn.num_layers,\n",
    "                batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((\n",
    "                        self.num_directions * self.rnn.num_layers,\n",
    "                        batch_size, self.num_hiddens), device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573178db",
   "metadata": {
    "origin_pos": 27
   },
   "source": [
    "## 训练与预测\n",
    "\n",
    "在训练模型之前，让我们[**基于一个具有随机权重的模型进行预测**]。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12981c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def predict_ch8(prefix, num_preds, net, vocab, device):  #@save\n",
    "    \"\"\"在prefix后面生成新字符\"\"\"\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
    "    for y in prefix[1:]:  # 预热期\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):  # 预测num_preds步\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93c01655",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:22:42.176888Z",
     "iopub.status.busy": "2023-08-18T07:22:42.175834Z",
     "iopub.status.idle": "2023-08-18T07:22:45.380419Z",
     "shell.execute_reply": "2023-08-18T07:22:45.379583Z"
    },
    "origin_pos": 29,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time travellernlvnnnnnnn'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = try_gpu()\n",
    "net = RNNModel(rnn_layer, vocab_size=len(vocab))\n",
    "net = net.to(device)\n",
    "predict_ch8('time traveller', 10, net, vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f0d45",
   "metadata": {
    "origin_pos": 32
   },
   "source": [
    "很明显，这种模型根本不能输出好的结果。\n",
    "接下来，我们使用 :numref:`sec_rnn_scratch`中\n",
    "定义的超参数调用`train_ch8`，并且[**使用高级API训练模型**]。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "375cd6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "\n",
    "class Timer:  #@save\n",
    "    \"\"\"记录多次运行时间\"\"\"\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"启动计时器\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"停止计时器并将时间记录在列表中\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"返回平均时间\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"返回时间总和\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"返回累计时间\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()\n",
    "\n",
    "class Accumulator:  #@save\n",
    "    \"\"\"在n个变量上累加\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def grad_clipping(net, theta):  #@save\n",
    "    \"\"\"裁剪梯度\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm\n",
    "\n",
    "#@save\n",
    "def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n",
    "    \"\"\"训练网络一个迭代周期（定义见第8章）\"\"\"\n",
    "    state, timer = None, Timer()\n",
    "    metric = Accumulator(2)  # 训练损失之和,词元数量\n",
    "    for X, Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            # 在第一次迭代或使用随机抽样时初始化state\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "        else:\n",
    "            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
    "                # state对于nn.GRU是个张量\n",
    "                state.detach_()\n",
    "            else:\n",
    "                # state对于nn.LSTM或对于我们从零开始实现的模型是个张量\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        y = Y.T.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat, state = net(X, state)\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            # 因为已经调用了mean函数\n",
    "            updater(batch_size=1)\n",
    "        metric.add(l * y.numel(), y.numel())\n",
    "    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a41e96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):  #@save\n",
    "    \"\"\"小批量随机梯度下降\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()\n",
    "\n",
    "from IPython import display\n",
    "class Animator:  #@save\n",
    "    \"\"\"在动画中绘制数据\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        # 增量地绘制多条线\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        \n",
    "        # 替换 d2l.use_svg_display()\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.rcParams['figure.figsize'] = figsize\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # 替换 d2l.set_axes，使用 lambda 函数直接配置参数\n",
    "        self.config_axes = lambda: self._set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def _set_axes(self, ax, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "        \"\"\"设置坐标轴标签、范围和比例\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        # 设置坐标轴标签\n",
    "        if xlabel:\n",
    "            ax.set_xlabel(xlabel)\n",
    "        if ylabel:\n",
    "            ax.set_ylabel(ylabel)\n",
    "        # 设置坐标轴范围\n",
    "        if xlim:\n",
    "            ax.set_xlim(xlim)\n",
    "        if ylim:\n",
    "            ax.set_ylim(ylim)\n",
    "        # 设置坐标轴比例\n",
    "        if xscale:\n",
    "            ax.set_xscale(xscale)\n",
    "        if yscale:\n",
    "            ax.set_yscale(yscale)\n",
    "        # 设置图例\n",
    "        if legend:\n",
    "            ax.legend(legend)\n",
    "        # 添加网格\n",
    "        ax.grid(True)\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # 向图表中添加多个数据点\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "\n",
    "#@save\n",
    "def train_ch8(net, train_iter, vocab, lr, num_epochs, device,\n",
    "              use_random_iter=False):\n",
    "    \"\"\"训练模型（定义见第8章）\"\"\"\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    animator = Animator(xlabel='epoch', ylabel='perplexity',\n",
    "                            legend=['train'], xlim=[10, num_epochs])\n",
    "    # 初始化\n",
    "    if isinstance(net, nn.Module):\n",
    "        updater = torch.optim.SGD(net.parameters(), lr)\n",
    "    else:\n",
    "        updater = lambda batch_size: sgd(net.params, lr, batch_size)\n",
    "    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)\n",
    "    # 训练和预测\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl, speed = train_epoch_ch8(\n",
    "            net, train_iter, loss, updater, device, use_random_iter)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict('time traveller'))\n",
    "            animator.add(epoch + 1, [ppl])\n",
    "    print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')\n",
    "    print(predict('time traveller'))\n",
    "    print(predict('traveller'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97fc8534",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:22:45.384672Z",
     "iopub.status.busy": "2023-08-18T07:22:45.383787Z",
     "iopub.status.idle": "2023-08-18T07:23:06.055571Z",
     "shell.execute_reply": "2023-08-18T07:23:06.054355Z"
    },
    "origin_pos": 34,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "困惑度 1.3, 67283.9 词元/秒 cpu\n",
      "time traveller came back andfilby s anecdote collapsedthe thing \n",
      "traveller surdent a dicwey then thought reas to jpein is fo\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs, lr = 500, 1\n",
    "train_ch8(net, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409494cb",
   "metadata": {
    "origin_pos": 37
   },
   "source": [
    "与上一节相比，由于深度学习框架的高级API对代码进行了更多的优化，\n",
    "该模型在较短的时间内达到了较低的困惑度。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 深度学习框架的高级API提供了循环神经网络层的实现。\n",
    "* 高级API的循环神经网络层返回一个输出和一个更新后的隐状态，我们还需要计算整个模型的输出层。\n",
    "* 相比从零开始实现的循环神经网络，使用高级API实现可以加速训练。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 尝试使用高级API，能使循环神经网络模型过拟合吗？\n",
    "1. 如果在循环神经网络模型中增加隐藏层的数量会发生什么？能使模型正常工作吗？\n",
    "1. 尝试使用循环神经网络实现 :numref:`sec_sequence`的自回归模型。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84e3e86",
   "metadata": {
    "origin_pos": 39,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/2106)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
