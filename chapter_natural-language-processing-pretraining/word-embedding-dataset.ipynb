{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f9eacb",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 用于预训练词嵌入的数据集\n",
    ":label:`sec_word2vec_data`\n",
    "\n",
    "现在我们已经了解了word2vec模型的技术细节和大致的训练方法，让我们来看看它们的实现。具体地说，我们将以 :numref:`sec_word2vec`的跳元模型和 :numref:`sec_approx_train`的负采样为例。本节从用于预训练词嵌入模型的数据集开始：数据的原始格式将被转换为可以在训练期间迭代的小批量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "596ed133",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:38.933299Z",
     "iopub.status.busy": "2023-08-18T07:01:38.932361Z",
     "iopub.status.idle": "2023-08-18T07:01:41.929964Z",
     "shell.execute_reply": "2023-08-18T07:01:41.928691Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "# from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8286adf0",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "## 读取数据集\n",
    "\n",
    "我们在这里使用的数据集是[Penn Tree Bank（PTB）](https://catalog.ldc.upenn.edu/LDC99T42)。该语料库取自“华尔街日报”的文章，分为训练集、验证集和测试集。在原始格式中，文本文件的每一行表示由空格分隔的一句话。在这里，我们将每个单词视为一个词元。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc6c9b2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:41.935897Z",
     "iopub.status.busy": "2023-08-18T07:01:41.934975Z",
     "iopub.status.idle": "2023-08-18T07:01:42.345380Z",
     "shell.execute_reply": "2023-08-18T07:01:42.344041Z"
    },
    "origin_pos": 5,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# sentences数: 42069'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import requests\n",
    "import zipfile\n",
    "import tarfile\n",
    "import hashlib\n",
    "\n",
    "DATA_HUB = {}\n",
    "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'\n",
    "\n",
    "DATA_HUB['ptb'] = (DATA_URL + 'ptb.zip',\n",
    "                       '319d85e578af0cdc590547f26231e4e31cdf1e42')\n",
    "\n",
    "def download(name, cache_dir=os.path.join('..', 'data')):  #@save\n",
    "    \"\"\"下载一个DATA_HUB中的文件，返回本地文件名\"\"\"\n",
    "    assert name in DATA_HUB, f\"{name} 不存在于 {DATA_HUB}\"\n",
    "    url, sha1_hash = DATA_HUB[name]\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
    "    if os.path.exists(fname):\n",
    "        sha1 = hashlib.sha1()\n",
    "        with open(fname, 'rb') as f:\n",
    "            while True:\n",
    "                data = f.read(1048576)\n",
    "                if not data:\n",
    "                    break\n",
    "                sha1.update(data)\n",
    "        if sha1.hexdigest() == sha1_hash:\n",
    "            return fname  # 命中缓存\n",
    "    print(f'正在从{url}下载{fname}...')\n",
    "    r = requests.get(url, stream=True, verify=True)\n",
    "    with open(fname, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    return fname\n",
    "\n",
    "def download_extract(name, folder=None):  #@save\n",
    "    \"\"\"下载并解压zip/tar文件\"\"\"\n",
    "    fname = download(name)\n",
    "    base_dir = os.path.dirname(fname)\n",
    "    data_dir, ext = os.path.splitext(fname)\n",
    "    if ext == '.zip':\n",
    "        fp = zipfile.ZipFile(fname, 'r')\n",
    "    elif ext in ('.tar', '.gz'):\n",
    "        fp = tarfile.open(fname, 'r')\n",
    "    else:\n",
    "        assert False, '只有zip/tar文件可以被解压缩'\n",
    "    fp.extractall(base_dir)\n",
    "    return os.path.join(base_dir, folder) if folder else data_dir\n",
    "\n",
    "#@save\n",
    "def read_ptb():\n",
    "    \"\"\"将PTB数据集加载到文本行的列表中\"\"\"\n",
    "    data_dir = download_extract('ptb')\n",
    "    # Readthetrainingset.\n",
    "    with open(os.path.join(data_dir, 'ptb.train.txt')) as f:\n",
    "        raw_text = f.read()\n",
    "    return [line.split() for line in raw_text.split('\\n')]\n",
    "\n",
    "sentences = read_ptb()\n",
    "f'# sentences数: {len(sentences)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7290de5",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "在读取训练集之后，我们为语料库构建了一个词表，其中出现次数少于10次的任何单词都将由“&lt;unk&gt;”词元替换。请注意，原始数据集还包含表示稀有（未知）单词的“&lt;unk&gt;”词元。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "921c4d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "def count_corpus(tokens):  #@save\n",
    "    \"\"\"统计词元的频率\"\"\"\n",
    "    # 这里的tokens是1D列表或2D列表\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # 将词元列表展平成一个列表\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)\n",
    "\n",
    "class Vocab:  #@save\n",
    "    \"\"\"文本词表\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        # 按出现频率排序\n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        # 未知词元的索引为0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # 未知词元的索引为0\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04285c2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:42.350103Z",
     "iopub.status.busy": "2023-08-18T07:01:42.349586Z",
     "iopub.status.idle": "2023-08-18T07:01:42.520737Z",
     "shell.execute_reply": "2023-08-18T07:01:42.519523Z"
    },
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vocab size: 6719'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocab(sentences, min_freq=10)\n",
    "f'vocab size: {len(vocab)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bba2291",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "## 下采样\n",
    "\n",
    "文本数据通常有“the”“a”和“in”等高频词：它们在非常大的语料库中甚至可能出现数十亿次。然而，这些词经常在上下文窗口中与许多不同的词共同出现，提供的有用信息很少。例如，考虑上下文窗口中的词“chip”：直观地说，它与低频单词“intel”的共现比与高频单词“a”的共现在训练中更有用。此外，大量（高频）单词的训练速度很慢。因此，当训练词嵌入模型时，可以对高频单词进行*下采样* :cite:`Mikolov.Sutskever.Chen.ea.2013`。具体地说，数据集中的每个词$w_i$将有概率地被丢弃\n",
    "\n",
    "$$ P(w_i) = \\max\\left(1 - \\sqrt{\\frac{t}{f(w_i)}}, 0\\right),$$\n",
    "\n",
    "其中$f(w_i)$是$w_i$的词数与数据集中的总词数的比率，常量$t$是超参数（在实验中为$10^{-4}$）。我们可以看到，只有当相对比率$f(w_i) > t$时，（高频）词$w_i$才能被丢弃，且该词的相对比率越高，被丢弃的概率就越大。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88d0f9c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:42.524901Z",
     "iopub.status.busy": "2023-08-18T07:01:42.524245Z",
     "iopub.status.idle": "2023-08-18T07:01:44.019122Z",
     "shell.execute_reply": "2023-08-18T07:01:44.017912Z"
    },
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "def subsample(sentences, vocab):\n",
    "    \"\"\"下采样高频词\"\"\"\n",
    "    # 排除未知词元'<unk>'\n",
    "    sentences = [[token for token in line if vocab[token] != vocab.unk]\n",
    "                 for line in sentences]\n",
    "    counter = count_corpus(sentences)\n",
    "    num_tokens = sum(counter.values())\n",
    "\n",
    "    # 如果在下采样期间保留词元，则返回True\n",
    "    def keep(token):\n",
    "        return(random.uniform(0, 1) <\n",
    "               math.sqrt(1e-4 / counter[token] * num_tokens))\n",
    "\n",
    "    return ([[token for token in line if keep(token)] for line in sentences],\n",
    "            counter)\n",
    "\n",
    "subsampled, counter = subsample(sentences, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c892ade",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "下面的代码片段绘制了下采样前后每句话的词元数量的直方图。正如预期的那样，下采样通过删除高频词来显著缩短句子，这将使训练加速。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6d23b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_list_len_pair_hist(legend, xlabel, ylabel, xlist, ylist):\n",
    "    \"\"\"绘制列表长度对的直方图\"\"\"\n",
    "    # 设置图形大小\n",
    "    plt.figure(figsize=(3.5, 2.5))\n",
    "    \n",
    "    # 计算每个列表的长度\n",
    "    x_lengths = [len(l) for l in xlist]\n",
    "    y_lengths = [len(l) for l in ylist]\n",
    "    \n",
    "    # 绘制直方图\n",
    "    _, bins, patches = plt.hist([x_lengths, y_lengths], label=legend)\n",
    "    \n",
    "    # 为第二个数据集（target）的条形添加斜线填充\n",
    "    for patch in patches[1]:\n",
    "        patch.set_hatch('/')\n",
    "    \n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend(legend)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dd0b4f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:44.024294Z",
     "iopub.status.busy": "2023-08-18T07:01:44.023765Z",
     "iopub.status.idle": "2023-08-18T07:01:44.272889Z",
     "shell.execute_reply": "2023-08-18T07:01:44.271933Z"
    },
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_list_len_pair_hist(\n",
    "    ['origin', 'subsampled'], '# tokens per sentence',\n",
    "    'count', sentences, subsampled);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80da6e9d",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "对于单个词元，高频词“the”的采样率不到1/20。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ac63b1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:44.277942Z",
     "iopub.status.busy": "2023-08-18T07:01:44.277661Z",
     "iopub.status.idle": "2023-08-18T07:01:44.319135Z",
     "shell.execute_reply": "2023-08-18T07:01:44.317982Z"
    },
    "origin_pos": 13,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"the\"的数量：之前=50770, 之后=2000'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_counts(token):\n",
    "    return (f'\"{token}\"的数量：'\n",
    "            f'之前={sum([l.count(token) for l in sentences])}, '\n",
    "            f'之后={sum([l.count(token) for l in subsampled])}')\n",
    "\n",
    "compare_counts('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ef69ef",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "相比之下，低频词“join”则被完全保留。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9307cb04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:44.324650Z",
     "iopub.status.busy": "2023-08-18T07:01:44.323726Z",
     "iopub.status.idle": "2023-08-18T07:01:44.366586Z",
     "shell.execute_reply": "2023-08-18T07:01:44.365449Z"
    },
    "origin_pos": 15,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"join\"的数量：之前=45, 之后=45'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_counts('join')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38762200",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "在下采样之后，我们将词元映射到它们在语料库中的索引。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed59e4d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:44.371681Z",
     "iopub.status.busy": "2023-08-18T07:01:44.370695Z",
     "iopub.status.idle": "2023-08-18T07:01:44.930927Z",
     "shell.execute_reply": "2023-08-18T07:01:44.929824Z"
    },
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [392, 2115, 145, 18, 406], [5277, 3054, 1580]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [vocab[line] for line in subsampled]\n",
    "corpus[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5918fb",
   "metadata": {
    "origin_pos": 18
   },
   "source": [
    "## 中心词和上下文词的提取\n",
    "\n",
    "下面的`get_centers_and_contexts`函数从`corpus`中提取所有中心词及其上下文词。它随机采样1到`max_window_size`之间的整数作为上下文窗口。对于任一中心词，与其距离不超过采样上下文窗口大小的词为其上下文词。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4a20ba3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:44.935833Z",
     "iopub.status.busy": "2023-08-18T07:01:44.935066Z",
     "iopub.status.idle": "2023-08-18T07:01:44.944963Z",
     "shell.execute_reply": "2023-08-18T07:01:44.943901Z"
    },
    "origin_pos": 19,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def get_centers_and_contexts(corpus, max_window_size):\n",
    "    \"\"\"返回跳元模型中的中心词和上下文词\"\"\"\n",
    "    centers, contexts = [], []\n",
    "    for line in corpus:\n",
    "        # 要形成“中心词-上下文词”对，每个句子至少需要有2个词\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        centers += line\n",
    "        for i in range(len(line)):  # 上下文窗口中间i\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, i - window_size),\n",
    "                                 min(len(line), i + 1 + window_size)))\n",
    "            # 从上下文词中排除中心词\n",
    "            indices.remove(i)\n",
    "            contexts.append([line[idx] for idx in indices])\n",
    "    return centers, contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fba895",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "接下来，我们创建一个人工数据集，分别包含7个和3个单词的两个句子。设置最大上下文窗口大小为2，并打印所有中心词及其上下文词。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fae4771b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:44.948910Z",
     "iopub.status.busy": "2023-08-18T07:01:44.948190Z",
     "iopub.status.idle": "2023-08-18T07:01:44.955563Z",
     "shell.execute_reply": "2023-08-18T07:01:44.954488Z"
    },
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集 [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n",
      "中心词 0 的上下文词是 [1, 2]\n",
      "中心词 1 的上下文词是 [0, 2]\n",
      "中心词 2 的上下文词是 [0, 1, 3, 4]\n",
      "中心词 3 的上下文词是 [1, 2, 4, 5]\n",
      "中心词 4 的上下文词是 [2, 3, 5, 6]\n",
      "中心词 5 的上下文词是 [4, 6]\n",
      "中心词 6 的上下文词是 [5]\n",
      "中心词 7 的上下文词是 [8]\n",
      "中心词 8 的上下文词是 [7, 9]\n",
      "中心词 9 的上下文词是 [7, 8]\n"
     ]
    }
   ],
   "source": [
    "tiny_dataset = [list(range(7)), list(range(7, 10))]\n",
    "print('数据集', tiny_dataset)\n",
    "for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\n",
    "    print('中心词', center, '的上下文词是', context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21272fc",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "在PTB数据集上进行训练时，我们将最大上下文窗口大小设置为5。下面提取数据集中的所有中心词及其上下文词。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec92f27e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:44.960145Z",
     "iopub.status.busy": "2023-08-18T07:01:44.959231Z",
     "iopub.status.idle": "2023-08-18T07:01:47.218796Z",
     "shell.execute_reply": "2023-08-18T07:01:47.217626Z"
    },
    "origin_pos": 23,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# “中心词-上下文词对”的数量: 1502622'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_centers, all_contexts = get_centers_and_contexts(corpus, 5)\n",
    "f'# “中心词-上下文词对”的数量: {sum([len(contexts) for contexts in all_contexts])}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48c535f",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "## 负采样\n",
    "\n",
    "我们使用负采样进行近似训练。为了根据预定义的分布对噪声词进行采样，我们定义以下`RandomGenerator`类，其中（可能未规范化的）采样分布通过变量`sampling_weights`传递。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "365189a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:47.223801Z",
     "iopub.status.busy": "2023-08-18T07:01:47.223354Z",
     "iopub.status.idle": "2023-08-18T07:01:47.232254Z",
     "shell.execute_reply": "2023-08-18T07:01:47.231166Z"
    },
    "origin_pos": 25,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "class RandomGenerator:\n",
    "    \"\"\"根据n个采样权重在{1,...,n}中随机抽取\"\"\"\n",
    "    def __init__(self, sampling_weights):\n",
    "        # Exclude\n",
    "        self.population = list(range(1, len(sampling_weights) + 1))\n",
    "        self.sampling_weights = sampling_weights\n",
    "        self.candidates = []\n",
    "        self.i = 0\n",
    "\n",
    "    def draw(self):\n",
    "        if self.i == len(self.candidates):\n",
    "            # 缓存k个随机采样结果\n",
    "            self.candidates = random.choices(\n",
    "                self.population, self.sampling_weights, k=10000)\n",
    "            self.i = 0\n",
    "        self.i += 1\n",
    "        return self.candidates[self.i - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f886ada9",
   "metadata": {
    "origin_pos": 26
   },
   "source": [
    "例如，我们可以在索引1、2和3中绘制10个随机变量$X$，采样概率为$P(X=1)=2/9, P(X=2)=3/9$和$P(X=3)=4/9$，如下所示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f534865c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:47.237153Z",
     "iopub.status.busy": "2023-08-18T07:01:47.236381Z",
     "iopub.status.idle": "2023-08-18T07:01:47.251510Z",
     "shell.execute_reply": "2023-08-18T07:01:47.250435Z"
    },
    "origin_pos": 27,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 1, 3, 3, 1, 2, 3, 3, 3]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@save\n",
    "generator = RandomGenerator([2, 3, 4])\n",
    "[generator.draw() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4049d4",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "对于一对中心词和上下文词，我们随机抽取了`K`个（实验中为5个）噪声词。根据word2vec论文中的建议，将噪声词$w$的采样概率$P(w)$设置为其在字典中的相对频率，其幂为0.75 :cite:`Mikolov.Sutskever.Chen.ea.2013`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21950025",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:47.256344Z",
     "iopub.status.busy": "2023-08-18T07:01:47.255586Z",
     "iopub.status.idle": "2023-08-18T07:01:59.259799Z",
     "shell.execute_reply": "2023-08-18T07:01:59.258793Z"
    },
    "origin_pos": 29,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def get_negatives(all_contexts, vocab, counter, K):\n",
    "    \"\"\"返回负采样中的噪声词\"\"\"\n",
    "    # 索引为1、2、...（索引0是词表中排除的未知标记）\n",
    "    sampling_weights = [counter[vocab.to_tokens(i)]**0.75\n",
    "                        for i in range(1, len(vocab))]\n",
    "    all_negatives, generator = [], RandomGenerator(sampling_weights)\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            neg = generator.draw()\n",
    "            # 噪声词不能是上下文词\n",
    "            if neg not in contexts:\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives\n",
    "\n",
    "all_negatives = get_negatives(all_contexts, vocab, counter, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa17e2d",
   "metadata": {
    "origin_pos": 30
   },
   "source": [
    "## 小批量加载训练实例\n",
    ":label:`subsec_word2vec-minibatch-loading`\n",
    "\n",
    "在提取所有中心词及其上下文词和采样噪声词后，将它们转换成小批量的样本，在训练过程中可以迭代加载。\n",
    "\n",
    "在小批量中，$i^\\mathrm{th}$个样本包括中心词及其$n_i$个上下文词和$m_i$个噪声词。由于上下文窗口大小不同，$n_i+m_i$对于不同的$i$是不同的。因此，对于每个样本，我们在`contexts_negatives`个变量中将其上下文词和噪声词连结起来，并填充零，直到连结长度达到$\\max_i n_i+m_i$(`max_len`)。为了在计算损失时排除填充，我们定义了掩码变量`masks`。在`masks`中的元素和`contexts_negatives`中的元素之间存在一一对应关系，其中`masks`中的0（否则为1）对应于`contexts_negatives`中的填充。\n",
    "\n",
    "为了区分正反例，我们在`contexts_negatives`中通过一个`labels`变量将上下文词与噪声词分开。类似于`masks`，在`labels`中的元素和`contexts_negatives`中的元素之间也存在一一对应关系，其中`labels`中的1（否则为0）对应于`contexts_negatives`中的上下文词的正例。\n",
    "\n",
    "上述思想在下面的`batchify`函数中实现。其输入`data`是长度等于批量大小的列表，其中每个元素是由中心词`center`、其上下文词`context`和其噪声词`negative`组成的样本。此函数返回一个可以在训练期间加载用于计算的小批量，例如包括掩码变量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e92a65e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:59.264970Z",
     "iopub.status.busy": "2023-08-18T07:01:59.264337Z",
     "iopub.status.idle": "2023-08-18T07:01:59.271417Z",
     "shell.execute_reply": "2023-08-18T07:01:59.270518Z"
    },
    "origin_pos": 31,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def batchify(data):\n",
    "    \"\"\"返回带有负采样的跳元模型的小批量样本\"\"\"\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers += [center]\n",
    "        contexts_negatives += \\\n",
    "            [context + negative + [0] * (max_len - cur_len)]\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "    return (torch.tensor(centers).reshape((-1, 1)), torch.tensor(\n",
    "        contexts_negatives), torch.tensor(masks), torch.tensor(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeb5c51",
   "metadata": {
    "origin_pos": 32
   },
   "source": [
    "让我们使用一个小批量的两个样本来测试此函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e14e34ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:59.276193Z",
     "iopub.status.busy": "2023-08-18T07:01:59.275387Z",
     "iopub.status.idle": "2023-08-18T07:01:59.282832Z",
     "shell.execute_reply": "2023-08-18T07:01:59.281912Z"
    },
    "origin_pos": 33,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers = tensor([[1],\n",
      "        [1]])\n",
      "contexts_negatives = tensor([[2, 2, 3, 3, 3, 3],\n",
      "        [2, 2, 2, 3, 3, 0]])\n",
      "masks = tensor([[1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0]])\n",
      "labels = tensor([[1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x_1 = (1, [2, 2], [3, 3, 3, 3])\n",
    "x_2 = (1, [2, 2, 2], [3, 3])\n",
    "batch = batchify((x_1, x_2))\n",
    "\n",
    "names = ['centers', 'contexts_negatives', 'masks', 'labels']\n",
    "for name, data in zip(names, batch):\n",
    "    print(name, '=', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eef3d8",
   "metadata": {
    "origin_pos": 34
   },
   "source": [
    "## 整合代码\n",
    "\n",
    "最后，我们定义了读取PTB数据集并返回数据迭代器和词表的`load_data_ptb`函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ff7f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader_workers():  #@save\n",
    "    \"\"\"使用4个进程来读取数据\"\"\"\n",
    "    return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ddfb20d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:59.287587Z",
     "iopub.status.busy": "2023-08-18T07:01:59.286823Z",
     "iopub.status.idle": "2023-08-18T07:01:59.296040Z",
     "shell.execute_reply": "2023-08-18T07:01:59.294978Z"
    },
    "origin_pos": 36,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class PTBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, centers, contexts, negatives):\n",
    "        assert len(centers) == len(contexts) == len(negatives)\n",
    "        self.centers = centers\n",
    "        self.contexts = contexts\n",
    "        self.negatives = negatives\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.centers[index], self.contexts[index],\n",
    "                self.negatives[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.centers)\n",
    "\n",
    "def load_data_ptb(batch_size, max_window_size, num_noise_words):\n",
    "    \"\"\"下载PTB数据集，然后将其加载到内存中\"\"\"\n",
    "    num_workers = get_dataloader_workers()\n",
    "    sentences = read_ptb()\n",
    "    vocab = Vocab(sentences, min_freq=10)\n",
    "    subsampled, counter = subsample(sentences, vocab)\n",
    "    corpus = [vocab[line] for line in subsampled]\n",
    "    all_centers, all_contexts = get_centers_and_contexts(\n",
    "        corpus, max_window_size)\n",
    "    all_negatives = get_negatives(\n",
    "        all_contexts, vocab, counter, num_noise_words)\n",
    "\n",
    "    dataset = PTBDataset(all_centers, all_contexts, all_negatives)\n",
    "\n",
    "    data_iter = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size, shuffle=True,\n",
    "        collate_fn=batchify, num_workers=num_workers)\n",
    "    return data_iter, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97991d10",
   "metadata": {
    "origin_pos": 38
   },
   "source": [
    "让我们打印数据迭代器的第一个小批量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5115b257",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:59.300574Z",
     "iopub.status.busy": "2023-08-18T07:01:59.299960Z",
     "iopub.status.idle": "2023-08-18T07:02:13.672095Z",
     "shell.execute_reply": "2023-08-18T07:02:13.671142Z"
    },
    "origin_pos": 39,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "data_iter, vocab = load_data_ptb(512, 5, 5)\n",
    "for batch in data_iter:\n",
    "    for name, data in zip(names, batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc03f54",
   "metadata": {
    "origin_pos": 40
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 高频词在训练中可能不是那么有用。我们可以对他们进行下采样，以便在训练中加快速度。\n",
    "* 为了提高计算效率，我们以小批量方式加载样本。我们可以定义其他变量来区分填充标记和非填充标记，以及正例和负例。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 如果不使用下采样，本节中代码的运行时间会发生什么变化？\n",
    "1. `RandomGenerator`类缓存`k`个随机采样结果。将`k`设置为其他值，看看它如何影响数据加载速度。\n",
    "1. 本节代码中的哪些其他超参数可能会影响数据加载速度？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e415387",
   "metadata": {
    "origin_pos": 42,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/5735)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
