{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d128f467",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 线性回归\n",
    ":label:`sec_linear_regression`\n",
    "\n",
    "*回归*（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。\n",
    "在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。\n",
    "\n",
    "在机器学习领域中的大多数任务通常都与*预测*（prediction）有关。\n",
    "当我们想预测一个数值时，就会涉及到回归问题。\n",
    "常见的例子包括：预测价格（房屋、股票等）、预测住院时间（针对住院病人等）、\n",
    "预测需求（零售销量等）。\n",
    "但不是所有的*预测*都是回归问题。\n",
    "在后面的章节中，我们将介绍分类问题。分类问题的目标是预测数据属于一组类别中的哪一个。\n",
    "\n",
    "## 线性回归的基本元素\n",
    "\n",
    "*线性回归*（linear regression）可以追溯到19世纪初，\n",
    "它在回归的各种标准工具中最简单而且最流行。\n",
    "线性回归基于几个简单的假设：\n",
    "首先，假设自变量$\\mathbf{x}$和因变量$y$之间的关系是线性的，\n",
    "即$y$可以表示为$\\mathbf{x}$中元素的加权和，这里通常允许包含观测值的一些噪声；\n",
    "其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。\n",
    "\n",
    "为了解释*线性回归*，我们举一个实际的例子：\n",
    "我们希望根据房屋的面积（平方英尺）和房龄（年）来估算房屋价格（美元）。\n",
    "为了开发一个能预测房价的模型，我们需要收集一个真实的数据集。\n",
    "这个数据集包括了房屋的销售价格、面积和房龄。\n",
    "在机器学习的术语中，该数据集称为*训练数据集*（training data set）\n",
    "或*训练集*（training set）。\n",
    "每行数据（比如一次房屋交易相对应的数据）称为*样本*（sample），\n",
    "也可以称为*数据点*（data point）或*数据样本*（data instance）。\n",
    "我们把试图预测的目标（比如预测房屋价格）称为*标签*（label）或*目标*（target）。\n",
    "预测所依据的自变量（面积和房龄）称为*特征*（feature）或*协变量*（covariate）。\n",
    "\n",
    "通常，我们使用$n$来表示数据集中的样本数。\n",
    "对索引为$i$的样本，其输入表示为$\\mathbf{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}]^\\top$，\n",
    "其对应的标签是$y^{(i)}$。\n",
    "\n",
    "### 线性模型\n",
    ":label:`subsec_linear_model`\n",
    "\n",
    "线性假设是指目标（房屋价格）可以表示为特征（面积和房龄）的加权和，如下面的式子：\n",
    "\n",
    "$$\\mathrm{price} = w_{\\mathrm{area}} \\cdot \\mathrm{area} + w_{\\mathrm{age}} \\cdot \\mathrm{age} + b.$$\n",
    ":eqlabel:`eq_price-area`\n",
    "\n",
    " :eqref:`eq_price-area`中的$w_{\\mathrm{area}}$和$w_{\\mathrm{age}}$\n",
    "称为*权重*（weight），权重决定了每个特征对我们预测值的影响。\n",
    "$b$称为*偏置*（bias）、*偏移量*（offset）或*截距*（intercept）。\n",
    "偏置是指当所有特征都取值为0时，预测值应该为多少。\n",
    "即使现实中不会有任何房子的面积是0或房龄正好是0年，我们仍然需要偏置项。\n",
    "如果没有偏置项，我们模型的表达能力将受到限制。\n",
    "严格来说， :eqref:`eq_price-area`是输入特征的一个\n",
    "*仿射变换*（affine transformation）。\n",
    "仿射变换的特点是通过加权和对特征进行*线性变换*（linear transformation），\n",
    "并通过偏置项来进行*平移*（translation）。\n",
    "\n",
    "给定一个数据集，我们的目标是寻找模型的权重$\\mathbf{w}$和偏置$b$，\n",
    "使得根据模型做出的预测大体符合数据里的真实价格。\n",
    "输出的预测值由输入特征通过*线性模型*的仿射变换决定，仿射变换由所选权重和偏置确定。\n",
    "\n",
    "而在机器学习领域，我们通常使用的是高维数据集，建模时采用线性代数表示法会比较方便。\n",
    "当我们的输入包含$d$个特征时，我们将预测结果$\\hat{y}$\n",
    "（通常使用“尖角”符号表示$y$的估计值）表示为：\n",
    "\n",
    "$$\\hat{y} = w_1  x_1 + ... + w_d  x_d + b.$$\n",
    "\n",
    "将所有特征放到向量$\\mathbf{x} \\in \\mathbb{R}^d$中，\n",
    "并将所有权重放到向量$\\mathbf{w} \\in \\mathbb{R}^d$中，\n",
    "我们可以用点积形式来简洁地表达模型：\n",
    "\n",
    "$$\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b.$$\n",
    ":eqlabel:`eq_linreg-y`\n",
    "\n",
    "在 :eqref:`eq_linreg-y`中，\n",
    "向量$\\mathbf{x}$对应于单个数据样本的特征。\n",
    "用符号表示的矩阵$\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$\n",
    "可以很方便地引用我们整个数据集的$n$个样本。\n",
    "其中，$\\mathbf{X}$的每一行是一个样本，每一列是一种特征。\n",
    "\n",
    "对于特征集合$\\mathbf{X}$，预测值$\\hat{\\mathbf{y}} \\in \\mathbb{R}^n$\n",
    "可以通过矩阵-向量乘法表示为：\n",
    "\n",
    "$${\\hat{\\mathbf{y}}} = \\mathbf{X} \\mathbf{w} + b$$\n",
    "\n",
    "这个过程中的求和将使用广播机制\n",
    "（广播机制在 :numref:`subsec_broadcasting`中有详细介绍）。\n",
    "给定训练数据特征$\\mathbf{X}$和对应的已知标签$\\mathbf{y}$，\n",
    "线性回归的目标是找到一组权重向量$\\mathbf{w}$和偏置$b$：\n",
    "当给定从$\\mathbf{X}$的同分布中取样的新样本特征时，\n",
    "这组权重向量和偏置能够使得新样本预测标签的误差尽可能小。\n",
    "\n",
    "虽然我们相信给定$\\mathbf{x}$预测$y$的最佳模型会是线性的，\n",
    "但我们很难找到一个有$n$个样本的真实数据集，其中对于所有的$1 \\leq i \\leq n$，$y^{(i)}$完全等于$\\mathbf{w}^\\top \\mathbf{x}^{(i)}+b$。\n",
    "无论我们使用什么手段来观察特征$\\mathbf{X}$和标签$\\mathbf{y}$，\n",
    "都可能会出现少量的观测误差。\n",
    "因此，即使确信特征与标签的潜在关系是线性的，\n",
    "我们也会加入一个噪声项来考虑观测误差带来的影响。\n",
    "\n",
    "在开始寻找最好的*模型参数*（model parameters）$\\mathbf{w}$和$b$之前，\n",
    "我们还需要两个东西：\n",
    "（1）一种模型质量的度量方式；\n",
    "（2）一种能够更新模型以提高模型预测质量的方法。\n",
    "\n",
    "### 损失函数\n",
    "\n",
    "在我们开始考虑如何用模型*拟合*（fit）数据之前，我们需要确定一个拟合程度的度量。\n",
    "*损失函数*（loss function）能够量化目标的*实际*值与*预测*值之间的差距。\n",
    "通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。\n",
    "回归问题中最常用的损失函数是平方误差函数。\n",
    "当样本$i$的预测值为$\\hat{y}^{(i)}$，其相应的真实标签为$y^{(i)}$时，\n",
    "平方误差可以定义为以下公式：\n",
    "\n",
    "$$l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2.$$\n",
    ":eqlabel:`eq_mse`\n",
    "\n",
    "常数$\\frac{1}{2}$不会带来本质的差别，但这样在形式上稍微简单一些\n",
    "（因为当我们对损失函数求导后常数系数为1）。\n",
    "由于训练数据集并不受我们控制，所以经验误差只是关于模型参数的函数。\n",
    "为了进一步说明，来看下面的例子。\n",
    "我们为一维情况下的回归问题绘制图像，如 :numref:`fig_fit_linreg`所示。\n",
    "\n",
    "![用线性模型拟合数据。](../img/fit-linreg.svg)\n",
    ":label:`fig_fit_linreg`\n",
    "\n",
    "由于平方误差函数中的二次方项，\n",
    "估计值$\\hat{y}^{(i)}$和观测值$y^{(i)}$之间较大的差异将导致更大的损失。\n",
    "为了度量模型在整个数据集上的质量，我们需计算在训练集$n$个样本上的损失均值（也等价于求和）。\n",
    "\n",
    "$$L(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2.$$\n",
    "\n",
    "在训练模型时，我们希望寻找一组参数（$\\mathbf{w}^*, b^*$），\n",
    "这组参数能最小化在所有训练样本上的总损失。如下式：\n",
    "\n",
    "$$\\mathbf{w}^*, b^* = \\operatorname*{argmin}_{\\mathbf{w}, b}\\  L(\\mathbf{w}, b).$$\n",
    "\n",
    "### 解析解\n",
    "\n",
    "线性回归刚好是一个很简单的优化问题。\n",
    "与我们将在本书中所讲到的其他大部分模型不同，线性回归的解可以用一个公式简单地表达出来，\n",
    "这类解叫作解析解（analytical solution）。\n",
    "首先，我们将偏置$b$合并到参数$\\mathbf{w}$中，合并方法是在包含所有参数的矩阵中附加一列。\n",
    "我们的预测问题是最小化$\\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^2$。\n",
    "这在损失平面上只有一个临界点，这个临界点对应于整个区域的损失极小点。\n",
    "将损失关于$\\mathbf{w}$的导数设为0，得到解析解：\n",
    "\n",
    "$$\\mathbf{w}^* = (\\mathbf X^\\top \\mathbf X)^{-1}\\mathbf X^\\top \\mathbf{y}.$$\n",
    "\n",
    "像线性回归这样的简单问题存在解析解，但并不是所有的问题都存在解析解。\n",
    "解析解可以进行很好的数学分析，但解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。\n",
    "\n",
    "### 随机梯度下降\n",
    "\n",
    "即使在我们无法得到解析解的情况下，我们仍然可以有效地训练模型。\n",
    "在许多任务上，那些难以优化的模型效果要更好。\n",
    "因此，弄清楚如何训练这些难以优化的模型是非常重要的。\n",
    "\n",
    "本书中我们用到一种名为*梯度下降*（gradient descent）的方法，\n",
    "这种方法几乎可以优化所有深度学习模型。\n",
    "它通过不断地在损失函数递减的方向上更新参数来降低误差。\n",
    "\n",
    "梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值）\n",
    "关于模型参数的导数（在这里也可以称为梯度）。\n",
    "但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。\n",
    "因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本，\n",
    "这种变体叫做*小批量随机梯度下降*（minibatch stochastic gradient descent）。\n",
    "\n",
    "在每次迭代中，我们首先随机抽样一个小批量$\\mathcal{B}$，\n",
    "它是由固定数量的训练样本组成的。\n",
    "然后，我们计算小批量的平均损失关于模型参数的导数（也可以称为梯度）。\n",
    "最后，我们将梯度乘以一个预先确定的正数$\\eta$，并从当前参数的值中减掉。\n",
    "\n",
    "我们用下面的数学公式来表示这一更新过程（$\\partial$表示偏导数）：\n",
    "\n",
    "$$(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b).$$\n",
    "\n",
    "总结一下，算法的步骤如下：\n",
    "（1）初始化模型参数的值，如随机初始化；\n",
    "（2）从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。\n",
    "对于平方损失和仿射变换，我们可以明确地写成如下形式:\n",
    "\n",
    "$$\\begin{aligned} \\mathbf{w} &\\leftarrow \\mathbf{w} -   \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{\\mathbf{w}} l^{(i)}(\\mathbf{w}, b) = \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right),\\\\ b &\\leftarrow b -  \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_b l^{(i)}(\\mathbf{w}, b)  = b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right). \\end{aligned}$$\n",
    ":eqlabel:`eq_linreg_batch_update`\n",
    "\n",
    "公式 :eqref:`eq_linreg_batch_update`中的$\\mathbf{w}$和$\\mathbf{x}$都是向量。\n",
    "在这里，更优雅的向量表示法比系数表示法（如$w_1, w_2, \\ldots, w_d$）更具可读性。\n",
    "$|\\mathcal{B}|$表示每个小批量中的样本数，这也称为*批量大小*（batch size）。\n",
    "$\\eta$表示*学习率*（learning rate）。\n",
    "批量大小和学习率的值通常是手动预先指定，而不是通过模型训练得到的。\n",
    "这些可以调整但不在训练过程中更新的参数称为*超参数*（hyperparameter）。\n",
    "*调参*（hyperparameter tuning）是选择超参数的过程。\n",
    "超参数通常是我们根据训练迭代结果来调整的，\n",
    "而训练迭代结果是在独立的*验证数据集*（validation dataset）上评估得到的。\n",
    "\n",
    "在训练了预先确定的若干迭代次数后（或者直到满足某些其他停止条件后），\n",
    "我们记录下模型参数的估计值，表示为$\\hat{\\mathbf{w}}, \\hat{b}$。\n",
    "但是，即使我们的函数确实是线性的且无噪声，这些估计值也不会使损失函数真正地达到最小值。\n",
    "因为算法会使得损失向最小值缓慢收敛，但却不能在有限的步数内非常精确地达到最小值。\n",
    "\n",
    "线性回归恰好是一个在整个域中只有一个最小值的学习问题。\n",
    "但是对像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。\n",
    "深度学习实践者很少会去花费大力气寻找这样一组参数，使得在*训练集*上的损失达到最小。\n",
    "事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失，\n",
    "这一挑战被称为*泛化*（generalization）。\n",
    "\n",
    "### 用模型进行预测\n",
    "\n",
    "给定“已学习”的线性回归模型$\\hat{\\mathbf{w}}^\\top \\mathbf{x} + \\hat{b}$，\n",
    "现在我们可以通过房屋面积$x_1$和房龄$x_2$来估计一个（未包含在训练数据中的）新房屋价格。\n",
    "给定特征估计目标的过程通常称为*预测*（prediction）或*推断*（inference）。\n",
    "\n",
    "本书将尝试坚持使用*预测*这个词。\n",
    "虽然*推断*这个词已经成为深度学习的标准术语，但其实*推断*这个词有些用词不当。\n",
    "在统计学中，*推断*更多地表示基于数据集估计参数。\n",
    "当深度学习从业者与统计学家交谈时，术语的误用经常导致一些误解。\n",
    "\n",
    "## 矢量化加速\n",
    "\n",
    "在训练我们的模型时，我们经常希望能够同时处理整个小批量的样本。\n",
    "为了实现这一点，需要(**我们对计算进行矢量化，\n",
    "从而利用线性代数库，而不是在Python中编写开销高昂的for循环**)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00f34f5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:05:53.225628Z",
     "iopub.status.busy": "2023-08-18T07:05:53.224888Z",
     "iopub.status.idle": "2023-08-18T07:05:55.670980Z",
     "shell.execute_reply": "2023-08-18T07:05:55.670021Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "# from d2l import torch as d2l\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16134deb",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "为了说明矢量化为什么如此重要，我们考虑(**对向量相加的两种方法**)。\n",
    "我们实例化两个全为1的10000维向量。\n",
    "在一种方法中，我们将使用Python的for循环遍历向量；\n",
    "在另一种方法中，我们将依赖对`+`的调用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dab2aed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:05:55.677138Z",
     "iopub.status.busy": "2023-08-18T07:05:55.676615Z",
     "iopub.status.idle": "2023-08-18T07:05:55.683088Z",
     "shell.execute_reply": "2023-08-18T07:05:55.682108Z"
    },
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "n = 10000\n",
    "a = torch.ones([n])\n",
    "b = torch.ones([n])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d858f1b",
   "metadata": {
    "origin_pos": 7
   },
   "source": [
    "由于在本书中我们将频繁地进行运行时间的基准测试，所以[**我们定义一个计时器**]：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "debf1949",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:05:55.687766Z",
     "iopub.status.busy": "2023-08-18T07:05:55.687275Z",
     "iopub.status.idle": "2023-08-18T07:05:55.694676Z",
     "shell.execute_reply": "2023-08-18T07:05:55.693692Z"
    },
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class Timer:  #@save\n",
    "    \"\"\"记录多次运行时间\"\"\"\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"启动计时器\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"停止计时器并将时间记录在列表中\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"返回平均时间\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"返回时间总和\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"返回累计时间\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e4d328",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "现在我们可以对工作负载进行基准测试。\n",
    "\n",
    "首先，[**我们使用for循环，每次执行一位的加法**]。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53998dfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:05:55.699851Z",
     "iopub.status.busy": "2023-08-18T07:05:55.699125Z",
     "iopub.status.idle": "2023-08-18T07:05:55.877034Z",
     "shell.execute_reply": "2023-08-18T07:05:55.875673Z"
    },
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.13556 sec'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.zeros(n)\n",
    "timer = Timer()\n",
    "for i in range(n):\n",
    "    c[i] = a[i] + b[i]\n",
    "f'{timer.stop():.5f} sec'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48c5704",
   "metadata": {
    "origin_pos": 13
   },
   "source": [
    "(**或者，我们使用重载的`+`运算符来计算按元素的和**)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a6c052c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:05:55.882550Z",
     "iopub.status.busy": "2023-08-18T07:05:55.881702Z",
     "iopub.status.idle": "2023-08-18T07:05:55.890012Z",
     "shell.execute_reply": "2023-08-18T07:05:55.888724Z"
    },
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.00000 sec'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer.start()\n",
    "d = a + b\n",
    "f'{timer.stop():.5f} sec'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e343af9",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "结果很明显，第二种方法比第一种方法快得多。\n",
    "矢量化代码通常会带来数量级的加速。\n",
    "另外，我们将更多的数学运算放到库中，而无须自己编写那么多的计算，从而减少了出错的可能性。\n",
    "\n",
    "## 正态分布与平方损失\n",
    ":label:`subsec_normal_distribution_and_squared_loss`\n",
    "\n",
    "接下来，我们通过对噪声分布的假设来解读平方损失目标函数。\n",
    "\n",
    "正态分布和线性回归之间的关系很密切。\n",
    "正态分布（normal distribution），也称为*高斯分布*（Gaussian distribution），\n",
    "最早由德国数学家高斯（Gauss）应用于天文学研究。\n",
    "简单的说，若随机变量$x$具有均值$\\mu$和方差$\\sigma^2$（标准差$\\sigma$），其正态分布概率密度函数如下：\n",
    "\n",
    "$$p(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (x - \\mu)^2\\right).$$\n",
    "\n",
    "下面[**我们定义一个Python函数来计算正态分布**]。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa7e3595",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:05:55.895314Z",
     "iopub.status.busy": "2023-08-18T07:05:55.894485Z",
     "iopub.status.idle": "2023-08-18T07:05:55.900942Z",
     "shell.execute_reply": "2023-08-18T07:05:55.899745Z"
    },
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def normal(x, mu, sigma):\n",
    "    p = 1 / math.sqrt(2 * math.pi * sigma**2)\n",
    "    return p * np.exp(-0.5 / sigma**2 * (x - mu)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5ec143",
   "metadata": {
    "origin_pos": 17
   },
   "source": [
    "我们现在(**可视化正态分布**)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d3e1e42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:05:55.906310Z",
     "iopub.status.busy": "2023-08-18T07:05:55.905361Z",
     "iopub.status.idle": "2023-08-18T07:05:56.341164Z",
     "shell.execute_reply": "2023-08-18T07:05:56.339902Z"
    },
    "origin_pos": 19,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 450x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib_inline import backend_inline\n",
    "\n",
    "# d2l = sys.modules[__name__]\n",
    "\n",
    "# 再次使用numpy进行可视化\n",
    "x = np.arange(-7, 7, 0.01)\n",
    "\n",
    "# 均值和标准差对\n",
    "params = [(0, 1), (0, 2), (3, 1)]\n",
    "\n",
    "# 计算不同参数下的正态分布值\n",
    "y_values = [normal(x, mu, sigma) for mu, sigma in params]\n",
    "\n",
    "# 绘制图像\n",
    "plt.figure(figsize=(4.5, 2.5))\n",
    "plt.plot(x, y_values[0], label='mean 0, std 1')\n",
    "plt.plot(x, y_values[1], '--', label='mean 0, std 2')\n",
    "plt.plot(x, y_values[2], '-.', label='mean 3, std 1')\n",
    "\n",
    "# 添加标签和图例\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('p(x)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# d2l.plot(x, [normal(x, mu, sigma) for mu, sigma in params], xlabel='x',\n",
    "#          ylabel='p(x)', figsize=(4.5, 2.5),\n",
    "#          legend=[f'mean {mu}, std {sigma}' for mu, sigma in params])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7509f1f1",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "就像我们所看到的，改变均值会产生沿$x$轴的偏移，增加方差将会分散分布、降低其峰值。\n",
    "\n",
    "均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是：\n",
    "我们假设了观测中包含噪声，其中噪声服从正态分布。\n",
    "噪声正态分布如下式:\n",
    "\n",
    "$$y = \\mathbf{w}^\\top \\mathbf{x} + b + \\epsilon,$$\n",
    "\n",
    "其中，$\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$。\n",
    "\n",
    "因此，我们现在可以写出通过给定的$\\mathbf{x}$观测到特定$y$的*似然*（likelihood）：\n",
    "\n",
    "$$P(y \\mid \\mathbf{x}) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (y - \\mathbf{w}^\\top \\mathbf{x} - b)^2\\right).$$\n",
    "\n",
    "现在，根据极大似然估计法，参数$\\mathbf{w}$和$b$的最优值是使整个数据集的*似然*最大的值：\n",
    "\n",
    "$$P(\\mathbf y \\mid \\mathbf X) = \\prod_{i=1}^{n} p(y^{(i)}|\\mathbf{x}^{(i)}).$$\n",
    "\n",
    "根据极大似然估计法选择的估计量称为*极大似然估计量*。\n",
    "虽然使许多指数函数的乘积最大化看起来很困难，\n",
    "但是我们可以在不改变目标的前提下，通过最大化似然对数来简化。\n",
    "由于历史原因，优化通常是说最小化而不是最大化。\n",
    "我们可以改为*最小化负对数似然*$-\\log P(\\mathbf y \\mid \\mathbf X)$。\n",
    "由此可以得到的数学公式是：\n",
    "\n",
    "$$-\\log P(\\mathbf y \\mid \\mathbf X) = \\sum_{i=1}^n \\frac{1}{2} \\log(2 \\pi \\sigma^2) + \\frac{1}{2 \\sigma^2} \\left(y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b\\right)^2.$$\n",
    "\n",
    "现在我们只需要假设$\\sigma$是某个固定常数就可以忽略第一项，\n",
    "因为第一项不依赖于$\\mathbf{w}$和$b$。\n",
    "现在第二项除了常数$\\frac{1}{\\sigma^2}$外，其余部分和前面介绍的均方误差是一样的。\n",
    "幸运的是，上面式子的解并不依赖于$\\sigma$。\n",
    "因此，在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计。\n",
    "\n",
    "## 从线性回归到深度网络\n",
    "\n",
    "到目前为止，我们只谈论了线性模型。\n",
    "尽管神经网络涵盖了更多更为丰富的模型，我们依然可以用描述神经网络的方式来描述线性模型，\n",
    "从而把线性模型看作一个神经网络。\n",
    "首先，我们用“层”符号来重写这个模型。\n",
    "\n",
    "### 神经网络图\n",
    "\n",
    "深度学习从业者喜欢绘制图表来可视化模型中正在发生的事情。\n",
    "在 :numref:`fig_single_neuron`中，我们将线性回归模型描述为一个神经网络。\n",
    "需要注意的是，该图只显示连接模式，即只显示每个输入如何连接到输出，隐去了权重和偏置的值。\n",
    "\n",
    "![线性回归是一个单层神经网络。](../img/singleneuron.svg)\n",
    ":label:`fig_single_neuron`\n",
    "\n",
    "在 :numref:`fig_single_neuron`所示的神经网络中，输入为$x_1, \\ldots, x_d$，\n",
    "因此输入层中的*输入数*（或称为*特征维度*，feature dimensionality）为$d$。\n",
    "网络的输出为$o_1$，因此输出层中的*输出数*是1。\n",
    "需要注意的是，输入值都是已经给定的，并且只有一个*计算*神经元。\n",
    "由于模型重点在发生计算的地方，所以通常我们在计算层数时不考虑输入层。\n",
    "也就是说， :numref:`fig_single_neuron`中神经网络的*层数*为1。\n",
    "我们可以将线性回归模型视为仅由单个人工神经元组成的神经网络，或称为单层神经网络。\n",
    "\n",
    "对于线性回归，每个输入都与每个输出（在本例中只有一个输出）相连，\n",
    "我们将这种变换（ :numref:`fig_single_neuron`中的输出层）\n",
    "称为*全连接层*（fully-connected layer）或称为*稠密层*（dense layer）。\n",
    "下一章将详细讨论由这些层组成的网络。\n",
    "\n",
    "### 生物学\n",
    "\n",
    "线性回归发明的时间（1795年）早于计算神经科学，所以将线性回归描述为神经网络似乎不合适。\n",
    "当控制学家、神经生物学家沃伦·麦库洛奇和沃尔特·皮茨开始开发人工神经元模型时，\n",
    "他们为什么将线性模型作为一个起点呢？\n",
    "我们来看一张图片 :numref:`fig_Neuron`：\n",
    "这是一张由*树突*（dendrites，输入终端）、\n",
    "*细胞核*（nucleus，CPU）组成的生物神经元图片。\n",
    "*轴突*（axon，输出线）和*轴突端子*（axon terminal，输出端子）\n",
    "通过*突触*（synapse）与其他神经元连接。\n",
    "\n",
    "![真实的神经元。](../img/neuron.svg)\n",
    ":label:`fig_Neuron`\n",
    "\n",
    "树突中接收到来自其他神经元（或视网膜等环境传感器）的信息$x_i$。\n",
    "该信息通过*突触权重*$w_i$来加权，以确定输入的影响（即，通过$x_i w_i$相乘来激活或抑制）。\n",
    "来自多个源的加权输入以加权和$y = \\sum_i x_i w_i + b$的形式汇聚在细胞核中，\n",
    "然后将这些信息发送到轴突$y$中进一步处理，通常会通过$\\sigma(y)$进行一些非线性处理。\n",
    "之后，它要么到达目的地（例如肌肉），要么通过树突进入另一个神经元。\n",
    "\n",
    "当然，许多这样的单元可以通过正确连接和正确的学习算法拼凑在一起，\n",
    "从而产生的行为会比单独一个神经元所产生的行为更有趣、更复杂，\n",
    "这种想法归功于我们对真实生物神经系统的研究。\n",
    "\n",
    "当今大多数深度学习的研究几乎没有直接从神经科学中获得灵感。\n",
    "我们援引斯图尔特·罗素和彼得·诺维格在他们的经典人工智能教科书\n",
    "*Artificial Intelligence:A Modern Approach* :cite:`Russell.Norvig.2016`\n",
    "中所说的：虽然飞机可能受到鸟类的启发，但几个世纪以来，鸟类学并不是航空创新的主要驱动力。\n",
    "同样地，如今在深度学习中的灵感同样或更多地来自数学、统计学和计算机科学。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 机器学习模型中的关键要素是训练数据、损失函数、优化算法，还有模型本身。\n",
    "* 矢量化使数学表达上更简洁，同时运行的更快。\n",
    "* 最小化目标函数和执行极大似然估计等价。\n",
    "* 线性回归模型也是一个简单的神经网络。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 假设我们有一些数据$x_1, \\ldots, x_n \\in \\mathbb{R}$。我们的目标是找到一个常数$b$，使得最小化$\\sum_i (x_i - b)^2$。\n",
    "    1. 找到最优值$b$的解析解。\n",
    "    1. 这个问题及其解与正态分布有什么关系?\n",
    "1. 推导出使用平方误差的线性回归优化问题的解析解。为了简化问题，可以忽略偏置$b$（我们可以通过向$\\mathbf X$添加所有值为1的一列来做到这一点）。\n",
    "    1. 用矩阵和向量表示法写出优化问题（将所有数据视为单个矩阵，将所有目标值视为单个向量）。\n",
    "    1. 计算损失对$w$的梯度。\n",
    "    1. 通过将梯度设为0、求解矩阵方程来找到解析解。\n",
    "    1. 什么时候可能比使用随机梯度下降更好？这种方法何时会失效？\n",
    "1. 假定控制附加噪声$\\epsilon$的噪声模型是指数分布。也就是说，$p(\\epsilon) = \\frac{1}{2} \\exp(-|\\epsilon|)$\n",
    "    1. 写出模型$-\\log P(\\mathbf y \\mid \\mathbf X)$下数据的负对数似然。\n",
    "    1. 请试着写出解析解。\n",
    "    1. 提出一种随机梯度下降算法来解决这个问题。哪里可能出错？（提示：当我们不断更新参数时，在驻点附近会发生什么情况）请尝试解决这个问题。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34204c85",
   "metadata": {},
   "source": [
    "1. 假设我们有一些数据$x_1, \\ldots, x_n \\in \\mathbb{R}$。我们的目标是找到一个常数$b$，使得最小化$\\sum_i (x_i - b)^2$。\n",
    "    1. 找到最优值$b$的解析解。\n",
    "    1. 这个问题及其解与正态分布有什么关系?\n",
    "\n",
    "我来解答这个关于最小化平方误差的问题。\n",
    "\n",
    "### 1. 最优值 $b$ 的解析解\n",
    "\n",
    "我们需要最小化目标函数：\n",
    "$$f(b) = \\sum_{i=1}^{n} (x_i - b)^2$$\n",
    "\n",
    "为了找到最优值，我们对 $b$ 求导并令其等于零：\n",
    "\n",
    "$$\\frac{df(b)}{db} = \\frac{d}{db} \\sum_{i=1}^{n} (x_i - b)^2$$\n",
    "\n",
    "$$= \\sum_{i=1}^{n} \\frac{d}{db} (x_i - b)^2$$\n",
    "\n",
    "$$= \\sum_{i=1}^{n} 2(x_i - b)(-1)$$\n",
    "\n",
    "$$= -2 \\sum_{i=1}^{n} (x_i - b)$$\n",
    "\n",
    "令导数等于零：\n",
    "$$-2 \\sum_{i=1}^{n} (x_i - b) = 0$$\n",
    "\n",
    "$$\\sum_{i=1}^{n} (x_i - b) = 0$$\n",
    "\n",
    "$$\\sum_{i=1}^{n} x_i - nb = 0$$\n",
    "\n",
    "$$b = \\frac{1}{n} \\sum_{i=1}^{n} x_i$$\n",
    "\n",
    "因此，最优值 $b^* = \\bar{x}$，即数据的样本均值。\n",
    "\n",
    "### 2. 与正态分布的关系\n",
    "\n",
    "这个问题及其解与正态分布有密切关系：\n",
    "\n",
    "1. **极大似然估计角度**：如果我们将 $x_i$ 视为从均值为 $b$、方差为 $\\sigma^2$ 的正态分布中独立采样的数据点，即 $x_i \\sim \\mathcal{N}(b, \\sigma^2)$，那么最大化似然函数等价于最小化 $\\sum_i (x_i - b)^2$。因此，样本均值 $\\bar{x}$ 就是均值参数 $b$ 的极大似然估计。\n",
    "\n",
    "2. **最小二乘估计**：在正态分布噪声假设下，最小二乘估计与极大似然估计是一致的。当我们假设观测数据 $x_i$ 是围绕真值 $b$ 的正态分布噪声时，使平方误差最小化的 $b$ 值正好是样本均值。\n",
    "\n",
    "3. **中心极限定理联系**：即使数据本身不是正态分布，当样本量足够大时，样本均值的分布会趋近于正态分布，这进一步加强了均值作为最优估计量的统计意义。\n",
    "\n",
    "这个结果体现了统计学中的一个基本原理：在正态分布假设下，样本均值是最优的估计量，无论是从最小化平方误差还是从极大似然估计的角度来看。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60af381a",
   "metadata": {},
   "source": [
    "1. 推导出使用平方误差的线性回归优化问题的解析解。为了简化问题，可以忽略偏置$b$（我们可以通过向$\\mathbf X$添加所有值为1的一列来做到这一点）。\n",
    "    1. 用矩阵和向量表示法写出优化问题（将所有数据视为单个矩阵，将所有目标值视为单个向量）。\n",
    "    1. 计算损失对$w$的梯度。\n",
    "    1. 通过将梯度设为0、求解矩阵方程来找到解析解。\n",
    "    1. 什么时候可能比使用随机梯度下降更好？这种方法何时会失效？\n",
    "\n",
    "我来推导使用平方误差的线性回归优化问题的解析解。\n",
    "\n",
    "### 1. 优化问题的矩阵表示\n",
    "\n",
    "忽略偏置 $b$，线性回归模型简化为：\n",
    "$$\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{w}$$\n",
    "\n",
    "其中：\n",
    "- $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ 是设计矩阵（包含所有训练样本的特征）\n",
    "- $\\mathbf{w} \\in \\mathbb{R}^{d}$ 是权重向量\n",
    "- $\\mathbf{y} \\in \\mathbb{R}^{n}$ 是目标值向量\n",
    "\n",
    "优化问题为最小化平方误差损失：\n",
    "$$L(\\mathbf{w}) = \\frac{1}{2} \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^2 = \\frac{1}{2} (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^\\top (\\mathbf{y} - \\mathbf{X}\\mathbf{w})$$\n",
    "\n",
    "展开得到：\n",
    "$$L(\\mathbf{w}) = \\frac{1}{2} (\\mathbf{y}^\\top\\mathbf{y} - 2\\mathbf{w}^\\top\\mathbf{X}^\\top\\mathbf{y} + \\mathbf{w}^\\top\\mathbf{X}^\\top\\mathbf{X}\\mathbf{w})$$\n",
    "\n",
    "### 2. 计算损失对 $\\mathbf{w}$ 的梯度\n",
    "\n",
    "对 $\\mathbf{w}$ 求梯度：\n",
    "$$\\nabla_{\\mathbf{w}} L(\\mathbf{w}) = \\frac{\\partial}{\\partial \\mathbf{w}} \\left[\\frac{1}{2} (\\mathbf{y}^\\top\\mathbf{y} - 2\\mathbf{w}^\\top\\mathbf{X}^\\top\\mathbf{y} + \\mathbf{w}^\\top\\mathbf{X}^\\top\\mathbf{X}\\mathbf{w})\\right]$$\n",
    "\n",
    "利用矩阵微分的性质：\n",
    "- $\\frac{\\partial}{\\partial \\mathbf{w}} (\\mathbf{w}^\\top\\mathbf{X}^\\top\\mathbf{y}) = \\mathbf{X}^\\top\\mathbf{y}$\n",
    "- $\\frac{\\partial}{\\partial \\mathbf{w}} (\\mathbf{w}^\\top\\mathbf{X}^\\top\\mathbf{X}\\mathbf{w}) = 2\\mathbf{X}^\\top\\mathbf{X}\\mathbf{w}$\n",
    "\n",
    "因此：\n",
    "$$\\nabla_{\\mathbf{w}} L(\\mathbf{w}) = \\frac{1}{2} (-2\\mathbf{X}^\\top\\mathbf{y} + 2\\mathbf{X}^\\top\\mathbf{X}\\mathbf{w}) = \\mathbf{X}^\\top\\mathbf{X}\\mathbf{w} - \\mathbf{X}^\\top\\mathbf{y}$$\n",
    "\n",
    "### 3. 通过将梯度设为0求解解析解\n",
    "\n",
    "令梯度为零：\n",
    "$$\\mathbf{X}^\\top\\mathbf{X}\\mathbf{w} - \\mathbf{X}^\\top\\mathbf{y} = 0$$\n",
    "\n",
    "得到正规方程：\n",
    "$$\\mathbf{X}^\\top\\mathbf{X}\\mathbf{w} = \\mathbf{X}^\\top\\mathbf{y}$$\n",
    "\n",
    "假设 $\\mathbf{X}^\\top\\mathbf{X}$ 可逆，则解析解为：\n",
    "$$\\mathbf{w}^* = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}$$\n",
    "\n",
    "这与笔记中给出的解析解一致。\n",
    "\n",
    "### 4. 与随机梯度下降的比较及失效情况\n",
    "\n",
    "### 解析解的优势：\n",
    "1. **精确解**：直接给出全局最优解，无需迭代\n",
    "2. **无超参数**：不需要选择学习率、批量大小等\n",
    "3. **确定性**：每次计算得到相同结果\n",
    "\n",
    "### 解析解的劣势及失效情况：\n",
    "1. **计算复杂度高**：需要计算 $\\mathbf{X}^\\top\\mathbf{X}$（复杂度 $O(nd^2)$）和求逆（复杂度 $O(d^3)$），当特征维度 $d$ 很大时计算代价昂贵\n",
    "2. **内存需求大**：需要存储和操作 $d \\times d$ 的矩阵\n",
    "3. **数值稳定性问题**：当 $\\mathbf{X}^\\top\\mathbf{X}$ 接近奇异（特征间存在强相关性）时，求逆可能不稳定\n",
    "4. **不可逆情况**：当特征数 $d$ 大于样本数 $n$，或存在线性相关特征时，$\\mathbf{X}^\\top\\mathbf{X}$ 不可逆\n",
    "5. **大数据集处理困难**：当数据无法完全载入内存时，解析解方法不适用\n",
    "\n",
    "### 何时使用解析解更好：\n",
    "- 数据集较小（样本数和特征数都不大）\n",
    "- 需要精确解而非近似解\n",
    "- 不需要在线学习或实时更新模型\n",
    "\n",
    "### 何时使用随机梯度下降更好：\n",
    "- 大数据集（样本数很大）\n",
    "- 高维特征空间\n",
    "- 需要在线学习或模型实时更新\n",
    "- 内存受限的情况\n",
    "- 可以接受近似解的情况\n",
    "\n",
    "总的来说，解析解适用于小规模问题，而随机梯度下降更适合大规模机器学习问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dedce6",
   "metadata": {},
   "source": [
    "1. 假定控制附加噪声$\\epsilon$的噪声模型是指数分布。也就是说，$p(\\epsilon) = \\frac{1}{2} \\exp(-|\\epsilon|)$\n",
    "    1. 写出模型$-\\log P(\\mathbf y \\mid \\mathbf X)$下数据的负对数似然。\n",
    "    1. 请试着写出解析解。\n",
    "    1. 提出一种随机梯度下降算法来解决这个问题。哪里可能出错？（提示：当我们不断更新参数时，在驻点附近会发生什么情况）请尝试解决这个问题。\n",
    "\n",
    "我来解答这个关于指数分布噪声模型的问题。\n",
    "\n",
    "### 1. 负对数似然\n",
    "\n",
    "给定噪声模型为指数分布：$p(\\epsilon) = \\frac{1}{2} \\exp(-|\\epsilon|)$\n",
    "\n",
    "线性回归模型为：$y = \\mathbf{w}^\\top \\mathbf{x} + b + \\epsilon$\n",
    "\n",
    "因此，给定 $\\mathbf{x}^{(i)}$ 观测到 $y^{(i)}$ 的概率为：\n",
    "$$p(y^{(i)}|\\mathbf{x}^{(i)}, \\mathbf{w}, b) = \\frac{1}{2} \\exp(-|y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b|)$$\n",
    "\n",
    "整个数据集的似然为：\n",
    "$$P(\\mathbf{y}|\\mathbf{X}, \\mathbf{w}, b) = \\prod_{i=1}^n \\frac{1}{2} \\exp(-|y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b|)$$\n",
    "\n",
    "负对数似然为：\n",
    "$$-\\log P(\\mathbf{y}|\\mathbf{X}, \\mathbf{w}, b) = -\\sum_{i=1}^n \\left[\\log\\left(\\frac{1}{2}\\right) - |y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b|\\right]$$\n",
    "\n",
    "$$= \\sum_{i=1}^n \\left[\\log(2) + |y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b|\\right]$$\n",
    "\n",
    "$$= n\\log(2) + \\sum_{i=1}^n |y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b|$$\n",
    "\n",
    "忽略常数项，优化问题变为最小化：\n",
    "$$L(\\mathbf{w}, b) = \\sum_{i=1}^n |y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b|$$\n",
    "\n",
    "这对应于使用绝对误差（L1损失）而不是平方误差（L2损失）。\n",
    "\n",
    "### 2. 解析解\n",
    "\n",
    "对于绝对误差损失的线性回归（也称为LAD回归或最小一乘回归），一般没有闭式解析解。\n",
    "\n",
    "这是因为绝对值函数 $|x|$ 在 $x=0$ 处不可导，使得我们无法通过简单地将梯度设为零来求解。\n",
    "\n",
    "不过，在某些特殊情况下可以找到解：\n",
    "- 当样本数 $n$ 为奇数且特征数 $d=1$ 时，最优的 $b$ 是残差的中位数\n",
    "- 在高维情况下，解通常通过线性规划方法求得\n",
    "\n",
    "一般来说，这个问题通常通过数值优化方法解决，如：\n",
    "1. 线性规划\n",
    "2. 迭代重加权最小二乘法\n",
    "3. 梯度下降法（次梯度方法）\n",
    "\n",
    "### 3. 随机梯度下降算法及问题\n",
    "\n",
    "### SGD算法：\n",
    "\n",
    "由于绝对值函数在0处不可导，我们需要使用次梯度：\n",
    "\n",
    "$$\\partial |x| = \\begin{cases} \n",
    "1 & \\text{if } x > 0 \\\\\n",
    "-1 & \\text{if } x < 0 \\\\\n",
    "[-1, 1] & \\text{if } x = 0\n",
    "\\end{cases}$$\n",
    "\n",
    "SGD更新规则为：\n",
    "$$\\mathbf{w} \\leftarrow \\mathbf{w} + \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\cdot \\text{sign}(y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b)$$\n",
    "\n",
    "$$b \\leftarrow b + \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\text{sign}(y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b)$$\n",
    "\n",
    "其中 $\\text{sign}(x) = \\begin{cases} 1 & x > 0 \\\\ -1 & x < 0 \\\\ 0 & x = 0 \\end{cases}$\n",
    "\n",
    "### 可能的问题：\n",
    "\n",
    "1. **收敛问题**：在最优点附近，梯度可能不会趋近于零，导致算法在最优解附近振荡\n",
    "2. **非平滑性**：在残差为零的点，梯度不唯一，可能导致算法行为不稳定\n",
    "3. **收敛速度**：相比平方误差损失，绝对误差损失的SGD可能收敛更慢\n",
    "\n",
    "### 解决方案：\n",
    "\n",
    "1. **使用平滑近似**：用光滑函数近似绝对值函数，如Huber损失：\n",
    "   $$L_\\delta(a) = \\begin{cases}\n",
    "   \\frac{1}{2}a^2 & \\text{for } |a| \\leq \\delta, \\\\\n",
    "   \\delta (|a| - \\frac{1}{2}\\delta), & \\text{otherwise}\n",
    "   \\end{cases}$$\n",
    "\n",
    "2. **学习率衰减**：随着训练进行逐渐减小学习率，减少振荡\n",
    "\n",
    "3. **动量方法**：使用动量项来平滑更新过程\n",
    "\n",
    "4. **次梯度方法**：专门处理非光滑优化问题的优化算法\n",
    "\n",
    "5. **Mini-batch平均**：使用较大批次来平均梯度，减少噪声影响\n",
    "\n",
    "这种基于绝对误差的回归对异常值更加鲁棒，但在优化上比平方误差回归更具挑战性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76f64dd",
   "metadata": {},
   "source": [
    "我来解释一下为什么偏导数的形式是这样的。\n",
    "\n",
    "对于线性回归的平方损失函数，我们需要计算损失函数相对于权重 $\\mathbf{w}$ 和偏置 $b$ 的偏导数。\n",
    "\n",
    "### 1. 损失函数定义\n",
    "\n",
    "单个样本的平方损失为：\n",
    "$$l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2}(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)})^2$$\n",
    "\n",
    "### 2. 计算权重的偏导数\n",
    "\n",
    "对 $\\mathbf{w}$ 求偏导数：\n",
    "$$\\partial_{\\mathbf{w}} l^{(i)}(\\mathbf{w}, b) = \\frac{\\partial}{\\partial \\mathbf{w}} \\left[\\frac{1}{2}(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)})^2\\right]$$\n",
    "\n",
    "使用链式法则：\n",
    "- 令 $u = \\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}$\n",
    "- 则 $\\frac{\\partial}{\\partial \\mathbf{w}} \\left[\\frac{1}{2}u^2\\right] = u \\cdot \\frac{\\partial u}{\\partial \\mathbf{w}}$\n",
    "- 而 $\\frac{\\partial u}{\\partial \\mathbf{w}} = \\frac{\\partial}{\\partial \\mathbf{w}}[\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}] = \\mathbf{x}^{(i)}$\n",
    "\n",
    "因此：\n",
    "$$\\partial_{\\mathbf{w}} l^{(i)}(\\mathbf{w}, b) = (\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}) \\cdot \\mathbf{x}^{(i)}$$\n",
    "\n",
    "### 3. 计算偏置的偏导数\n",
    "\n",
    "对 $b$ 求偏导数：\n",
    "$$\\partial_b l^{(i)}(\\mathbf{w}, b) = \\frac{\\partial}{\\partial b} \\left[\\frac{1}{2}(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)})^2\\right]$$\n",
    "\n",
    "同样使用链式法则：\n",
    "- $\\frac{\\partial u}{\\partial b} = \\frac{\\partial}{\\partial b}[\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}] = 1$\n",
    "\n",
    "因此：\n",
    "$$\\partial_b l^{(i)}(\\mathbf{w}, b) = (\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)})$$\n",
    "\n",
    "### 4. 小批量更新公式\n",
    "\n",
    "将这些偏导数代入随机梯度下降的更新公式中，就得到了代码片段中的形式：\n",
    "\n",
    "$$\\begin{aligned} \n",
    "\\mathbf{w} &\\leftarrow \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right),\\\\ \n",
    "b &\\leftarrow b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right). \n",
    "\\end{aligned}$$\n",
    "\n",
    "其中 $\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}$ 表示预测值与真实值之间的误差，这个误差乘以相应的特征值 $\\mathbf{x}^{(i)}$ 就是权重更新的方向和幅度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b321d9b9",
   "metadata": {},
   "source": [
    "有一个问题，就是求w的时候，为什么是整体减同一个值，而不是向量里面单独减一个值？\n",
    "\n",
    "这是一个很好的问题！让我来解释为什么在更新权重 $\\mathbf{w}$ 时是向量整体减去一个向量，而不是单独更新每个元素。\n",
    "\n",
    "## 1. 向量运算的本质\n",
    "\n",
    "权重 $\\mathbf{w}$ 是一个向量，而不是标量。在梯度下降中，我们需要对整个参数向量进行更新：\n",
    "\n",
    "$$\\mathbf{w} \\leftarrow \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)$$\n",
    "\n",
    "这里的 $\\mathbf{x}^{(i)}$ 是一个向量，$(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)})$ 是一个标量，所以乘积 $\\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)$ 仍然是一个向量。\n",
    "\n",
    "## 2. 为什么是向量减法而不是单独元素更新\n",
    "\n",
    "让我们展开这个表达式来理解：\n",
    "\n",
    "假设我们有 $d$ 个特征，那么：\n",
    "- $\\mathbf{w} = [w_1, w_2, ..., w_d]^\\top$\n",
    "- $\\mathbf{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}, ..., x_d^{(i)}]^\\top$\n",
    "\n",
    "对于单个样本，梯度是：\n",
    "$$\\frac{\\partial l^{(i)}}{\\partial \\mathbf{w}} = \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)$$\n",
    "\n",
    "展开这个向量：\n",
    "$$\\frac{\\partial l^{(i)}}{\\partial \\mathbf{w}} = \\begin{bmatrix} \n",
    "x_1^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right) \\\\\n",
    "x_2^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "x_d^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "所以更新公式实际上是同时更新所有权重：\n",
    "$$\\begin{bmatrix} \n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "\\vdots \\\\\n",
    "w_d\n",
    "\\end{bmatrix}\n",
    "\\leftarrow\n",
    "\\begin{bmatrix} \n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "\\vdots \\\\\n",
    "w_d\n",
    "\\end{bmatrix}\n",
    "- \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\begin{bmatrix} \n",
    "x_1^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right) \\\\\n",
    "x_2^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "x_d^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "## 3. 这样设计的原因\n",
    "\n",
    "1. **数学一致性**：梯度 $\\frac{\\partial L}{\\partial \\mathbf{w}}$ 本身就是一个向量，与参数 $\\mathbf{w}$ 维度相同，所以更新操作必须是向量减去向量。\n",
    "\n",
    "2. **物理意义**：每个权重的更新量取决于对应特征的值和预测误差的乘积。不同特征对预测误差的贡献不同，因此每个权重的更新量也应该不同。\n",
    "\n",
    "3. **计算效率**：使用向量运算可以利用高效的线性代数库（如BLAS），比逐个更新元素要快得多。\n",
    "\n",
    "## 4. 实际例子\n",
    "\n",
    "假设有2个特征的样本：\n",
    "- $\\mathbf{x}^{(1)} = [2, 3]^\\top$\n",
    "- 预测误差：$\\mathbf{w}^\\top \\mathbf{x}^{(1)} + b - y^{(1)} = 0.5$\n",
    "\n",
    "那么梯度为：\n",
    "$$\\frac{\\partial l^{(1)}}{\\partial \\mathbf{w}} = [2, 3]^\\top \\times 0.5 = [1, 1.5]^\\top$$\n",
    "\n",
    "如果学习率 $\\eta = 0.1$，则更新为：\n",
    "$$\\mathbf{w} \\leftarrow \\mathbf{w} - 0.1 \\times [1, 1.5]^\\top = [w_1 - 0.1, w_2 - 0.15]^\\top$$\n",
    "\n",
    "可以看到，$w_1$ 和 $w_2$ 的更新量是不同的，这正是我们希望的：不同特征对预测误差的贡献不同，应该有不同的更新幅度。\n",
    "\n",
    "总结：使用向量整体更新而不是单独元素更新，既保证了数学上的正确性，又体现了不同特征应该有不同更新量的物理意义，同时还提高了计算效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c59afc9",
   "metadata": {
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/1775)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
