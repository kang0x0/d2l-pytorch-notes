{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6570c16f",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 深度循环神经网络\n",
    "\n",
    ":label:`sec_deep_rnn`\n",
    "\n",
    "到目前为止，我们只讨论了具有一个单向隐藏层的循环神经网络。\n",
    "其中，隐变量和观测值与具体的函数形式的交互方式是相当随意的。\n",
    "只要交互类型建模具有足够的灵活性，这就不是一个大问题。\n",
    "然而，对一个单层来说，这可能具有相当的挑战性。\n",
    "之前在线性模型中，我们通过添加更多的层来解决这个问题。\n",
    "而在循环神经网络中，我们首先需要确定如何添加更多的层，\n",
    "以及在哪里添加额外的非线性，因此这个问题有点棘手。\n",
    "\n",
    "事实上，我们可以将多层循环神经网络堆叠在一起，\n",
    "通过对几个简单层的组合，产生了一个灵活的机制。\n",
    "特别是，数据可能与不同层的堆叠有关。\n",
    "例如，我们可能希望保持有关金融市场状况\n",
    "（熊市或牛市）的宏观数据可用，\n",
    "而微观数据只记录较短期的时间动态。\n",
    "\n",
    " :numref:`fig_deep_rnn`描述了一个具有$L$个隐藏层的深度循环神经网络，\n",
    "每个隐状态都连续地传递到当前层的下一个时间步和下一层的当前时间步。\n",
    "\n",
    "![深度循环神经网络结构](../img/deep-rnn.svg)\n",
    ":label:`fig_deep_rnn`\n",
    "\n",
    "## 函数依赖关系\n",
    "\n",
    "我们可以将深度架构中的函数依赖关系形式化，\n",
    "这个架构是由 :numref:`fig_deep_rnn`中描述了$L$个隐藏层构成。\n",
    "后续的讨论主要集中在经典的循环神经网络模型上，\n",
    "但是这些讨论也适应于其他序列模型。\n",
    "\n",
    "假设在时间步$t$有一个小批量的输入数据\n",
    "$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$\n",
    "（样本数：$n$，每个样本中的输入数：$d$）。\n",
    "同时，将$l^\\mathrm{th}$隐藏层（$l=1,\\ldots,L$）\n",
    "的隐状态设为$\\mathbf{H}_t^{(l)}  \\in \\mathbb{R}^{n \\times h}$\n",
    "（隐藏单元数：$h$），\n",
    "输出层变量设为$\\mathbf{O}_t \\in \\mathbb{R}^{n \\times q}$\n",
    "（输出数：$q$）。\n",
    "设置$\\mathbf{H}_t^{(0)} = \\mathbf{X}_t$，\n",
    "第$l$个隐藏层的隐状态使用激活函数$\\phi_l$，则：\n",
    "\n",
    "$$\\mathbf{H}_t^{(l)} = \\phi_l(\\mathbf{H}_t^{(l-1)} \\mathbf{W}_{xh}^{(l)} + \\mathbf{H}_{t-1}^{(l)} \\mathbf{W}_{hh}^{(l)}  + \\mathbf{b}_h^{(l)}),$$\n",
    ":eqlabel:`eq_deep_rnn_H`\n",
    "\n",
    "其中，权重$\\mathbf{W}_{xh}^{(l)} \\in \\mathbb{R}^{h \\times h}$，\n",
    "$\\mathbf{W}_{hh}^{(l)} \\in \\mathbb{R}^{h \\times h}$和\n",
    "偏置$\\mathbf{b}_h^{(l)} \\in \\mathbb{R}^{1 \\times h}$\n",
    "都是第$l$个隐藏层的模型参数。\n",
    "\n",
    "最后，输出层的计算仅基于第$l$个隐藏层最终的隐状态：\n",
    "\n",
    "$$\\mathbf{O}_t = \\mathbf{H}_t^{(L)} \\mathbf{W}_{hq} + \\mathbf{b}_q,$$\n",
    "\n",
    "其中，权重$\\mathbf{W}_{hq} \\in \\mathbb{R}^{h \\times q}$和偏置$\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$都是输出层的模型参数。\n",
    "\n",
    "与多层感知机一样，隐藏层数目$L$和隐藏单元数目$h$都是超参数。\n",
    "也就是说，它们可以由我们调整的。\n",
    "另外，用门控循环单元或长短期记忆网络的隐状态\n",
    "来代替 :eqref:`eq_deep_rnn_H`中的隐状态进行计算，\n",
    "可以很容易地得到深度门控循环神经网络或深度长短期记忆神经网络。\n",
    "\n",
    "## 简洁实现\n",
    "\n",
    "实现多层循环神经网络所需的许多逻辑细节在高级API中都是现成的。\n",
    "简单起见，我们仅示范使用此类内置函数的实现方式。\n",
    "以长短期记忆网络模型为例，\n",
    "该代码与之前在 :numref:`sec_lstm`中使用的代码非常相似，\n",
    "实际上唯一的区别是我们指定了层的数量，\n",
    "而不是使用单一层这个默认值。\n",
    "像往常一样，我们从加载数据集开始。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0a80da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import os\n",
    "import collections\n",
    "import random\n",
    "import torch\n",
    "\n",
    "class Vocab:  #@save\n",
    "    \"\"\"文本词表\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        # 按出现频率排序\n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        # 未知词元的索引为0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # 未知词元的索引为0\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs\n",
    "    \n",
    "def count_corpus(tokens):  #@save\n",
    "    \"\"\"统计词元的频率\"\"\"\n",
    "    # 这里的tokens是1D列表或2D列表\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # 将词元列表展平成一个列表\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)\n",
    "\n",
    "def download_time_machine(data_dir='../data', filename='timemachine.txt'):\n",
    "    \"\"\"下载时间机器文本文件\"\"\"\n",
    "    url = 'http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt'\n",
    "    \n",
    "    # 创建数据目录（如果不存在）\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    # 完整文件路径\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "    \n",
    "    # 如果文件不存在，则下载\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f'Downloading {filename} from {url}...')\n",
    "        response = requests.get(url)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f'File saved to {file_path}')\n",
    "    else:\n",
    "        print(f'File already exists at {file_path}')\n",
    "    \n",
    "    return file_path\n",
    "\n",
    "def read_time_machine():\n",
    "    \"\"\"将时间机器数据集加载到文本行的列表中\"\"\"\n",
    "    # 下载文件并获取路径\n",
    "    file_path = download_time_machine()\n",
    "    \n",
    "    # 读取文件内容\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # 处理每一行：只保留字母，转为小写，去除首尾空格\n",
    "    processed_lines = [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "    \n",
    "    return processed_lines\n",
    "\n",
    "def tokenize(lines, token='word'):  #@save\n",
    "    \"\"\"将文本行拆分为单词或字符词元\"\"\"\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('错误：未知词元类型：' + token)\n",
    "\n",
    "def load_corpus_time_machine(max_tokens=-1):  #@save\n",
    "    \"\"\"返回时光机器数据集的词元索引列表和词表\"\"\"\n",
    "    lines = read_time_machine()\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    vocab = Vocab(tokens)\n",
    "    # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，\n",
    "    # 所以将所有文本行展平到一个列表中\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab\n",
    "\n",
    "def seq_data_iter_random(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"使用随机抽样生成一个小批量子序列\"\"\"\n",
    "    # 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1\n",
    "    corpus = corpus[random.randint(0, num_steps - 1):]\n",
    "    # 减去1，是因为我们需要考虑标签\n",
    "    num_subseqs = (len(corpus) - 1) // num_steps\n",
    "    # 长度为num_steps的子序列的起始索引\n",
    "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
    "    # 在随机抽样的迭代过程中，\n",
    "    # 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻\n",
    "    random.shuffle(initial_indices)\n",
    "\n",
    "    def data(pos):\n",
    "        # 返回从pos位置开始的长度为num_steps的序列\n",
    "        return corpus[pos: pos + num_steps]\n",
    "\n",
    "    num_batches = num_subseqs // batch_size\n",
    "    for i in range(0, batch_size * num_batches, batch_size):\n",
    "        # 在这里，initial_indices包含子序列的随机起始索引\n",
    "        initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
    "        X = [data(j) for j in initial_indices_per_batch]\n",
    "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
    "        yield torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"使用顺序分区生成一个小批量子序列\"\"\"\n",
    "    # 从随机偏移量开始划分序列\n",
    "    offset = random.randint(0, num_steps)\n",
    "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
    "    Xs = torch.tensor(corpus[offset: offset + num_tokens])\n",
    "    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
    "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
    "    num_batches = Xs.shape[1] // num_steps\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\n",
    "        X = Xs[:, i: i + num_steps]\n",
    "        Y = Ys[:, i: i + num_steps]\n",
    "        yield X, Y\n",
    "\n",
    "class SeqDataLoader:  #@save\n",
    "    \"\"\"加载序列数据的迭代器\"\"\"\n",
    "    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
    "        if use_random_iter:\n",
    "            self.data_iter_fn = seq_data_iter_random\n",
    "        else:\n",
    "            self.data_iter_fn = seq_data_iter_sequential\n",
    "        self.corpus, self.vocab = load_corpus_time_machine(max_tokens)\n",
    "        self.batch_size, self.num_steps = batch_size, num_steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)\n",
    "\n",
    "def load_data_time_machine(batch_size, num_steps,  #@save\n",
    "                           use_random_iter=False, max_tokens=10000):\n",
    "    \"\"\"返回时光机器数据集的迭代器和词表\"\"\"\n",
    "    data_iter = SeqDataLoader(\n",
    "        batch_size, num_steps, use_random_iter, max_tokens)\n",
    "    return data_iter, data_iter.vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c3000b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:24:40.256397Z",
     "iopub.status.busy": "2023-08-18T07:24:40.255737Z",
     "iopub.status.idle": "2023-08-18T07:24:43.344661Z",
     "shell.execute_reply": "2023-08-18T07:24:43.343759Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists at ../data\\timemachine.txt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# from d2l import torch as d2l\n",
    "\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83fd896",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "像选择超参数这类架构决策也跟 :numref:`sec_lstm`中的决策非常相似。\n",
    "因为我们有不同的词元，所以输入和输出都选择相同数量，即`vocab_size`。\n",
    "隐藏单元的数量仍然是$256$。\n",
    "唯一的区别是，我们现在(**通过`num_layers`的值来设定隐藏层数**)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6925245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bb697f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "#@save\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"循环神经网络模型\"\"\"\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1\n",
    "        if not self.rnn.bidirectional:\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "        else:\n",
    "            self.num_directions = 2\n",
    "            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        # 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)\n",
    "        # 它的输出形状是(时间步数*批量大小,词表大小)。\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # nn.GRU以张量作为隐状态\n",
    "            return  torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens),\n",
    "                                device=device)\n",
    "        else:\n",
    "            # nn.LSTM以元组作为隐状态\n",
    "            return (torch.zeros((\n",
    "                self.num_directions * self.rnn.num_layers,\n",
    "                batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((\n",
    "                        self.num_directions * self.rnn.num_layers,\n",
    "                        batch_size, self.num_hiddens), device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9ce730c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:24:43.348714Z",
     "iopub.status.busy": "2023-08-18T07:24:43.348349Z",
     "iopub.status.idle": "2023-08-18T07:24:45.383961Z",
     "shell.execute_reply": "2023-08-18T07:24:45.383098Z"
    },
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "vocab_size, num_hiddens, num_layers = len(vocab), 256, 2\n",
    "num_inputs = vocab_size\n",
    "device = try_gpu()\n",
    "lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)\n",
    "model = RNNModel(lstm_layer, len(vocab))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7a543d",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "## [**训练**]与预测\n",
    "\n",
    "由于使用了长短期记忆网络模型来实例化两个层，因此训练速度被大大降低了。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e04477e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "\n",
    "class Timer:  #@save\n",
    "    \"\"\"记录多次运行时间\"\"\"\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"启动计时器\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"停止计时器并将时间记录在列表中\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"返回平均时间\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"返回时间总和\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"返回累计时间\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()\n",
    "\n",
    "class Accumulator:  #@save\n",
    "    \"\"\"在n个变量上累加\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def grad_clipping(net, theta):  #@save\n",
    "    \"\"\"裁剪梯度\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm\n",
    "\n",
    "#@save\n",
    "def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n",
    "    \"\"\"训练网络一个迭代周期（定义见第8章）\"\"\"\n",
    "    state, timer = None, Timer()\n",
    "    metric = Accumulator(2)  # 训练损失之和,词元数量\n",
    "    for X, Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            # 在第一次迭代或使用随机抽样时初始化state\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "        else:\n",
    "            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
    "                # state对于nn.GRU是个张量\n",
    "                state.detach_()\n",
    "            else:\n",
    "                # state对于nn.LSTM或对于我们从零开始实现的模型是个张量\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        y = Y.T.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat, state = net(X, state)\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            # 因为已经调用了mean函数\n",
    "            updater(batch_size=1)\n",
    "        metric.add(l * y.numel(), y.numel())\n",
    "    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d91c99e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):  #@save\n",
    "    \"\"\"小批量随机梯度下降\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()\n",
    "\n",
    "from IPython import display\n",
    "class Animator:  #@save\n",
    "    \"\"\"在动画中绘制数据\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        # 增量地绘制多条线\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        \n",
    "        # 替换 d2l.use_svg_display()\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.rcParams['figure.figsize'] = figsize\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # 替换 d2l.set_axes，使用 lambda 函数直接配置参数\n",
    "        self.config_axes = lambda: self._set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def _set_axes(self, ax, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "        \"\"\"设置坐标轴标签、范围和比例\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        # 设置坐标轴标签\n",
    "        if xlabel:\n",
    "            ax.set_xlabel(xlabel)\n",
    "        if ylabel:\n",
    "            ax.set_ylabel(ylabel)\n",
    "        # 设置坐标轴范围\n",
    "        if xlim:\n",
    "            ax.set_xlim(xlim)\n",
    "        if ylim:\n",
    "            ax.set_ylim(ylim)\n",
    "        # 设置坐标轴比例\n",
    "        if xscale:\n",
    "            ax.set_xscale(xscale)\n",
    "        if yscale:\n",
    "            ax.set_yscale(yscale)\n",
    "        # 设置图例\n",
    "        if legend:\n",
    "            ax.legend(legend)\n",
    "        # 添加网格\n",
    "        ax.grid(True)\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # 向图表中添加多个数据点\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "def predict_ch8(prefix, num_preds, net, vocab, device):  #@save\n",
    "    \"\"\"在prefix后面生成新字符\"\"\"\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
    "    for y in prefix[1:]:  # 预热期\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):  # 预测num_preds步\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])\n",
    "\n",
    "#@save\n",
    "def train_ch8(net, train_iter, vocab, lr, num_epochs, device,\n",
    "              use_random_iter=False):\n",
    "    \"\"\"训练模型（定义见第8章）\"\"\"\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    animator = Animator(xlabel='epoch', ylabel='perplexity',\n",
    "                            legend=['train'], xlim=[10, num_epochs])\n",
    "    # 初始化\n",
    "    if isinstance(net, nn.Module):\n",
    "        updater = torch.optim.SGD(net.parameters(), lr)\n",
    "    else:\n",
    "        updater = lambda batch_size: sgd(net.params, lr, batch_size)\n",
    "    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)\n",
    "    # 训练和预测\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl, speed = train_epoch_ch8(\n",
    "            net, train_iter, loss, updater, device, use_random_iter)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict('time traveller'))\n",
    "            animator.add(epoch + 1, [ppl])\n",
    "    print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')\n",
    "    print(predict('time traveller'))\n",
    "    print(predict('traveller'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97524e5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:24:45.388179Z",
     "iopub.status.busy": "2023-08-18T07:24:45.387601Z",
     "iopub.status.idle": "2023-08-18T07:25:17.829775Z",
     "shell.execute_reply": "2023-08-18T07:25:17.828890Z"
    },
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "困惑度 1.0, 25550.6 词元/秒 cpu\n",
      "time travelleryou can show black is white by argument said filby\n",
      "travelleryou can show black is white by argument said filby\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV8AAAD/CAYAAABW3tXbAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjUsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvWftoOwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAK5NJREFUeJzt3Qd4VFXaB/B/pqQnhJJAIKEZWkKRJl2QKihNVxDQRcGyn+gqCIoNpKisrIhldVdRRBTZFVEQCIJUQbqUAEpvgdACpBBS537Pe4aJSQgQkpncuZP/z+c6d0pmzpmEd86ce+77emmapoGIiEqVqXRfjoiIBIMvEZEOGHyJiHTA4EtEpAMGXyIiHTD4EhHpgMGXiEgHFng4m82GU6dOISgoCF5eXno3h4gMQtM0pKSkoGrVqjCZnD9O9fjgK4E3MjJS72YQkUGdOHECERERTn9ejw++MuJ1vIHBwcEwqqysLCxbtgzdu3eH1WqFJ2HfjMuT+3fhwgXUqlUrN4Y4m8cHX8dUgwReowdff39/1QdP+yNn34zLk/uXlZWlLl01XckDbkREOmDwJSLSAYMvEZEOPH7Ol8jTl1JmZmbqOi9qsViQnp6OnJwcGInVaoXZbNbt9ctM8L2UlgnNkgV4yQQ64GMxwcei3xtPVFISdI8cOaICsJ5rYatUqaJWExlxHX1ISIhqvx5tLzPBt/0/VsHk45973Wr2wvD2tTGqW114Wzj7QsYiQS8hIUGN3GQduytOAigKCfypqakIDAzUrQ3Fff/S0tJw9uxZdT08PBylrcwE34KycjT8e80h/HroPN57sClqVQrQu0lERZadna2Ch5x9JUu99J728PX1NVTwFX5+fupSAnBYWFipT0GUmeC7fVw3BAUFQ4O9atLK389i7Pw47IpPwj3v/4KJfRvi/mbVDPnVicoex/yqt7e33k0xNP+rH1wyd13awddYH1UlYDWb1PSCzPPK1rNROGKf7YDWtSsgLTMHo7/dib/P3YGkK/aF1URGwMGCcd+/MhN8C1M1xA9fP9YaY3rUg9nkhR93nsLA/2xAjo01RYnItcp08BUSdEfcFYV5f2uDIB8L/jidgq1HL+jdLCLycGU++Do0rV4e3WOqqP3Y3af1bg4RFUHNmjUxffp0GBGDbx69GjmCbwJsnHogcolOnTrhueeec8pzbdmyBU888QSMiME3j/Z1KiHQx4IzyRnYfuKS3s0hKpM0TVNL6YoiNDRU16V2JcHgm4esgujaIEztx8Yl6N0cols7aSAzW5dNXruoHnnkEaxZswbvvfeeWmkg2xdffKEuY2Nj0bx5c/j4+GDdunU4dOgQ+vbti8qVK6uTOFq2bImff/75htMO8jwzZsxA//79VVCuU6cOFi5cCHdUZtb5FpUsQfthxyk17/vKPQ24lIcM4UpWDqLH/aTLa28Y1RrlivhYCbr79+9Hw4YNMXHiRHXbnj171OXYsWPxz3/+E7Vr10b58uXVKcu9evXCG2+8oQLyl19+id69e2Pfvn2oXr36dV9jwoQJePvttzF16lR88MEHGDJkCI4dO4YKFSrAnXDkW0DHuqHw9zbj5KUr6gQMInKecuXKqRND/P39VU4F2RwnN0gw7tatG2677TYVKJs0aYInn3xSBWoZwU6aNEndd7ORrIyuBw0ahKioKLz55pvq9OfNmzfD3XDkW4Cv1Yy76odh8a4ELNmdgCaRIXo3ieim/Kxm7J3YQ5fTi7OuXHbKc7Vo0SLfdQmar7/+OhYvXqzyWMg88JUrV3D8+PEbPk/jxo1z9wMCAlSVDUcOB3fC4FuIXg3DVfCNjTuNsXfX59QDuT35G/X3tugSfJPTnfPvIyAgf36V0aNHY/ny5WoqQkaxkovhL3/5y01TaBYsZyTvjZ6Z366HwbcQneqFwtdqwvELadibkIyYqkWd0SKim5Fph5wi5P5dv369mkKQg2eOkfDRo0fhKTjnW4gAH4ua+xUy+iUi55EVCps2bVKB9Pz589cdlco87/z587Fjxw7s3LkTgwcPdssRbHEx+F5Hr0b2/J5L4hJuaSkNEd2YTCeYzWZER0erdbrXm8OdNm2aWvXQtm1btcqhR48eaNasGTwFpx2uo3P9MHibTTh8/jL2n0lFvSpBejeJyCPUrVsXGzZsyHebTC8UNkJeuXJlvttGjBiR73rBaYjCBkqXLrnnCVMc+V5HkK8Vd9atlDv6JSJyJgbfG+jZMDw31wMRkTMx+N5A1waVYTF5qWmHg2dT9W4OEXkQBt8bKOdvRbso+9TDvG3xejeHiDwIg+9NDLrDfg75Z+sO4+DZFL2bQ5QPV+KUjJ5L17ja4SZ6xFRWKx9W/nEWL82Pw3+faAOTiWe8kb7kLC45c+vcuXNquZZeZ2E6qhenp6cbqnqxpmmq3fL+Sbv1KESqa/Bdu3atyjy0bds2de72999/j379+uVbfjJr1qx8PyNr/ZYuXVpqbZQ/6ol9Y7DxcCK2HL2IuVtOYHCr62dUIioNsk42IiIC8fHxup71JUFM8i3Iqb9GPA3f399fZUjT44ND1+B7+fJllblo2LBhuO+++wp9zN13342ZM2fmXpfUcqUtorw/nu9eD5MW7cVbsb+rnL9hwb6l3g6ivCTHrZwFJmXP9SKvLYOoO++885qcCkb4ALNYLLp9aOgafHv27Km2G5FgK2nn9PZI25pYsOOkSjM5YdFe/Guw55xpQ8YlAcSRklGv15dsY76+voYLvnpz+znf1atXIywsTJ1m2LlzZ0yePBkVK1a87uMzMjLU5pCcnJz7CV3SEcLE3g1w/382qYxnfRqfQud69vwPpcHRdj1HOa7CvhmXJ/cvy8V98tLc5HCpDP0LzvnOnTtXzcnUqlVLlRR5+eWX1VctOTXxep/2kv9TMtkXNGfOHKfUelpw1ISVCSaU99bw0u058NFv0EFELpSWlqaS+SQlJamcwGUq+BZ0+PBhlcle6jh16dKlyCPfyMhIlT3JGW+g1Ky654NfEX8pHY+2rYGXe9ZDaX0KS25TyfTvaV/v2Dfj8uT+JSYmIjw83GXB1+2nHfKS2k6VKlXCwYMHrxt8ZY64sINy8ofhjD+OclYrJvdvhEdmbsGsDcfwTJe6qBBQestUnNUPd8S+GZcn9s/q4v4YZ2EeoJbVOD6N9NSpXhhqhwbApgE7WWKeiIwWfCUzvSRKlk0cOXJE7Ut+T7lvzJgx2Lhxo1rHuGLFClVGWsqJyFpfvTWJsNd22xnP4EtEBgu+W7duRdOmTdUmRo0apfbHjRunDqjt2rULffr0Ufk/hw8fjubNm+OXX37RZa1vQY0j7KWFWOGYiAw359upU6cbnpv+008/wV01vjry3RV/SfXBiGf3EJF+DDXn605iqgbDbPLC+dRMnEpK17s5RGQwDL7F5Gs1o25le2mhXTzoRkS3iMG3BJpcnffdyXlfIrpFDL5OmPeNO8mRLxHdGgZfJ614sMmiXyKiImLwLQEpJ+9jMSElPRtHEy/r3RwiMhAG3xKwmk2Irmo/55vrfYnoVjD4lhDPdCOi4mDwLSGe6UZExcHg66QVD3tOJSE7R79KqERkLAy+JVS7UgCCfCxIz7Jh/5lUvZtDRAbB4FtCUka+YTXH1APnfYmoaBh8naBxJM90I6Jbw+DrxBUPHPkSUVEx+DpxxcO+0ylIz8rRuzlEZAAMvk5QLcQPFQO8kW3TsDfBXqqeiOhGGHydQBKp5673ZXpJIioCBl+nV7bgQTciujkGXydpkrvigSNfIro5Bl8nj3wPn7+MlPQsvZtDRG6OwddJKgX6qANvUg807iSnHojoxhh8nYhJdojIpcF35syZSEtLK86PerRm1cury7mbj+NKJtf7EpGTg+/YsWNRpUoVDB8+HL/++mtxnsIjDbwjElWCfXE0MQ3Tlu/TuzlE5GnB9+TJk5g1axbOnz+PTp06oX79+vjHP/6B06dPoywL9rXizfsaqv3P1h3Bb8cv6t0kIvKk4GuxWNC/f38sWLAAJ06cwOOPP46vv/4a1atXR58+fdTtNlvZzG3buX5l3Ne0GqSe5gvzdvF0YyJyzQG3ypUro3379mjTpg1MJhPi4uIwdOhQ3HbbbVi9ejXKonG9o9Xqh4NnU/H+igN6N4eIPCn4njlzBv/85z8RExOjph6Sk5OxaNEiHDlyRE1LDBgwQAXhsijE3xuT+9mnH/6z9jDiuPqBiJwRfHv37o3IyEh88cUXaspBgu0333yDrl27qvsDAgLw/PPPqymJsuruhlVwT+Nw5Ng0jJm3E5nZZXMahogKZ0ExhIWFYc2aNWqq4XpCQ0PVKLgsm9gnBhsOJeKP0yn4aPVBPNe1rt5NIiIjj3w7duyIZs2aXXN7ZmYmvvzyy9xMXzVq1EBZVjHQB6/3iVH7H648iNi4BL2bRERGDr6PPvookpKuncdMSUlR99GfejcOV6sfJNfviDm/Yd62eL2bRERGDb6apqmRbUHx8fEoV85+ii3Zyfs09YEmGNAiQi0/G/3tTsz69ajezSIiI835Nm3aVAUT2bp06aLW+zrk5OSoOd67777bFe00NLPJC1Pua4wAHwtmrj+K8Qv3IDUjGyPuitK7aURkhODbr18/dbljxw706NEDgYGBufd5e3ujZs2auP/++53fSg8pMT/u3mgE+VrV2t+pP+1TAfiFHvUK/RZBRJ7tloLv+PHj1aUE2YEDB8LX19dV7fJIEmRHdauLQB8z3lzyBz5efQg+FhNXQRCVQcWa85WTJxh4i++JO2/DpL72VRAfrTqEExeYIY6orCly8K1QoYJKpCPKly+vrl9vo5t7qHUNtIuqiMwcm5qCIKKypcjTDu+++y6CgoJy9zlPWTLy/r3cqwHu/WAdFu48hWHta+H2SHspIiLyfEUOvnnzNDzyyCOuak+ZElO1HO5rGoHvfovHG4v34n9PtuGHGlEZUaw5X8npUJjs7Gy89NJLJW1TmTK6R134Wk3YcvQiftpzRu/mEJE7B9+///3veOCBB3Dx4p/Jwvft24dWrVqpBDtUdOHl/PB4h9pqf0rs70zAQ1RGFCv4bt++XZ3N1qhRIyxfvhz/+te/VK4HqWixc+fOIj/P2rVrVYa0qlWrqq/bP/zwwzVn0o0bNw7h4eHw8/NTWdMOHPC8/LhPdrwNlQK9VfmhOZuO6d0cInLX4CuJ0tevX4/77rtPndE2cuRIzJgxQ1WzuJXTiy9fvowmTZqo4F2Yt99+G++//z7+/e9/Y9OmTSpVpZzckZ6eDk8S6GPByG72tb7vrTiApCtZejeJiNw1mfrixYsxd+5clVYyJCQEn332GU6dOnVLz9GzZ09MnjxZlSQqSEa906dPx6uvvoq+ffuicePGKmOavEbBEbInGNgiEnXCAnExLQsfrTqod3OIyB3z+T755JOqgOYbb7yBUaNGqaoWw4YNU9MQH3/8sapiUVKSJ0IKcjoStAsZVcu88oYNG/Dggw8W+nMZGRlqc5AKGyIrK0tt7uyFHnXw+Ozt+Hz9EXRvEIrGEX9+i3C03d37UBzsm3F5cv+yXNynYgVfmXKQaQCZMhBSRn7JkiVq+kCCsDOCr6MSstSIy0uu36hK8ltvvYUJEyZcc/uyZcvg7+8Pd6ZpQHSICXsvmfDQjI14OiYHEQH5HyNz7J6KfTMuT+xfWlqa+wXfbdu2wcfH55rbR4wYkW+kqgdZ6iaj8bwjXyl51L17dwQHB8PddeyajWGztmH7iSTMOOiHr4a1QN3KQepTWP7Au3XrBqvVCk/CvhmXJ/cvMTHR/YKvBN5Dhw5h5syZ6vK9995TpYViY2NV+XhnkNG0kCkNWe3gINdvv/32G7atsA8G+cMwwh9HeasVs4a3wsMzNmFnfBKGfrENc59ogxrlfQzVj+Jg34zLE/tndXF/inXATeq3yfyuTD3Mnz8fqamp6nZZZubIfFZStWrVUgF4xYoV+Uax8po3qh3nCYJ9rfhyWCvEVA3G+dRMDP50I44mXta7WUSkd/AdO3asWqUgXzckj69D586dsXHjxiI/jwRtyQ0sm+Mgm+wfP35crft97rnn1OssXLgQcXFx+Otf/6rWBDvyCnuycv5WfDW8FepXCcLZlAw8/PlWJHrWCjuiMq1YwVcCYWHLw2TqwZH5rCi2bt2qqmPIJmSuVvblxArxwgsv4JlnnsETTzyBli1bqmC9dOnSMpPOsnyAN756rBWiwgJxOjkDn+0zq1L0RFRGg6+s601ISCj0zLdq1aoV+Xk6deqk1vMW3By5I2T0O3HiRLW6QU6s+Pnnn1G3btlKPF4p0AdfP9YK5fwsOJnmhW+2nNC7SUSkV/CVNbYvvviiCooSIG02m1p+Nnr0aDU1QM5VOdgXI7vWUfvv/nwQ51P/XMdMRGUo+L755psqj4Ms4ZKpgOjoaNx5551o27atOiONnO/BFhGICNCQnJ6Nf8T+oXdziEiP4CsH2T799FO1zGzRokX46quv8Mcff2D27Nkwm80lbRNdpwLyX2rlqP1vt8Vj27E/M8oRkfEUa52vg6zpdda6Xrq5WkHA/c2q4rvfTmH8wt1YMKK9CspE5MHBN+9ZYzczbdq04raHbmJMtzpYvvcsdp9MxpzNx/Fw6xp6N4mIXBl8ZSVDUbAMjmtVDPTB6B71MG7BHkxd+gd6NayibiMiDw2+q1atcm1LqMiGtKqBuZtPYG9CMt5eug//+EtjvZtERKWVz9fhxIkTaqPSI/O8k/rFqP3/bj2B9QeLfmILERk4+EqhzNdee03l161Zs6baZF+WmXliXk931LxGBQy6I1LtPz3nN5y44Nr0d0TkBsFXTvn95JNPVJkfmQuWTfalmoUU16TSMb53jEq4LtUvnpi9DWmZ2Xo3iYhcudRszpw5qoSQlAFykDI/ctLFoEGDVDULcj1fqxn/fqg5+ny4Dr8nJOOFebvwwaCmPOhJ5KkjX8mXK1MNhaWBzJvljFyvaogfPhrSHBaTFxbtSsDHaw7p3SQiclXwffrppzFp0qR8tdJkX2q6yX1Uuu6oVQGv97EfgJv60z6s2ndW7yYRkSumHWSOV5KcR0RE5NZxk0TqmZmZ6NKliyop7yDJ1sn1HmpdA3tOJeObzcfx92+2Y8GIdqgdGqh3s4jImcFXUkref//9+W6T+V7S14Q+MThwJgVbj13EiDnb8ePT7WAxl3g1IRG5Q/CVfLtSHTg0NBR+fn6uaBMVk7fFhI8eaobu765VB+BmbTiG4e1r6d0sIiqEqTjBNyoqCvHx8bf6o1QKwoJ8Mfbu+mp/2rJ9OJ3E2kNEHhF8TSYT6tSp4/KyylR8A1pEomn1EFzOzMGkxXv1bg4RFaJYE4JTpkzBmDFjsHv37uL8OLmYyeSFyf0aQrJNLt6VgLX7z+ndJCJyRvCVUkGbN29WKx1k3rdChQr5NtJfTNVyGNrWvhZ73ILdSM+yJ2InIgOvdpg+fbrzW0JON6pbXTXyPZqYhk/WHsbfu9jrwBGRQYPv0KFDnd8ScrogXytevTdarfv9cNVB9L29KmpUDNC7WURUkpSSUr9NsphJLoezZ+1nVMXGxmLPnj3ObB+VUO/G4WgXVRGZ2TaVgF1WqxCRQYPvmjVr0KhRI2zatEmdwSYVjB1nuY0fP97ZbaQSkCQ7k/o2hLfZhDX7z6nim0Rk0OA7duxYTJ48GcuXL8+XSKdz587YuHGjM9tHTiCnGT/TOUrtv/r9bmw7dkHvJhGVecUKvnFxcejfv/81t4eFheH8eVZVcEcj7opCj5jKyMyx4cnZ23Dy0hW9m0RUppmKm9shISGh0IQ71apVc0a7yAVrf6cNuB0NwoNxPjUTj83ayuTrREYLvg8++CBefPFFnD59Ws0p2mw2rF+/HqNHj1ZrgMk9BfhY8Olfm6NigLfK/fD8/3bCZuMBOCLDBN8333wT9evXV5nM5GBbdHQ0OnTogLZt26oVEOS+Isr74z8PN4fV7IXY3acxfcUBvZtEVCYVK/jKQbZPP/0Uhw8fxqJFi/D1119j//79mD17Nsxms/NbSU7VomYFvNm/kdp/f8UBLNp1Su8mEZU5xV7nK8UypYabHHh76KGH0K9fP8yYMcO5rSOXeaBFJB7vYE83Ofa7OGY/IzJC8B03bhyeffZZ9O7dG99++63aZH/kyJHqPjKGsT0bqOxnqRnZmPAjT44hcvvTi6U6sUw7yNltDn369FEVjKWs/MSJE53ZRnIRs8lLTT/c+8E6Nf/7894z6BpdWe9mEZUJxRr5ZmVloUWLFtfc3rx5c2Rnc/mSkcjSs8euTj9I9rPLGfz9Eblt8H344YfV6LegTz75BEOGDHFGu6gUPdulDiLK++FUUjreXb5f7+YQlQnFmnZwHHBbtmwZWrdura5Lnofjx4+rdb6jRo3Kfdy0adOc01JyGX9vCyb1a4hHZ27B5+uPoF/TamhYrZzezSLyaMUKvlLBolmzZrnZzUSlSpXUlre6hZyAQcZwV70w3Ns4HIt2JeDl7+Pw/VPt1JwwEblR8F21apXzW0K6G3dvtMp8tis+CbM3HMUj7Vj5mMjt1vmS5wkL9sWLVysfT/1pH04x+Q6RyzD4Uj6D76iOZlcrHz8xeytXPxC5CIMvXZP9bPrApqgQ4I3dJ5Px7NztyGHyHSKnY/Cla1Sv6I9P/9oCPhYTfv79LCb+yPJDRGUq+L7++utqxUTeTbKpkes1r1Ee7w68Xe3P2nAMn68/qneTiDyKWwdfERMToxK3O7Z169bp3aQyo1ejcLzcy/5hN3nxXvy057TeTSLyGG4ffC0WC6pUqZK7yVpiKj2Pd6iNIa2qQ2YdZP5354lLejeJqGyf4VZaDhw4gKpVq8LX1xdt2rTBW2+9herVq1/38RkZGWpzSE5Ozs1HIZtROdquRx9e7VkX8RfSsObAeTz+5VYseaYtyvlZPaJvrubJffP0/mW5uE9emhsfSYmNjVWVMurVq6emHCZMmICTJ0+qs+iCgoKuO08sjytozpw58Pf3L4VWe6b0HOCdXWacTfdCmzAbHrzNpneTiFwqLS0NgwcPRlJSEoKDg8tW8C3o0qVLqFGjhsoXMXz48CKPfKXckVRVdsUbWJqfwsuXL0e3bt1gtTpv1Hkrthy9iMGfbVH7sx9tgda1K3hM31zFk/vm6f1LTExEeHi4y4Kv2087FKyaXLduXRw8ePC6j/Hx8VFbQfKH4Ql/HHr2o22dMDX/+/Wm43ht4V4sfe5O+FqdVzbKU35HZa1vnto/q4v74/YH3PKSKQhJ5COfRqSPF3vWR+VgHxxNTMN7LL5J5JnBV0rRr1mzBkePHsWvv/6q6sVJgc68FTSodAX7WjGpb0O1/8naw9hzKknvJhEZklsH3/j4eBVo5YDbgAEDULFiRWzcuBGhoaF6N61M6x5TBfc0ClenHUvxzewcHnwj8qg537lz5+rdBLqO8X2i8cuBc4g7mYSZ64/i8Ttr690kIkNx65Evua+wIF+8ek+02n9n+T4cS7ysd5OIDIXBl4rtgRYRaHtbRaRn2TBp0V69m0NkKAy+VGyS6Ghi34awmLxU9jOZhiCiomHwpRKJCgvEw21qqH0Z/fLgG1HRMPiSU0rPh/hbsf9MKr7ZfFzv5hAZAoMvlViIvzdGdaur9qct34+kNM9LskLkbAy+5LTab3UrB+JiWhamr9ivd3OI3B6DLzmFxWzCa/fal57N3nAMB8+m6t0kIrfG4EtO06FOKLrUD0O2TVOVL4jo+hh8yaleuacBrGYvrN53Dqv2ndW7OURui8GXnKp2aCCGtqmZu/QsM5tLz4gKw+BLTvdMlzqoGOCNw+cu499rDundHCK3xOBLTif13cb1th98+3DlQRw8m6J3k4jcDoMvuUSfJlXRqV4oMnNseGl+HGw2w1SrIioVDL7ksrwPk/s1hL+3WdV++2YLz3wjyovBl1wmorw/Rnevp/anLPkDZ5LT9W4Skdtg8CWXGtq2JppEhiAlIxvjF+zRuzlEboPBl1zKbPLClPsaqbSTS/ecxtLdp/VuEpFbYPAll2sQHownO9rLDI1bsBvJ6Uy8Q8TgS6Ximc51UKtSAM6mZODl+XHQNK5+oLKNwZdKha/VjKl/aaymHxbtSlDrf4nKMgZfKjUtalbApH4N1f47y/cjNi5B7yYR6YbBl0rVoDuq49F29twPo/63E7tPJundJCJdMPhSqXulVwPcWTcUV7Jy8PiXW9U8MFFZw+BLuiRe/2BQU9QODUBCUjqemrMDWUx+RmUMgy/plnzns6Et1eXO+CTM2m/CH6dTuAqCygwGX9KNLD37eEgztQIi7qIJvf+1AV3eWYOpP/2h5oIZiMmTMfiSrtpGVcJnf22GRuVt8LaYcPj8Zfxr1SHc+8E6dJy6misiyGMx+JLu2t5WEY/Vt2HT2E54f1BT9GxYBb5WE45fSMNTc37DVxuP6d1EIqezOP8piYon0Mei8gDLlpaZjTcW/46vNx3Hqz/sRtKVLDzV6TaVqpLIE3DkS27J39ui8gE/0zlKXZ/60z4VjDkPTJ6CwZfcloxyn+9eD6/e00Bdn7HuCF6YtwvZOVyXRsbHaQdye491qK2WpL343S58uy0e8RevoH2dSogo76cStkeW90NokM8NpyRybBpS0rPU9IVswb5WhIf4wsdiLtW+EDkw+JIhPNAiEsF+VjwzZzs2HE5UW16yUiLA2wyzyQSr2UvlEbaaTSroSrCVNJYFZywkVocF+agALoG8crDv1Z81wezlBcvV55F9k7q05ye279svZZmceszVfZOXfd90dV+z5eBAkhc2H70Ab6sVJi+o22WT15c2SbNkOsV+ad+XdkvZO5sml5q6XT2v+tk/2+Ftlv7a+yyX8j7I82bnaPbNZkO2zb5vMv352o5+iSx5TI6GrBz7Y3NsNvUYx/M5nt9iMkFaKe1S7dWArOwsnLsCVanabDFDvpRI22WTefu0zJyrm31f2u5jNcPPalbJluTAqlzKeyevI++57MtrqfdP/svzHtl/b/b3wPEeOq57wXHdvi+khmBGVg7Ss2zIyLZfyvNYLabc15Q+5r6mCbmXyZczXfo3zeBLhtEjpgp+GNFOJWU/efEKTlxMU5cJSVeQmW1T281ITTkZ9V66kqn+IZ5JzlDbtmMXXdhyMz7cuxWeywLsWA9PY8tIc+nzM/iSoURXDVZbXjJiO52UjvSsnNxRnozmZPQl458QfyvK+XmrqQsZ5QgZ/SRezlTBW6Yx4i+m4XxqxtWRn4wYNeSokaN95KlGczIKzXvd8bgC+2rkqh4D2Gw2JCWnwD8gUI3eHCNZW4HPCceMiVw6RtVqhHp1X9jytMH+/Ff7mmNTIzzHvtxnNeUZRZpNaqQrA0dH2x3PIfKPOu2X8ris7D+fW42KZfRsH1aq9/XqLmy2HPjIqP7qaFpGovKc8kHn72OGv9Viv/Q2q36nqxFojvrws1/++XuTkXqWGrXb1Pul3pOro131Lsj/cr8R4JqRuOM9dnzLkW8GPldH1z4W+6W8nfJajn7J68kHd97fseO9cSUGXzI8CRyRFfxv6WfkH3OlQB+1SY05V8nKysKSJUvQq1c7WK1WeJo/+9fD7fqnaVqJliaeO3ceYdPhMlztQEQeyauEa8Id3zhchcGXiEgHDL5ERDpg8CUi0gGDLxGRDhh8iYh04PFLzRxnxSQnJ8PoS3rS0tJUP9xtSU9JsW/G5cn9S0lJUZeuSubk8cHX8QZGRkbq3RQiMqDExESUK1fO6c/rpXl4jj45w+jUqVMICgoydC5YGVnIB8iJEycQHJz/DC+jY9+My5P7l5SUhOrVq+PixYsICXH+iTgeP/I1mUyIiIiAp5A/cE/7I3dg34zLk/tnkiw7rnhelzwrERHdEIMvEZEOGHwNwsfHB+PHj1eXnoZ9My5P7p+Pi/vm8QfciIjcEUe+REQ6YPAlItIBgy8RkQ4YfImIdMDgq6O1a9eid+/eqFq1qjr77ocffsh3vxwLHTduHMLDw+Hn54euXbviwIED+R5z4cIFDBkyRC1wl7Nwhg8fjtTUVOjtrbfeQsuWLdWZhWFhYejXrx/27duX7zHp6ekYMWIEKlasiMDAQNx///04c+ZMvsccP34c99xzD/z9/dXzjBkzBtnZ2dDTxx9/jMaNG+eeWNCmTRvExsYavl+FmTJlivrbfO655zyif6+//rq9HlyerX79+vr0TVY7kD6WLFmivfLKK9r8+fNVZezvv/8+3/1TpkzRypUrp/3www/azp07tT59+mi1atXSrly5kvuYu+++W2vSpIm2ceNG7ZdfftGioqK0QYMGaXrr0aOHNnPmTG337t3ajh07tF69emnVq1fXUlNTcx/zt7/9TYuMjNRWrFihbd26VWvdurXWtm3b3Puzs7O1hg0bal27dtW2b9+u3q9KlSppL730kqanhQsXaosXL9b279+v7du3T3v55Zc1q9Wq+mrkfhW0efNmrWbNmlrjxo21Z599Nvd2I/dv/PjxWkxMjJaQkJC7nTt3Tpe+Mfi6iYLB12azaVWqVNGmTp2ae9ulS5c0Hx8f7ZtvvlHX9+7dq35uy5YtuY+JjY3VvLy8tJMnT2ru5OzZs6qta9asye2LBKxvv/029zG///67esyGDRvUdfnDNplM2unTp3Mf8/HHH2vBwcFaRkaG5k7Kly+vzZgxw2P6lZKSotWpU0dbvny51rFjx9zga/T+jR8/Xg1WClPafeO0g5s6cuQITp8+raYaHCSzUqtWrbBhwwZ1XS5lqqFFixa5j5HHy7nomzZtgrslKREVKlRQl9u2bVPpCPP2T77+SSKTvP1r1KgRKleunPuYHj16qGQue/bsgTvIycnB3LlzcfnyZTX94Cn9kq/e8tU6bz+EJ/TvwIEDaqqvdu3aaspOphH06JvHJ9YxKgm8Iu8v2XHdcZ9cypxTXhaLRQU4x2PcJbOczBm2a9cODRs2VLdJ+7y9va/JFlWwf4X133GfnuLi4lSwlTlCmRv8/vvvER0djR07dhi6X0I+TH777Tds2bLlmvuM/ntr1aoVvvjiC9SrVw8JCQmYMGECOnTogN27d5d63xh8qVRGUfLHvW7dOngK+ccrgVZG9PPmzcPQoUOxZs0aGJ2khnz22WexfPly+Pr6wtP07Nkzd18OmkowrlGjBv73v/+pg9qlidMObqpKlSrqsuCRVrnuuE8uz549m+9+OeoqKyAcj9Hb008/jUWLFmHVqlX5UntK+zIzM3Hp0qUb9q+w/jvu05OMkKKiotC8eXO1sqNJkyZ47733DN8v+eotf1PNmjVT36Jkkw+V999/X+3LKM/I/StIRrl169bFwYMHS/13x+DrpmrVqqV+mStWrMi9TeaVZC5Xvu4KuZQ/FPkH47By5Ur1NV8+0fUkxxAl8MrXcWmT9CcvCVpSdiZv/2Qpmsy/5e2ffL3P+wEjIzJZ3iVf8d2JvOcZGRmG71eXLl1U22RU79jkmILMjTr2jdy/gmRZ5qFDh9RyzlL/3RX7sCE55YiyLFeRTX4V06ZNU/vHjh3LXWoWEhKiLViwQNu1a5fWt2/fQpeaNW3aVNu0aZO2bt06dYTaHZaa/d///Z9aJrd69ep8y3rS0tLyLeuR5WcrV65Uy3ratGmjtoLLerp3766Wqy1dulQLDQ3VfcnS2LFj1aqNI0eOqN+LXJcVJsuWLTN0v64n72oHo/fv+eefV3+T8rtbv369WjImS8VkNU5p943BV0erVq1SQbfgNnTo0NzlZq+99ppWuXJltcSsS5cual1pXomJiSrYBgYGquUujz76qArqeiusX7LJ2l8H+RB56qmn1DItf39/rX///ipA53X06FGtZ8+emp+fn/pHIv94srKyND0NGzZMq1Gjhubt7a3+4cnvxRF4jdyvogZfI/dv4MCBWnh4uPrdVatWTV0/ePCgLn1jSkkiIh1wzpeISAcMvkREOmDwJSLSAYMvEZEOGHyJiHTA4EtEpAMGXyIiHTD4EhHpgMGX6BatXr1alZ8pmICF6FYw+BIR6YDBl4hIBwy+ZDiSvlFy6EqaSkmALbl0JaF53imBxYsXq2TZkhC8devWKpl7Xt999x1iYmLg4+ODmjVr4p133sl3v6SHfPHFFxEZGakeI7l7P/vss3yPkVSekmJRqti2bdv2murMRDfkjExBRKVp8uTJWv369VU6v0OHDqlMaZL1TVIFOjLFNWjQQGUak5SP9957r6rCm5mZqX5eUgVKEcSJEyeqLHHy85KhKm/GtQEDBqgqtlJZWl7j559/1ubOnavuc7xGq1at1Gvu2bNH69ChQ74qt0Q3w+BLhpKenq5S/f3666/5bh8+fLhKrekIjI5A6Ui7KcH1v//9r7o+ePBgrVu3bvl+fsyYMVp0dLTal4AszyGVewvjeA0JyA5SSl5uy5trmehGOO1AhiLlXtLS0tCtWzdVuNKxffnll6oigYOj8oCQgqJSc+33339X1+VSinnmJdelqq1UI5aKDWazGR07drxhW2Raw0EqIYiCZZ2IrocFNMlQpOyLkDndatWq5btP5mbzBuDiKmohRSk54yDzzI75aKKi4MiXDEXqZEmQlbpachAs7yYHxxw2btyYu3/x4kXs378fDRo0UNflcv369fmeV65LIUUZ8TZq1EgFUU+oRkzuiyNfMpSgoCCMHj0aI0eOVAGyffv2qny7BE8pYihlwMXEiRNRsWJFVW33lVdeQaVKldCvXz913/PPP4+WLVti0qRJGDhwIDZs2IAPP/wQH330kbpfVj9IKfhhw4apqr2ymuLYsWNqSmHAgAG69p88yA1nhInckNS2mz59ulavXj3NarWqOmo9evRQRS0dB8N+/PFHLSYmRtXquuOOO7SdO3fme4558+apA2zy81IwcerUqfnulwNnI0eOzK33FRUVpX3++efqPsdrXLx4MffxjiKoUpiRqChYw408iqzzveuuu9RUQ0hIiN7NIbouzvkSEemAwZeISAecdiAi0gFHvkREOmDwJSLSAYMvEZEOGHyJiHTA4EtEpAMGXyIiHTD4EhHpgMGXiAil7/8Br3HIekXcKWUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs, lr = 500, 2\n",
    "train_ch8(model, train_iter, vocab, lr*1.0, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f552d9",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 在深度循环神经网络中，隐状态的信息被传递到当前层的下一时间步和下一层的当前时间步。\n",
    "* 有许多不同风格的深度循环神经网络，\n",
    "  如长短期记忆网络、门控循环单元、或经典循环神经网络。\n",
    "  这些模型在深度学习框架的高级API中都有涵盖。\n",
    "* 总体而言，深度循环神经网络需要大量的调参（如学习率和修剪）\n",
    "  来确保合适的收敛，模型的初始化也需要谨慎。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 基于我们在 :numref:`sec_rnn_scratch`中讨论的单层实现，\n",
    "   尝试从零开始实现两层循环神经网络。\n",
    "1. 在本节训练模型中，比较使用门控循环单元替换长短期记忆网络后模型的精确度和训练速度。\n",
    "1. 如果增加训练数据，能够将困惑度降到多低？\n",
    "1. 在为文本建模时，是否可以将不同作者的源数据合并？有何优劣呢？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1affbed5",
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/2770)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
