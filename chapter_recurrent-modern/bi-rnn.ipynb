{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba91d77a",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 双向循环神经网络\n",
    ":label:`sec_bi_rnn`\n",
    "\n",
    "在序列学习中，我们以往假设的目标是：\n",
    "在给定观测的情况下\n",
    "（例如，在时间序列的上下文中或在语言模型的上下文中），\n",
    "对下一个输出进行建模。\n",
    "虽然这是一个典型情景，但不是唯一的。\n",
    "还可能发生什么其它的情况呢？\n",
    "我们考虑以下三个在文本序列中填空的任务。\n",
    "\n",
    "* 我`___`。\n",
    "* 我`___`饿了。\n",
    "* 我`___`饿了，我可以吃半头猪。\n",
    "\n",
    "根据可获得的信息量，我们可以用不同的词填空，\n",
    "如“很高兴”（\"happy\"）、“不”（\"not\"）和“非常”（\"very\"）。\n",
    "很明显，每个短语的“下文”传达了重要信息（如果有的话），\n",
    "而这些信息关乎到选择哪个词来填空，\n",
    "所以无法利用这一点的序列模型将在相关任务上表现不佳。\n",
    "例如，如果要做好命名实体识别\n",
    "（例如，识别“Green”指的是“格林先生”还是绿色），\n",
    "不同长度的上下文范围重要性是相同的。\n",
    "为了获得一些解决问题的灵感，让我们先迂回到概率图模型。\n",
    "\n",
    "## 隐马尔可夫模型中的动态规划\n",
    "\n",
    "这一小节是用来说明动态规划问题的，\n",
    "具体的技术细节对于理解深度学习模型并不重要，\n",
    "但它有助于我们思考为什么要使用深度学习，\n",
    "以及为什么要选择特定的架构。\n",
    "\n",
    "如果我们想用概率图模型来解决这个问题，\n",
    "可以设计一个隐变量模型：\n",
    "在任意时间步$t$，假设存在某个隐变量$h_t$，\n",
    "通过概率$P(x_t \\mid h_t)$控制我们观测到的$x_t$。\n",
    "此外，任何$h_t \\to h_{t+1}$转移\n",
    "都是由一些状态转移概率$P(h_{t+1} \\mid h_{t})$给出。\n",
    "这个概率图模型就是一个*隐马尔可夫模型*（hidden Markov model，HMM），\n",
    "如 :numref:`fig_hmm`所示。\n",
    "\n",
    "![隐马尔可夫模型](../img/hmm.svg)\n",
    ":label:`fig_hmm`\n",
    "\n",
    "因此，对于有$T$个观测值的序列，\n",
    "我们在观测状态和隐状态上具有以下联合概率分布：\n",
    "\n",
    "$$P(x_1, \\ldots, x_T, h_1, \\ldots, h_T) = \\prod_{t=1}^T P(h_t \\mid h_{t-1}) P(x_t \\mid h_t), \\text{ where } P(h_1 \\mid h_0) = P(h_1).$$\n",
    ":eqlabel:`eq_hmm_jointP`\n",
    "\n",
    "现在，假设我们观测到所有的$x_i$，除了$x_j$，\n",
    "并且我们的目标是计算$P(x_j \\mid x_{-j})$，\n",
    "其中$x_{-j} = (x_1, \\ldots, x_{j-1}, x_{j+1}, \\ldots, x_{T})$。\n",
    "由于$P(x_j \\mid x_{-j})$中没有隐变量，\n",
    "因此我们考虑对$h_1, \\ldots, h_T$选择构成的\n",
    "所有可能的组合进行求和。\n",
    "如果任何$h_i$可以接受$k$个不同的值（有限的状态数），\n",
    "这意味着我们需要对$k^T$个项求和，\n",
    "这个任务显然难于登天。\n",
    "幸运的是，有个巧妙的解决方案：*动态规划*（dynamic programming）。\n",
    "\n",
    "要了解动态规划的工作方式，\n",
    "我们考虑对隐变量$h_1, \\ldots, h_T$的依次求和。\n",
    "根据 :eqref:`eq_hmm_jointP`，将得出：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    &P(x_1, \\ldots, x_T) \\\\\n",
    "    =& \\sum_{h_1, \\ldots, h_T} P(x_1, \\ldots, x_T, h_1, \\ldots, h_T) \\\\\n",
    "    =& \\sum_{h_1, \\ldots, h_T} \\prod_{t=1}^T P(h_t \\mid h_{t-1}) P(x_t \\mid h_t) \\\\\n",
    "    =& \\sum_{h_2, \\ldots, h_T} \\underbrace{\\left[\\sum_{h_1} P(h_1) P(x_1 \\mid h_1) P(h_2 \\mid h_1)\\right]}_{\\pi_2(h_2) \\stackrel{\\mathrm{def}}{=}}\n",
    "    P(x_2 \\mid h_2) \\prod_{t=3}^T P(h_t \\mid h_{t-1}) P(x_t \\mid h_t) \\\\\n",
    "    =& \\sum_{h_3, \\ldots, h_T} \\underbrace{\\left[\\sum_{h_2} \\pi_2(h_2) P(x_2 \\mid h_2) P(h_3 \\mid h_2)\\right]}_{\\pi_3(h_3)\\stackrel{\\mathrm{def}}{=}}\n",
    "    P(x_3 \\mid h_3) \\prod_{t=4}^T P(h_t \\mid h_{t-1}) P(x_t \\mid h_t)\\\\\n",
    "    =& \\dots \\\\\n",
    "    =& \\sum_{h_T} \\pi_T(h_T) P(x_T \\mid h_T).\n",
    "\\end{aligned}$$\n",
    "\n",
    "通常，我们将*前向递归*（forward recursion）写为：\n",
    "\n",
    "$$\\pi_{t+1}(h_{t+1}) = \\sum_{h_t} \\pi_t(h_t) P(x_t \\mid h_t) P(h_{t+1} \\mid h_t).$$\n",
    "\n",
    "递归被初始化为$\\pi_1(h_1) = P(h_1)$。\n",
    "符号简化，也可以写成$\\pi_{t+1} = f(\\pi_t, x_t)$，\n",
    "其中$f$是一些可学习的函数。\n",
    "这看起来就像我们在循环神经网络中讨论的隐变量模型中的更新方程。\n",
    "\n",
    "与前向递归一样，我们也可以使用后向递归对同一组隐变量求和。这将得到：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    & P(x_1, \\ldots, x_T) \\\\\n",
    "     =& \\sum_{h_1, \\ldots, h_T} P(x_1, \\ldots, x_T, h_1, \\ldots, h_T) \\\\\n",
    "    =& \\sum_{h_1, \\ldots, h_T} \\prod_{t=1}^{T-1} P(h_t \\mid h_{t-1}) P(x_t \\mid h_t) \\cdot P(h_T \\mid h_{T-1}) P(x_T \\mid h_T) \\\\\n",
    "    =& \\sum_{h_1, \\ldots, h_{T-1}} \\prod_{t=1}^{T-1} P(h_t \\mid h_{t-1}) P(x_t \\mid h_t) \\cdot\n",
    "    \\underbrace{\\left[\\sum_{h_T} P(h_T \\mid h_{T-1}) P(x_T \\mid h_T)\\right]}_{\\rho_{T-1}(h_{T-1})\\stackrel{\\mathrm{def}}{=}} \\\\\n",
    "    =& \\sum_{h_1, \\ldots, h_{T-2}} \\prod_{t=1}^{T-2} P(h_t \\mid h_{t-1}) P(x_t \\mid h_t) \\cdot\n",
    "    \\underbrace{\\left[\\sum_{h_{T-1}} P(h_{T-1} \\mid h_{T-2}) P(x_{T-1} \\mid h_{T-1}) \\rho_{T-1}(h_{T-1}) \\right]}_{\\rho_{T-2}(h_{T-2})\\stackrel{\\mathrm{def}}{=}} \\\\\n",
    "    =& \\ldots \\\\\n",
    "    =& \\sum_{h_1} P(h_1) P(x_1 \\mid h_1)\\rho_{1}(h_{1}).\n",
    "\\end{aligned}$$\n",
    "\n",
    "因此，我们可以将*后向递归*（backward recursion）写为：\n",
    "\n",
    "$$\\rho_{t-1}(h_{t-1})= \\sum_{h_{t}} P(h_{t} \\mid h_{t-1}) P(x_{t} \\mid h_{t}) \\rho_{t}(h_{t}),$$\n",
    "\n",
    "初始化$\\rho_T(h_T) = 1$。\n",
    "前向和后向递归都允许我们对$T$个隐变量在$\\mathcal{O}(kT)$\n",
    "（线性而不是指数）时间内对$(h_1, \\ldots, h_T)$的所有值求和。\n",
    "这是使用图模型进行概率推理的巨大好处之一。\n",
    "它也是通用消息传递算法 :cite:`Aji.McEliece.2000`的一个非常特殊的例子。\n",
    "结合前向和后向递归，我们能够计算\n",
    "\n",
    "$$P(x_j \\mid x_{-j}) \\propto \\sum_{h_j} \\pi_j(h_j) \\rho_j(h_j) P(x_j \\mid h_j).$$\n",
    "\n",
    "因为符号简化的需要，后向递归也可以写为$\\rho_{t-1} = g(\\rho_t, x_t)$，\n",
    "其中$g$是一个可以学习的函数。\n",
    "同样，这看起来非常像一个更新方程，\n",
    "只是不像我们在循环神经网络中看到的那样前向运算，而是后向计算。\n",
    "事实上，知道未来数据何时可用对隐马尔可夫模型是有益的。\n",
    "信号处理学家将是否知道未来观测这两种情况区分为内插和外推，\n",
    "有关更多详细信息，请参阅 :cite:`Doucet.De-Freitas.Gordon.2001`。\n",
    "\n",
    "## 双向模型\n",
    "\n",
    "如果我们希望在循环神经网络中拥有一种机制，\n",
    "使之能够提供与隐马尔可夫模型类似的前瞻能力，\n",
    "我们就需要修改循环神经网络的设计。\n",
    "幸运的是，这在概念上很容易，\n",
    "只需要增加一个“从最后一个词元开始从后向前运行”的循环神经网络，\n",
    "而不是只有一个在前向模式下“从第一个词元开始运行”的循环神经网络。\n",
    "*双向循环神经网络*（bidirectional RNNs）\n",
    "添加了反向传递信息的隐藏层，以便更灵活地处理此类信息。\n",
    " :numref:`fig_birnn`描述了具有单个隐藏层的双向循环神经网络的架构。\n",
    "\n",
    "![双向循环神经网络架构](../img/birnn.svg)\n",
    ":label:`fig_birnn`\n",
    "\n",
    "事实上，这与隐马尔可夫模型中的动态规划的前向和后向递归没有太大区别。\n",
    "其主要区别是，在隐马尔可夫模型中的方程具有特定的统计意义。\n",
    "双向循环神经网络没有这样容易理解的解释，\n",
    "我们只能把它们当作通用的、可学习的函数。\n",
    "这种转变集中体现了现代深度网络的设计原则：\n",
    "首先使用经典统计模型的函数依赖类型，然后将其参数化为通用形式。\n",
    "\n",
    "### 定义\n",
    "\n",
    "双向循环神经网络是由 :cite:`Schuster.Paliwal.1997`提出的，\n",
    "关于各种架构的详细讨论请参阅 :cite:`Graves.Schmidhuber.2005`。\n",
    "让我们看看这样一个网络的细节。\n",
    "\n",
    "对于任意时间步$t$，给定一个小批量的输入数据\n",
    "$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$\n",
    "（样本数$n$，每个示例中的输入数$d$），\n",
    "并且令隐藏层激活函数为$\\phi$。\n",
    "在双向架构中，我们设该时间步的前向和反向隐状态分别为\n",
    "$\\overrightarrow{\\mathbf{H}}_t  \\in \\mathbb{R}^{n \\times h}$和\n",
    "$\\overleftarrow{\\mathbf{H}}_t  \\in \\mathbb{R}^{n \\times h}$，\n",
    "其中$h$是隐藏单元的数目。\n",
    "前向和反向隐状态的更新如下：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\overrightarrow{\\mathbf{H}}_t &= \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(f)} + \\overrightarrow{\\mathbf{H}}_{t-1} \\mathbf{W}_{hh}^{(f)}  + \\mathbf{b}_h^{(f)}),\\\\\n",
    "\\overleftarrow{\\mathbf{H}}_t &= \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(b)} + \\overleftarrow{\\mathbf{H}}_{t+1} \\mathbf{W}_{hh}^{(b)}  + \\mathbf{b}_h^{(b)}),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中，权重$\\mathbf{W}_{xh}^{(f)} \\in \\mathbb{R}^{d \\times h}, \\mathbf{W}_{hh}^{(f)} \\in \\mathbb{R}^{h \\times h}, \\mathbf{W}_{xh}^{(b)} \\in \\mathbb{R}^{d \\times h}, \\mathbf{W}_{hh}^{(b)} \\in \\mathbb{R}^{h \\times h}$\n",
    "和偏置$\\mathbf{b}_h^{(f)} \\in \\mathbb{R}^{1 \\times h}, \\mathbf{b}_h^{(b)} \\in \\mathbb{R}^{1 \\times h}$都是模型参数。\n",
    "\n",
    "接下来，将前向隐状态$\\overrightarrow{\\mathbf{H}}_t$\n",
    "和反向隐状态$\\overleftarrow{\\mathbf{H}}_t$连接起来，\n",
    "获得需要送入输出层的隐状态$\\mathbf{H}_t \\in \\mathbb{R}^{n \\times 2h}$。\n",
    "在具有多个隐藏层的深度双向循环神经网络中，\n",
    "该信息作为输入传递到下一个双向层。\n",
    "最后，输出层计算得到的输出为\n",
    "$\\mathbf{O}_t \\in \\mathbb{R}^{n \\times q}$（$q$是输出单元的数目）：\n",
    "\n",
    "$$\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q.$$\n",
    "\n",
    "这里，权重矩阵$\\mathbf{W}_{hq} \\in \\mathbb{R}^{2h \\times q}$\n",
    "和偏置$\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$\n",
    "是输出层的模型参数。\n",
    "事实上，这两个方向可以拥有不同数量的隐藏单元。\n",
    "\n",
    "### 模型的计算代价及其应用\n",
    "\n",
    "双向循环神经网络的一个关键特性是：使用来自序列两端的信息来估计输出。\n",
    "也就是说，我们使用来自过去和未来的观测信息来预测当前的观测。\n",
    "但是在对下一个词元进行预测的情况中，这样的模型并不是我们所需的。\n",
    "因为在预测下一个词元时，我们终究无法知道下一个词元的下文是什么，\n",
    "所以将不会得到很好的精度。\n",
    "具体地说，在训练期间，我们能够利用过去和未来的数据来估计现在空缺的词；\n",
    "而在测试期间，我们只有过去的数据，因此精度将会很差。\n",
    "下面的实验将说明这一点。\n",
    "\n",
    "另一个严重问题是，双向循环神经网络的计算速度非常慢。\n",
    "其主要原因是网络的前向传播需要在双向层中进行前向和后向递归，\n",
    "并且网络的反向传播还依赖于前向传播的结果。\n",
    "因此，梯度求解将有一个非常长的链。\n",
    "\n",
    "双向层的使用在实践中非常少，并且仅仅应用于部分场合。\n",
    "例如，填充缺失的单词、词元注释（例如，用于命名实体识别）\n",
    "以及作为序列处理流水线中的一个步骤对序列进行编码（例如，用于机器翻译）。\n",
    "在 :numref:`sec_bert`和 :numref:`sec_sentiment_rnn`中，\n",
    "我们将介绍如何使用双向循环神经网络编码文本序列。\n",
    "\n",
    "## (**双向循环神经网络的错误应用**)\n",
    "\n",
    "由于双向循环神经网络使用了过去的和未来的数据，\n",
    "所以我们不能盲目地将这一语言模型应用于任何预测任务。\n",
    "尽管模型产出的困惑度是合理的，\n",
    "该模型预测未来词元的能力却可能存在严重缺陷。\n",
    "我们用下面的示例代码引以为戒，以防在错误的环境中使用它们。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ba4d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fec98b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import os\n",
    "import collections\n",
    "import random\n",
    "import torch\n",
    "\n",
    "class Vocab:  #@save\n",
    "    \"\"\"文本词表\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        # 按出现频率排序\n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        # 未知词元的索引为0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # 未知词元的索引为0\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs\n",
    "    \n",
    "def count_corpus(tokens):  #@save\n",
    "    \"\"\"统计词元的频率\"\"\"\n",
    "    # 这里的tokens是1D列表或2D列表\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # 将词元列表展平成一个列表\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)\n",
    "\n",
    "def download_time_machine(data_dir='../data', filename='timemachine.txt'):\n",
    "    \"\"\"下载时间机器文本文件\"\"\"\n",
    "    url = 'http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt'\n",
    "    \n",
    "    # 创建数据目录（如果不存在）\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    # 完整文件路径\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "    \n",
    "    # 如果文件不存在，则下载\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f'Downloading {filename} from {url}...')\n",
    "        response = requests.get(url)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f'File saved to {file_path}')\n",
    "    else:\n",
    "        print(f'File already exists at {file_path}')\n",
    "    \n",
    "    return file_path\n",
    "\n",
    "def read_time_machine():\n",
    "    \"\"\"将时间机器数据集加载到文本行的列表中\"\"\"\n",
    "    # 下载文件并获取路径\n",
    "    file_path = download_time_machine()\n",
    "    \n",
    "    # 读取文件内容\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # 处理每一行：只保留字母，转为小写，去除首尾空格\n",
    "    processed_lines = [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "    \n",
    "    return processed_lines\n",
    "\n",
    "def tokenize(lines, token='word'):  #@save\n",
    "    \"\"\"将文本行拆分为单词或字符词元\"\"\"\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('错误：未知词元类型：' + token)\n",
    "\n",
    "def load_corpus_time_machine(max_tokens=-1):  #@save\n",
    "    \"\"\"返回时光机器数据集的词元索引列表和词表\"\"\"\n",
    "    lines = read_time_machine()\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    vocab = Vocab(tokens)\n",
    "    # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，\n",
    "    # 所以将所有文本行展平到一个列表中\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab\n",
    "\n",
    "def seq_data_iter_random(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"使用随机抽样生成一个小批量子序列\"\"\"\n",
    "    # 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1\n",
    "    corpus = corpus[random.randint(0, num_steps - 1):]\n",
    "    # 减去1，是因为我们需要考虑标签\n",
    "    num_subseqs = (len(corpus) - 1) // num_steps\n",
    "    # 长度为num_steps的子序列的起始索引\n",
    "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
    "    # 在随机抽样的迭代过程中，\n",
    "    # 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻\n",
    "    random.shuffle(initial_indices)\n",
    "\n",
    "    def data(pos):\n",
    "        # 返回从pos位置开始的长度为num_steps的序列\n",
    "        return corpus[pos: pos + num_steps]\n",
    "\n",
    "    num_batches = num_subseqs // batch_size\n",
    "    for i in range(0, batch_size * num_batches, batch_size):\n",
    "        # 在这里，initial_indices包含子序列的随机起始索引\n",
    "        initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
    "        X = [data(j) for j in initial_indices_per_batch]\n",
    "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
    "        yield torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"使用顺序分区生成一个小批量子序列\"\"\"\n",
    "    # 从随机偏移量开始划分序列\n",
    "    offset = random.randint(0, num_steps)\n",
    "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
    "    Xs = torch.tensor(corpus[offset: offset + num_tokens])\n",
    "    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
    "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
    "    num_batches = Xs.shape[1] // num_steps\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\n",
    "        X = Xs[:, i: i + num_steps]\n",
    "        Y = Ys[:, i: i + num_steps]\n",
    "        yield X, Y\n",
    "\n",
    "class SeqDataLoader:  #@save\n",
    "    \"\"\"加载序列数据的迭代器\"\"\"\n",
    "    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
    "        if use_random_iter:\n",
    "            self.data_iter_fn = seq_data_iter_random\n",
    "        else:\n",
    "            self.data_iter_fn = seq_data_iter_sequential\n",
    "        self.corpus, self.vocab = load_corpus_time_machine(max_tokens)\n",
    "        self.batch_size, self.num_steps = batch_size, num_steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)\n",
    "\n",
    "def load_data_time_machine(batch_size, num_steps,  #@save\n",
    "                           use_random_iter=False, max_tokens=10000):\n",
    "    \"\"\"返回时光机器数据集的迭代器和词表\"\"\"\n",
    "    data_iter = SeqDataLoader(\n",
    "        batch_size, num_steps, use_random_iter, max_tokens)\n",
    "    return data_iter, data_iter.vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a355b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "#@save\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"循环神经网络模型\"\"\"\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1\n",
    "        if not self.rnn.bidirectional:\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "        else:\n",
    "            self.num_directions = 2\n",
    "            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        # 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)\n",
    "        # 它的输出形状是(时间步数*批量大小,词表大小)。\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # nn.GRU以张量作为隐状态\n",
    "            return  torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens),\n",
    "                                device=device)\n",
    "        else:\n",
    "            # nn.LSTM以元组作为隐状态\n",
    "            return (torch.zeros((\n",
    "                self.num_directions * self.rnn.num_layers,\n",
    "                batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((\n",
    "                        self.num_directions * self.rnn.num_layers,\n",
    "                        batch_size, self.num_hiddens), device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e6bf754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "\n",
    "class Timer:  #@save\n",
    "    \"\"\"记录多次运行时间\"\"\"\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"启动计时器\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"停止计时器并将时间记录在列表中\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"返回平均时间\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"返回时间总和\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"返回累计时间\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()\n",
    "\n",
    "class Accumulator:  #@save\n",
    "    \"\"\"在n个变量上累加\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def grad_clipping(net, theta):  #@save\n",
    "    \"\"\"裁剪梯度\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm\n",
    "\n",
    "#@save\n",
    "def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n",
    "    \"\"\"训练网络一个迭代周期（定义见第8章）\"\"\"\n",
    "    state, timer = None, Timer()\n",
    "    metric = Accumulator(2)  # 训练损失之和,词元数量\n",
    "    for X, Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            # 在第一次迭代或使用随机抽样时初始化state\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "        else:\n",
    "            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
    "                # state对于nn.GRU是个张量\n",
    "                state.detach_()\n",
    "            else:\n",
    "                # state对于nn.LSTM或对于我们从零开始实现的模型是个张量\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        y = Y.T.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat, state = net(X, state)\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            # 因为已经调用了mean函数\n",
    "            updater(batch_size=1)\n",
    "        metric.add(l * y.numel(), y.numel())\n",
    "    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e0699af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):  #@save\n",
    "    \"\"\"小批量随机梯度下降\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()\n",
    "\n",
    "from IPython import display\n",
    "class Animator:  #@save\n",
    "    \"\"\"在动画中绘制数据\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        # 增量地绘制多条线\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        \n",
    "        # 替换 d2l.use_svg_display()\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.rcParams['figure.figsize'] = figsize\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # 替换 d2l.set_axes，使用 lambda 函数直接配置参数\n",
    "        self.config_axes = lambda: self._set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def _set_axes(self, ax, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "        \"\"\"设置坐标轴标签、范围和比例\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        # 设置坐标轴标签\n",
    "        if xlabel:\n",
    "            ax.set_xlabel(xlabel)\n",
    "        if ylabel:\n",
    "            ax.set_ylabel(ylabel)\n",
    "        # 设置坐标轴范围\n",
    "        if xlim:\n",
    "            ax.set_xlim(xlim)\n",
    "        if ylim:\n",
    "            ax.set_ylim(ylim)\n",
    "        # 设置坐标轴比例\n",
    "        if xscale:\n",
    "            ax.set_xscale(xscale)\n",
    "        if yscale:\n",
    "            ax.set_yscale(yscale)\n",
    "        # 设置图例\n",
    "        if legend:\n",
    "            ax.legend(legend)\n",
    "        # 添加网格\n",
    "        ax.grid(True)\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # 向图表中添加多个数据点\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "def predict_ch8(prefix, num_preds, net, vocab, device):  #@save\n",
    "    \"\"\"在prefix后面生成新字符\"\"\"\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
    "    for y in prefix[1:]:  # 预热期\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):  # 预测num_preds步\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])\n",
    "\n",
    "#@save\n",
    "def train_ch8(net, train_iter, vocab, lr, num_epochs, device,\n",
    "              use_random_iter=False):\n",
    "    \"\"\"训练模型（定义见第8章）\"\"\"\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    animator = Animator(xlabel='epoch', ylabel='perplexity',\n",
    "                            legend=['train'], xlim=[10, num_epochs])\n",
    "    # 初始化\n",
    "    if isinstance(net, nn.Module):\n",
    "        updater = torch.optim.SGD(net.parameters(), lr)\n",
    "    else:\n",
    "        updater = lambda batch_size: sgd(net.params, lr, batch_size)\n",
    "    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)\n",
    "    # 训练和预测\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl, speed = train_epoch_ch8(\n",
    "            net, train_iter, loss, updater, device, use_random_iter)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict('time traveller'))\n",
    "            animator.add(epoch + 1, [ppl])\n",
    "    print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')\n",
    "    print(predict('time traveller'))\n",
    "    print(predict('traveller'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40b9f7d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:18:55.992113Z",
     "iopub.status.busy": "2023-08-18T07:18:55.991547Z",
     "iopub.status.idle": "2023-08-18T07:19:45.366029Z",
     "shell.execute_reply": "2023-08-18T07:19:45.365163Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "困惑度 1.2, 10140.5 词元/秒 cpu\n",
      "time travellerlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlr\n",
      "travellerererrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# from d2l import torch as d2l\n",
    "\n",
    "# 加载数据\n",
    "batch_size, num_steps, device = 32, 35, try_gpu()\n",
    "train_iter, vocab = load_data_time_machine(batch_size, num_steps)\n",
    "# 通过设置“bidirective=True”来定义双向LSTM模型\n",
    "vocab_size, num_hiddens, num_layers = len(vocab), 256, 2\n",
    "num_inputs = vocab_size\n",
    "lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=True)\n",
    "model = RNNModel(lstm_layer, len(vocab))\n",
    "model = model.to(device)\n",
    "# 训练模型\n",
    "num_epochs, lr = 500, 1\n",
    "train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffafe68",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "上述结果显然令人瞠目结舌。\n",
    "关于如何更有效地使用双向循环神经网络的讨论，\n",
    "请参阅 :numref:`sec_sentiment_rnn`中的情感分类应用。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 在双向循环神经网络中，每个时间步的隐状态由当前时间步的前后数据同时决定。\n",
    "* 双向循环神经网络与概率图模型中的“前向-后向”算法具有相似性。\n",
    "* 双向循环神经网络主要用于序列编码和给定双向上下文的观测估计。\n",
    "* 由于梯度链更长，因此双向循环神经网络的训练代价非常高。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 如果不同方向使用不同数量的隐藏单位，$\\mathbf{H_t}$的形状会发生怎样的变化？\n",
    "1. 设计一个具有多个隐藏层的双向循环神经网络。\n",
    "1. 在自然语言中一词多义很常见。例如，“bank”一词在不同的上下文“i went to the bank to deposit cash”和“i went to the bank to sit down”中有不同的含义。如何设计一个神经网络模型，使其在给定上下文序列和单词的情况下，返回该单词在此上下文中的向量表示？哪种类型的神经网络架构更适合处理一词多义？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30b84f3",
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/2773)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
