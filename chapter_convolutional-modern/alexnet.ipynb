{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "076ca4f2",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 深度卷积神经网络（AlexNet）\n",
    ":label:`sec_alexnet`\n",
    "\n",
    "在LeNet提出后，卷积神经网络在计算机视觉和机器学习领域中很有名气。但卷积神经网络并没有主导这些领域。这是因为虽然LeNet在小数据集上取得了很好的效果，但是在更大、更真实的数据集上训练卷积神经网络的性能和可行性还有待研究。事实上，在上世纪90年代初到2012年之间的大部分时间里，神经网络往往被其他机器学习方法超越，如支持向量机（support vector machines）。\n",
    "\n",
    "在计算机视觉中，直接将神经网络与其他机器学习方法进行比较也许不公平。这是因为，卷积神经网络的输入是由原始像素值或是经过简单预处理（例如居中、缩放）的像素值组成的。但在使用传统机器学习方法时，从业者永远不会将原始像素作为输入。在传统机器学习方法中，计算机视觉流水线是由经过人的手工精心设计的特征流水线组成的。对于这些传统方法，大部分的进展都来自于对特征有了更聪明的想法，并且学习到的算法往往归于事后的解释。\n",
    "\n",
    "虽然上世纪90年代就有了一些神经网络加速卡，但仅靠它们还不足以开发出有大量参数的深层多通道多层卷积神经网络。此外，当时的数据集仍然相对较小。除了这些障碍，训练神经网络的一些关键技巧仍然缺失，包括启发式参数初始化、随机梯度下降的变体、非挤压激活函数和有效的正则化技术。\n",
    "\n",
    "因此，与训练*端到端*（从像素到分类结果）系统不同，经典机器学习的流水线看起来更像下面这样：\n",
    "\n",
    "1. 获取一个有趣的数据集。在早期，收集这些数据集需要昂贵的传感器（在当时最先进的图像也就100万像素）。\n",
    "2. 根据光学、几何学、其他知识以及偶然的发现，手工对特征数据集进行预处理。\n",
    "3. 通过标准的特征提取算法，如SIFT（尺度不变特征变换） :cite:`Lowe.2004`和SURF（加速鲁棒特征） :cite:`Bay.Tuytelaars.Van-Gool.2006`或其他手动调整的流水线来输入数据。\n",
    "4. 将提取的特征送入最喜欢的分类器中（例如线性模型或其它核方法），以训练分类器。\n",
    "\n",
    "当人们和机器学习研究人员交谈时，会发现机器学习研究人员相信机器学习既重要又美丽：优雅的理论去证明各种模型的性质。机器学习是一个正在蓬勃发展、严谨且非常有用的领域。然而，当人们和计算机视觉研究人员交谈，会听到一个完全不同的故事。计算机视觉研究人员会告诉一个诡异事实————推动领域进步的是数据特征，而不是学习算法。计算机视觉研究人员相信，从对最终模型精度的影响来说，更大或更干净的数据集、或是稍微改进的特征提取，比任何学习算法带来的进步要大得多。\n",
    "\n",
    "## 学习表征\n",
    "\n",
    "另一种预测这个领域发展的方法————观察图像特征的提取方法。在2012年前，图像特征都是机械地计算出来的。事实上，设计一套新的特征函数、改进结果，并撰写论文是盛极一时的潮流。SIFT :cite:`Lowe.2004`、SURF :cite:`Bay.Tuytelaars.Van-Gool.2006`、HOG（定向梯度直方图） :cite:`Dalal.Triggs.2005`、[bags of visual words](https://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision)和类似的特征提取方法占据了主导地位。\n",
    "\n",
    "另一组研究人员，包括Yann LeCun、Geoff Hinton、Yoshua Bengio、Andrew Ng、Shun ichi Amari和Juergen Schmidhuber，想法则与众不同：他们认为特征本身应该被学习。此外，他们还认为，在合理地复杂性前提下，特征应该由多个共同学习的神经网络层组成，每个层都有可学习的参数。在机器视觉中，最底层可能检测边缘、颜色和纹理。事实上，Alex Krizhevsky、Ilya Sutskever和Geoff Hinton提出了一种新的卷积神经网络变体*AlexNet*。在2012年ImageNet挑战赛中取得了轰动一时的成绩。AlexNet以Alex Krizhevsky的名字命名，他是论文 :cite:`Krizhevsky.Sutskever.Hinton.2012`的第一作者。\n",
    "\n",
    "有趣的是，在网络的最底层，模型学习到了一些类似于传统滤波器的特征抽取器。 :numref:`fig_filters`是从AlexNet论文 :cite:`Krizhevsky.Sutskever.Hinton.2012`复制的，描述了底层图像特征。\n",
    "\n",
    "![AlexNet第一层学习到的特征抽取器。](../img/filters.png)\n",
    ":width:`400px`\n",
    ":label:`fig_filters`\n",
    "\n",
    "AlexNet的更高层建立在这些底层表示的基础上，以表示更大的特征，如眼睛、鼻子、草叶等等。而更高的层可以检测整个物体，如人、飞机、狗或飞盘。最终的隐藏神经元可以学习图像的综合表示，从而使属于不同类别的数据易于区分。尽管一直有一群执着的研究者不断钻研，试图学习视觉数据的逐级表征，然而很长一段时间里这些尝试都未有突破。深度卷积神经网络的突破出现在2012年。突破可归因于两个关键因素。\n",
    "\n",
    "### 缺少的成分：数据\n",
    "\n",
    "包含许多特征的深度模型需要大量的有标签数据，才能显著优于基于凸优化的传统方法（如线性方法和核方法）。\n",
    "然而，限于早期计算机有限的存储和90年代有限的研究预算，大部分研究只基于小的公开数据集。例如，不少研究论文基于加州大学欧文分校（UCI）提供的若干个公开数据集，其中许多数据集只有几百至几千张在非自然环境下以低分辨率拍摄的图像。这一状况在2010年前后兴起的大数据浪潮中得到改善。2009年，ImageNet数据集发布，并发起ImageNet挑战赛：要求研究人员从100万个样本中训练模型，以区分1000个不同类别的对象。ImageNet数据集由斯坦福教授李飞飞小组的研究人员开发，利用谷歌图像搜索（Google Image Search）对每一类图像进行预筛选，并利用亚马逊众包（Amazon Mechanical Turk）来标注每张图片的相关类别。这种规模是前所未有的。这项被称为ImageNet的挑战赛推动了计算机视觉和机器学习研究的发展，挑战研究人员确定哪些模型能够在更大的数据规模下表现最好。\n",
    "\n",
    "### 缺少的成分：硬件\n",
    "\n",
    "深度学习对计算资源要求很高，训练可能需要数百个迭代轮数，每次迭代都需要通过代价高昂的许多线性代数层传递数据。这也是为什么在20世纪90年代至21世纪初，优化凸目标的简单算法是研究人员的首选。然而，用GPU训练神经网络改变了这一格局。*图形处理器*（Graphics Processing Unit，GPU）早年用来加速图形处理，使电脑游戏玩家受益。GPU可优化高吞吐量的$4 \\times 4$矩阵和向量乘法，从而服务于基本的图形任务。幸运的是，这些数学运算与卷积层的计算惊人地相似。由此，英伟达（NVIDIA）和ATI已经开始为通用计算操作优化gpu，甚至把它们作为*通用GPU*（general-purpose GPUs，GPGPU）来销售。\n",
    "\n",
    "那么GPU比CPU强在哪里呢？\n",
    "\n",
    "首先，我们深度理解一下中央处理器（Central Processing Unit，CPU）的*核心*。\n",
    "CPU的每个核心都拥有高时钟频率的运行能力，和高达数MB的三级缓存（L3Cache）。\n",
    "它们非常适合执行各种指令，具有分支预测器、深层流水线和其他使CPU能够运行各种程序的功能。\n",
    "然而，这种明显的优势也是它的致命弱点：通用核心的制造成本非常高。\n",
    "它们需要大量的芯片面积、复杂的支持结构（内存接口、内核之间的缓存逻辑、高速互连等等），而且它们在任何单个任务上的性能都相对较差。\n",
    "现代笔记本电脑最多有4核，即使是高端服务器也很少超过64核，因为它们的性价比不高。\n",
    "\n",
    "相比于CPU，GPU由$100 \\sim 1000$个小的处理单元组成（NVIDIA、ATI、ARM和其他芯片供应商之间的细节稍有不同），通常被分成更大的组（NVIDIA称之为warps）。\n",
    "虽然每个GPU核心都相对较弱，有时甚至以低于1GHz的时钟频率运行，但庞大的核心数量使GPU比CPU快几个数量级。\n",
    "例如，NVIDIA最近一代的Ampere GPU架构为每个芯片提供了高达312 TFlops的浮点性能，而CPU的浮点性能到目前为止还没有超过1 TFlops。\n",
    "之所以有如此大的差距，原因其实很简单：首先，功耗往往会随时钟频率呈二次方增长。\n",
    "对于一个CPU核心，假设它的运行速度比GPU快4倍，但可以使用16个GPU核代替，那么GPU的综合性能就是CPU的$16 \\times 1/4 = 4$倍。\n",
    "其次，GPU内核要简单得多，这使得它们更节能。\n",
    "此外，深度学习中的许多操作需要相对较高的内存带宽，而GPU拥有10倍于CPU的带宽。\n",
    "\n",
    "回到2012年的重大突破，当Alex Krizhevsky和Ilya Sutskever实现了可以在GPU硬件上运行的深度卷积神经网络时，一个重大突破出现了。他们意识到卷积神经网络中的计算瓶颈：卷积和矩阵乘法，都是可以在硬件上并行化的操作。\n",
    "于是，他们使用两个显存为3GB的NVIDIA GTX580 GPU实现了快速卷积运算。他们的创新[cuda-convnet](https://code.google.com/archive/p/cuda-convnet/)几年来它一直是行业标准，并推动了深度学习热潮。\n",
    "\n",
    "## AlexNet\n",
    "\n",
    "2012年，AlexNet横空出世。它首次证明了学习到的特征可以超越手工设计的特征。它一举打破了计算机视觉研究的现状。\n",
    "AlexNet使用了8层卷积神经网络，并以很大的优势赢得了2012年ImageNet图像识别挑战赛。\n",
    "\n",
    "AlexNet和LeNet的架构非常相似，如 :numref:`fig_alexnet`所示。\n",
    "注意，本书在这里提供的是一个稍微精简版本的AlexNet，去除了当年需要两个小型GPU同时运算的设计特点。\n",
    "\n",
    "![从LeNet（左）到AlexNet（右）](../img/alexnet.svg)\n",
    ":label:`fig_alexnet`\n",
    "\n",
    "AlexNet和LeNet的设计理念非常相似，但也存在显著差异。\n",
    "\n",
    "1. AlexNet比相对较小的LeNet5要深得多。AlexNet由八层组成：五个卷积层、两个全连接隐藏层和一个全连接输出层。\n",
    "2. AlexNet使用ReLU而不是sigmoid作为其激活函数。\n",
    "\n",
    "下面的内容将深入研究AlexNet的细节。\n",
    "\n",
    "### 模型设计\n",
    "\n",
    "在AlexNet的第一层，卷积窗口的形状是$11\\times11$。\n",
    "由于ImageNet中大多数图像的宽和高比MNIST图像的多10倍以上，因此，需要一个更大的卷积窗口来捕获目标。\n",
    "第二层中的卷积窗口形状被缩减为$5\\times5$，然后是$3\\times3$。\n",
    "此外，在第一层、第二层和第五层卷积层之后，加入窗口形状为$3\\times3$、步幅为2的最大汇聚层。\n",
    "而且，AlexNet的卷积通道数目是LeNet的10倍。\n",
    "\n",
    "在最后一个卷积层后有两个全连接层，分别有4096个输出。\n",
    "这两个巨大的全连接层拥有将近1GB的模型参数。\n",
    "由于早期GPU显存有限，原版的AlexNet采用了双数据流设计，使得每个GPU只负责存储和计算模型的一半参数。\n",
    "幸运的是，现在GPU显存相对充裕，所以现在很少需要跨GPU分解模型（因此，本书的AlexNet模型在这方面与原始论文稍有不同）。\n",
    "\n",
    "### 激活函数\n",
    "\n",
    "此外，AlexNet将sigmoid激活函数改为更简单的ReLU激活函数。\n",
    "一方面，ReLU激活函数的计算更简单，它不需要如sigmoid激活函数那般复杂的求幂运算。\n",
    "另一方面，当使用不同的参数初始化方法时，ReLU激活函数使训练模型更加容易。\n",
    "当sigmoid激活函数的输出非常接近于0或1时，这些区域的梯度几乎为0，因此反向传播无法继续更新一些模型参数。\n",
    "相反，ReLU激活函数在正区间的梯度总是1。\n",
    "因此，如果模型参数没有正确初始化，sigmoid函数可能在正区间内得到几乎为0的梯度，从而使模型无法得到有效的训练。\n",
    "\n",
    "### 容量控制和预处理\n",
    "\n",
    "AlexNet通过暂退法（ :numref:`sec_dropout`）控制全连接层的模型复杂度，而LeNet只使用了权重衰减。\n",
    "为了进一步扩充数据，AlexNet在训练时增加了大量的图像增强数据，如翻转、裁切和变色。\n",
    "这使得模型更健壮，更大的样本量有效地减少了过拟合。\n",
    "在 :numref:`sec_image_augmentation`中更详细地讨论数据扩增。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c793505",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:25:53.331824Z",
     "iopub.status.busy": "2023-08-18T07:25:53.331288Z",
     "iopub.status.idle": "2023-08-18T07:25:56.717289Z",
     "shell.execute_reply": "2023-08-18T07:25:56.716283Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# from d2l import torch as d2l\n",
    "\n",
    "net = nn.Sequential(\n",
    "    # 这里使用一个11*11的更大窗口来捕捉对象。\n",
    "    # 同时，步幅为4，以减少输出的高度和宽度。\n",
    "    # 另外，输出通道的数目远大于LeNet\n",
    "    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
    "    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    # 使用三个连续的卷积层和较小的卷积窗口。\n",
    "    # 除了最后的卷积层，输出通道的数量进一步增加。\n",
    "    # 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度\n",
    "    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nn.Flatten(),\n",
    "    # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合\n",
    "    nn.Linear(6400, 4096), nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(4096, 4096), nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    # 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000\n",
    "    nn.Linear(4096, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dde541",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "[**我们构造一个**]高度和宽度都为224的(**单通道数据，来观察每一层输出的形状**)。\n",
    "它与 :numref:`fig_alexnet`中的AlexNet架构相匹配。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77cc7ef4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:25:56.723055Z",
     "iopub.status.busy": "2023-08-18T07:25:56.722359Z",
     "iopub.status.idle": "2023-08-18T07:25:56.778616Z",
     "shell.execute_reply": "2023-08-18T07:25:56.777770Z"
    },
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d output shape:\t torch.Size([1, 96, 54, 54])\n",
      "ReLU output shape:\t torch.Size([1, 96, 54, 54])\n",
      "MaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\n",
      "Conv2d output shape:\t torch.Size([1, 256, 26, 26])\n",
      "ReLU output shape:\t torch.Size([1, 256, 26, 26])\n",
      "MaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\n",
      "Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n",
      "ReLU output shape:\t torch.Size([1, 384, 12, 12])\n",
      "Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n",
      "ReLU output shape:\t torch.Size([1, 384, 12, 12])\n",
      "Conv2d output shape:\t torch.Size([1, 256, 12, 12])\n",
      "ReLU output shape:\t torch.Size([1, 256, 12, 12])\n",
      "MaxPool2d output shape:\t torch.Size([1, 256, 5, 5])\n",
      "Flatten output shape:\t torch.Size([1, 6400])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(1, 1, 224, 224)\n",
    "for layer in net:\n",
    "    X=layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t',X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf9d2fb",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "## 读取数据集\n",
    "\n",
    "尽管原文中AlexNet是在ImageNet上进行训练的，但本书在这里使用的是Fashion-MNIST数据集。因为即使在现代GPU上，训练ImageNet模型，同时使其收敛可能需要数小时或数天的时间。\n",
    "将AlexNet直接应用于Fashion-MNIST的一个问题是，[**Fashion-MNIST图像的分辨率**]（$28 \\times 28$像素）(**低于ImageNet图像。**)\n",
    "为了解决这个问题，(**我们将它们增加到$224 \\times 224$**)（通常来讲这不是一个明智的做法，但在这里这样做是为了有效使用AlexNet架构）。\n",
    "这里需要使用`d2l.load_data_fashion_mnist`函数中的`resize`参数执行此调整。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1afceea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:25:56.782593Z",
     "iopub.status.busy": "2023-08-18T07:25:56.782045Z",
     "iopub.status.idle": "2023-08-18T07:25:56.870711Z",
     "shell.execute_reply": "2023-08-18T07:25:56.869484Z"
    },
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "def get_dataloader_workers():  #@save\n",
    "    \"\"\"使用4个进程来读取数据\"\"\"\n",
    "    return 4\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None):  #@save\n",
    "    \"\"\"下载Fashion-MNIST数据集，然后将其加载到内存中\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=True, transform=trans, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                            num_workers=get_dataloader_workers()),\n",
    "            data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "                            num_workers=get_dataloader_workers()))\n",
    "\n",
    "batch_size = 128\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f94523",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "## [**训练AlexNet**]\n",
    "\n",
    "现在AlexNet可以开始被训练了。与 :numref:`sec_lenet`中的LeNet相比，这里的主要变化是使用更小的学习速率训练，这是因为网络更深更广、图像分辨率更高，训练卷积神经网络就更昂贵。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67cdc7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:  #@save\n",
    "    \"\"\"在n个变量上累加\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def accuracy(y_hat, y):  #@save\n",
    "    \"\"\"计算预测正确的数量\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "\n",
    "def evaluate_accuracy_gpu(net, data_iter, device=None): #@save\n",
    "    \"\"\"使用GPU计算模型在数据集上的精度\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()  # 设置为评估模式\n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    # 正确预测的数量，总预测的数量\n",
    "    metric = Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(X, list):\n",
    "                # BERT微调所需的（之后将介绍）\n",
    "                X = [x.to(device) for x in X]\n",
    "            else:\n",
    "                X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            metric.add(accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1737d1ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:25:56.876133Z",
     "iopub.status.busy": "2023-08-18T07:25:56.875855Z",
     "iopub.status.idle": "2023-08-18T07:29:34.981913Z",
     "shell.execute_reply": "2023-08-18T07:29:34.980933Z"
    },
    "origin_pos": 13,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 146\u001b[39m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.device(\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    145\u001b[39m lr, num_epochs = \u001b[32m0.01\u001b[39m, \u001b[32m10\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m \u001b[43mtrain_ch6\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 122\u001b[39m, in \u001b[36mtrain_ch6\u001b[39m\u001b[34m(net, train_iter, test_iter, num_epochs, lr, device)\u001b[39m\n\u001b[32m    120\u001b[39m y_hat = net(X)\n\u001b[32m    121\u001b[39m l = loss(y_hat, y)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[43ml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m optimizer.step()\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\d2l-pytorch-notes\\.venv\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\d2l-pytorch-notes\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\d2l-pytorch-notes\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython import display\n",
    "\n",
    "class Animator:  #@save\n",
    "    \"\"\"在动画中绘制数据\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        # 增量地绘制多条线\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        \n",
    "        # 替换 d2l.use_svg_display()\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.rcParams['figure.figsize'] = figsize\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # 替换 d2l.set_axes，使用 lambda 函数直接配置参数\n",
    "        self.config_axes = lambda: self._set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def _set_axes(self, ax, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "        \"\"\"设置坐标轴标签、范围和比例\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        # 设置坐标轴标签\n",
    "        if xlabel:\n",
    "            ax.set_xlabel(xlabel)\n",
    "        if ylabel:\n",
    "            ax.set_ylabel(ylabel)\n",
    "        # 设置坐标轴范围\n",
    "        if xlim:\n",
    "            ax.set_xlim(xlim)\n",
    "        if ylim:\n",
    "            ax.set_ylim(ylim)\n",
    "        # 设置坐标轴比例\n",
    "        if xscale:\n",
    "            ax.set_xscale(xscale)\n",
    "        if yscale:\n",
    "            ax.set_yscale(yscale)\n",
    "        # 设置图例\n",
    "        if legend:\n",
    "            ax.legend(legend)\n",
    "        # 添加网格\n",
    "        ax.grid(True)\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # 向图表中添加多个数据点\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "import time\n",
    "class Timer:  #@save\n",
    "    \"\"\"记录多次运行时间\"\"\"\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"启动计时器\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"停止计时器并将时间记录在列表中\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"返回平均时间\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"返回时间总和\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"返回累计时间\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()\n",
    "\n",
    "#@save\n",
    "def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):\n",
    "    \"\"\"用GPU训练模型(在第六章定义)\"\"\"\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                            legend=['train loss', 'train acc', 'test acc'])\n",
    "    timer, num_batches = Timer(), len(train_iter)\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练损失之和，训练准确率之和，样本数\n",
    "        metric = Accumulator(3)\n",
    "        net.train()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            optimizer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l * X.shape[0], accuracy(y_hat, y), X.shape[0])\n",
    "            timer.stop()\n",
    "            train_l = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (train_l, train_acc, None))\n",
    "        test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "        animator.add(epoch + 1, (None, None, test_acc))\n",
    "    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
    "          f'test acc {test_acc:.3f}')\n",
    "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n",
    "          f'on {str(device)}')\n",
    "\n",
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "lr, num_epochs = 0.01, 10\n",
    "train_ch6(net, train_iter, test_iter, num_epochs, lr, try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1dd27a",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "## 小结\n",
    "\n",
    "* AlexNet的架构与LeNet相似，但使用了更多的卷积层和更多的参数来拟合大规模的ImageNet数据集。\n",
    "* 今天，AlexNet已经被更有效的架构所超越，但它是从浅层网络到深层网络的关键一步。\n",
    "* 尽管AlexNet的代码只比LeNet多出几行，但学术界花了很多年才接受深度学习这一概念，并应用其出色的实验结果。这也是由于缺乏有效的计算工具。\n",
    "* Dropout、ReLU和预处理是提升计算机视觉任务性能的其他关键步骤。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 试着增加迭代轮数。对比LeNet的结果有什么不同？为什么？\n",
    "1. AlexNet对Fashion-MNIST数据集来说可能太复杂了。\n",
    "    1. 尝试简化模型以加快训练速度，同时确保准确性不会显著下降。\n",
    "    1. 设计一个更好的模型，可以直接在$28 \\times 28$图像上工作。\n",
    "1. 修改批量大小，并观察模型精度和GPU显存变化。\n",
    "1. 分析了AlexNet的计算性能。\n",
    "    1. 在AlexNet中主要是哪部分占用显存？\n",
    "    1. 在AlexNet中主要是哪部分需要更多的计算？\n",
    "    1. 计算结果时显存带宽如何？\n",
    "1. 将dropout和ReLU应用于LeNet-5，效果有提升吗？再试试预处理会怎么样？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7add4d8d",
   "metadata": {
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/1863)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
