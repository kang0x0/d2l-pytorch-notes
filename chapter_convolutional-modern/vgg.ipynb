{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a44b7e5",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 使用块的网络（VGG）\n",
    ":label:`sec_vgg`\n",
    "\n",
    "虽然AlexNet证明深层神经网络卓有成效，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络。\n",
    "在下面的几个章节中，我们将介绍一些常用于设计深层神经网络的启发式概念。\n",
    "\n",
    "与芯片设计中工程师从放置晶体管到逻辑元件再到逻辑块的过程类似，神经网络架构的设计也逐渐变得更加抽象。研究人员开始从单个神经元的角度思考问题，发展到整个层，现在又转向块，重复层的模式。\n",
    "\n",
    "使用块的想法首先出现在牛津大学的[视觉几何组（visual geometry group）](http://www.robots.ox.ac.uk/~vgg/)的*VGG网络*中。通过使用循环和子程序，可以很容易地在任何现代深度学习框架的代码中实现这些重复的架构。\n",
    "\n",
    "## (**VGG块**)\n",
    "\n",
    "经典卷积神经网络的基本组成部分是下面的这个序列：\n",
    "\n",
    "1. 带填充以保持分辨率的卷积层；\n",
    "1. 非线性激活函数，如ReLU；\n",
    "1. 汇聚层，如最大汇聚层。\n",
    "\n",
    "而一个VGG块与之类似，由一系列卷积层组成，后面再加上用于空间下采样的最大汇聚层。在最初的VGG论文中 :cite:`Simonyan.Zisserman.2014`，作者使用了带有$3\\times3$卷积核、填充为1（保持高度和宽度）的卷积层，和带有$2 \\times 2$汇聚窗口、步幅为2（每个块后的分辨率减半）的最大汇聚层。在下面的代码中，我们定义了一个名为`vgg_block`的函数来实现一个VGG块。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe0e7e",
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "该函数有三个参数，分别对应于卷积层的数量`num_convs`、输入通道的数量`in_channels`\n",
    "和输出通道的数量`out_channels`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "328fda20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:17:59.402403Z",
     "iopub.status.busy": "2023-08-18T07:17:59.401674Z",
     "iopub.status.idle": "2023-08-18T07:18:04.621162Z",
     "shell.execute_reply": "2023-08-18T07:18:04.619845Z"
    },
    "origin_pos": 4,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# from d2l import torch as d2l\n",
    "\n",
    "\n",
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels,\n",
    "                                kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e136ba7b",
   "metadata": {
    "origin_pos": 7
   },
   "source": [
    "## [**VGG网络**]\n",
    "\n",
    "与AlexNet、LeNet一样，VGG网络可以分为两部分：第一部分主要由卷积层和汇聚层组成，第二部分由全连接层组成。如 :numref:`fig_vgg`中所示。\n",
    "\n",
    "![从AlexNet到VGG，它们本质上都是块设计。](../img/vgg.svg)\n",
    ":width:`400px`\n",
    ":label:`fig_vgg`\n",
    "\n",
    "VGG神经网络连接 :numref:`fig_vgg`的几个VGG块（在`vgg_block`函数中定义）。其中有超参数变量`conv_arch`。该变量指定了每个VGG块里卷积层个数和输出通道数。全连接模块则与AlexNet中的相同。\n",
    "\n",
    "原始VGG网络有5个卷积块，其中前两个块各有一个卷积层，后三个块各包含两个卷积层。\n",
    "第一个模块有64个输出通道，每个后续模块将输出通道数量翻倍，直到该数字达到512。由于该网络使用8个卷积层和3个全连接层，因此它通常被称为VGG-11。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93cf8f6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:18:04.627044Z",
     "iopub.status.busy": "2023-08-18T07:18:04.626251Z",
     "iopub.status.idle": "2023-08-18T07:18:04.632251Z",
     "shell.execute_reply": "2023-08-18T07:18:04.631189Z"
    },
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d59caf",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "下面的代码实现了VGG-11。可以通过在`conv_arch`上执行for循环来简单实现。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3daef8f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:18:04.637160Z",
     "iopub.status.busy": "2023-08-18T07:18:04.636507Z",
     "iopub.status.idle": "2023-08-18T07:18:06.122904Z",
     "shell.execute_reply": "2023-08-18T07:18:06.121824Z"
    },
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def vgg(conv_arch):\n",
    "    conv_blks = []\n",
    "    in_channels = 1\n",
    "    # 卷积层部分\n",
    "    for (num_convs, out_channels) in conv_arch:\n",
    "        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))\n",
    "        in_channels = out_channels\n",
    "\n",
    "    return nn.Sequential(\n",
    "        *conv_blks, nn.Flatten(),\n",
    "        # 全连接层部分\n",
    "        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 10))\n",
    "\n",
    "net = vgg(conv_arch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf269a2",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "接下来，我们将构建一个高度和宽度为224的单通道数据样本，以[**观察每个层输出的形状**]。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3f0eba0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:18:06.128720Z",
     "iopub.status.busy": "2023-08-18T07:18:06.128055Z",
     "iopub.status.idle": "2023-08-18T07:18:06.433530Z",
     "shell.execute_reply": "2023-08-18T07:18:06.432421Z"
    },
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 112, 112])\n",
      "Sequential output shape:\t torch.Size([1, 128, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 256, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 512, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
      "Flatten output shape:\t torch.Size([1, 25088])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(size=(1, 1, 224, 224))\n",
    "for blk in net:\n",
    "    X = blk(X)\n",
    "    print(blk.__class__.__name__,'output shape:\\t',X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d53faf",
   "metadata": {
    "origin_pos": 19
   },
   "source": [
    "正如从代码中所看到的，我们在每个块的高度和宽度减半，最终高度和宽度都为7。最后再展平表示，送入全连接层处理。\n",
    "\n",
    "## 训练模型\n",
    "\n",
    "[**由于VGG-11比AlexNet计算量更大，因此我们构建了一个通道数较少的网络**]，足够用于训练Fashion-MNIST数据集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f90a296d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:18:06.441259Z",
     "iopub.status.busy": "2023-08-18T07:18:06.439026Z",
     "iopub.status.idle": "2023-08-18T07:18:06.938917Z",
     "shell.execute_reply": "2023-08-18T07:18:06.938010Z"
    },
    "origin_pos": 20,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "ratio = 4\n",
    "small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]\n",
    "net = vgg(small_conv_arch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4a93ee",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "除了使用略高的学习率外，[**模型训练**]过程与 :numref:`sec_alexnet`中的AlexNet类似。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "017eedeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "def get_dataloader_workers():  #@save\n",
    "    \"\"\"使用4个进程来读取数据\"\"\"\n",
    "    return 4\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None):  #@save\n",
    "    \"\"\"下载Fashion-MNIST数据集，然后将其加载到内存中\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=True, transform=trans, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                            num_workers=get_dataloader_workers()),\n",
    "            data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "                            num_workers=get_dataloader_workers()))\n",
    "\n",
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "class Accumulator:  #@save\n",
    "    \"\"\"在n个变量上累加\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def accuracy(y_hat, y):  #@save\n",
    "    \"\"\"计算预测正确的数量\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "\n",
    "def evaluate_accuracy_gpu(net, data_iter, device=None): #@save\n",
    "    \"\"\"使用GPU计算模型在数据集上的精度\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()  # 设置为评估模式\n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    # 正确预测的数量，总预测的数量\n",
    "    metric = Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(X, list):\n",
    "                # BERT微调所需的（之后将介绍）\n",
    "                X = [x.to(device) for x in X]\n",
    "            else:\n",
    "                X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            metric.add(accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]\n",
    "\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "class Animator:  #@save\n",
    "    \"\"\"在动画中绘制数据\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        # 增量地绘制多条线\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        \n",
    "        # 替换 d2l.use_svg_display()\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.rcParams['figure.figsize'] = figsize\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # 替换 d2l.set_axes，使用 lambda 函数直接配置参数\n",
    "        self.config_axes = lambda: self._set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def _set_axes(self, ax, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "        \"\"\"设置坐标轴标签、范围和比例\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        # 设置坐标轴标签\n",
    "        if xlabel:\n",
    "            ax.set_xlabel(xlabel)\n",
    "        if ylabel:\n",
    "            ax.set_ylabel(ylabel)\n",
    "        # 设置坐标轴范围\n",
    "        if xlim:\n",
    "            ax.set_xlim(xlim)\n",
    "        if ylim:\n",
    "            ax.set_ylim(ylim)\n",
    "        # 设置坐标轴比例\n",
    "        if xscale:\n",
    "            ax.set_xscale(xscale)\n",
    "        if yscale:\n",
    "            ax.set_yscale(yscale)\n",
    "        # 设置图例\n",
    "        if legend:\n",
    "            ax.legend(legend)\n",
    "        # 添加网格\n",
    "        ax.grid(True)\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # 向图表中添加多个数据点\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "import time\n",
    "class Timer:  #@save\n",
    "    \"\"\"记录多次运行时间\"\"\"\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"启动计时器\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"停止计时器并将时间记录在列表中\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"返回平均时间\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"返回时间总和\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"返回累计时间\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()\n",
    "\n",
    "#@save\n",
    "def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):\n",
    "    \"\"\"用GPU训练模型(在第六章定义)\"\"\"\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                            legend=['train loss', 'train acc', 'test acc'])\n",
    "    timer, num_batches = Timer(), len(train_iter)\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练损失之和，训练准确率之和，样本数\n",
    "        metric = Accumulator(3)\n",
    "        net.train()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            optimizer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l * X.shape[0], accuracy(y_hat, y), X.shape[0])\n",
    "            timer.stop()\n",
    "            train_l = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (train_l, train_acc, None))\n",
    "        test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "        animator.add(epoch + 1, (None, None, test_acc))\n",
    "    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
    "          f'test acc {test_acc:.3f}')\n",
    "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n",
    "          f'on {str(device)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9512213b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:18:06.943484Z",
     "iopub.status.busy": "2023-08-18T07:18:06.942820Z",
     "iopub.status.idle": "2023-08-18T07:23:21.330858Z",
     "shell.execute_reply": "2023-08-18T07:23:21.329877Z"
    },
    "origin_pos": 23,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m lr, num_epochs, batch_size = \u001b[32m0.05\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m128\u001b[39m\n\u001b[32m      2\u001b[39m train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=\u001b[32m224\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtrain_ch6\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 191\u001b[39m, in \u001b[36mtrain_ch6\u001b[39m\u001b[34m(net, train_iter, test_iter, num_epochs, lr, device)\u001b[39m\n\u001b[32m    189\u001b[39m optimizer.zero_grad()\n\u001b[32m    190\u001b[39m X, y = X.to(device), y.to(device)\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m y_hat = \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m l = loss(y_hat, y)\n\u001b[32m    193\u001b[39m l.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\d2l-pytorch-notes\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\d2l-pytorch-notes\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\d2l-pytorch-notes\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\d2l-pytorch-notes\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\d2l-pytorch-notes\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\d2l-pytorch-notes\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\d2l-pytorch-notes\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\d2l-pytorch-notes\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\d2l-pytorch-notes\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\d2l-pytorch-notes\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr, num_epochs, batch_size = 0.05, 10, 128\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)\n",
    "train_ch6(net, train_iter, test_iter, num_epochs, lr, try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d421c970",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "## 小结\n",
    "\n",
    "* VGG-11使用可复用的卷积块构造网络。不同的VGG模型可通过每个块中卷积层数量和输出通道数量的差异来定义。\n",
    "* 块的使用导致网络定义的非常简洁。使用块可以有效地设计复杂的网络。\n",
    "* 在VGG论文中，Simonyan和Ziserman尝试了各种架构。特别是他们发现深层且窄的卷积（即$3 \\times 3$）比较浅层且宽的卷积更有效。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 打印层的尺寸时，我们只看到8个结果，而不是11个结果。剩余的3层信息去哪了？\n",
    "1. 与AlexNet相比，VGG的计算要慢得多，而且它还需要更多的显存。分析出现这种情况的原因。\n",
    "1. 尝试将Fashion-MNIST数据集图像的高度和宽度从224改为96。这对实验有什么影响？\n",
    "1. 请参考VGG论文 :cite:`Simonyan.Zisserman.2014`中的表1构建其他常见模型，如VGG-16或VGG-19。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567907d0",
   "metadata": {
    "origin_pos": 26,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/1866)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
