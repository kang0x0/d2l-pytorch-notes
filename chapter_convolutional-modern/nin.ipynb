{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "324028bf",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 网络中的网络（NiN）\n",
    ":label:`sec_nin`\n",
    "\n",
    "LeNet、AlexNet和VGG都有一个共同的设计模式：通过一系列的卷积层与汇聚层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。\n",
    "AlexNet和VGG对LeNet的改进主要在于如何扩大和加深这两个模块。\n",
    "或者，可以想象在这个过程的早期使用全连接层。然而，如果使用了全连接层，可能会完全放弃表征的空间结构。\n",
    "*网络中的网络*（*NiN*）提供了一个非常简单的解决方案：在每个像素的通道上分别使用多层感知机 :cite:`Lin.Chen.Yan.2013`\n",
    "\n",
    "## (**NiN块**)\n",
    "\n",
    "回想一下，卷积层的输入和输出由四维张量组成，张量的每个轴分别对应样本、通道、高度和宽度。\n",
    "另外，全连接层的输入和输出通常是分别对应于样本和特征的二维张量。\n",
    "NiN的想法是在每个像素位置（针对每个高度和宽度）应用一个全连接层。\n",
    "如果我们将权重连接到每个空间位置，我们可以将其视为$1\\times 1$卷积层（如 :numref:`sec_channels`中所述），或作为在每个像素位置上独立作用的全连接层。\n",
    "从另一个角度看，即将空间维度中的每个像素视为单个样本，将通道维度视为不同特征（feature）。\n",
    "\n",
    " :numref:`fig_nin`说明了VGG和NiN及它们的块之间主要架构差异。\n",
    "NiN块以一个普通卷积层开始，后面是两个$1 \\times 1$的卷积层。这两个$1 \\times 1$卷积层充当带有ReLU激活函数的逐像素全连接层。\n",
    "第一层的卷积窗口形状通常由用户设置。\n",
    "随后的卷积窗口形状固定为$1 \\times 1$。\n",
    "\n",
    "![对比 VGG 和 NiN 及它们的块之间主要架构差异。](../img/nin.svg)\n",
    ":width:`600px`\n",
    ":label:`fig_nin`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b116832",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:19:49.388461Z",
     "iopub.status.busy": "2023-08-18T07:19:49.387694Z",
     "iopub.status.idle": "2023-08-18T07:19:52.241405Z",
     "shell.execute_reply": "2023-08-18T07:19:52.240502Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# from d2l import torch as d2l\n",
    "\n",
    "\n",
    "def nin_block(in_channels, out_channels, kernel_size, strides, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc11a427",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "## [**NiN模型**]\n",
    "\n",
    "最初的NiN网络是在AlexNet后不久提出的，显然从中得到了一些启示。\n",
    "NiN使用窗口形状为$11\\times 11$、$5\\times 5$和$3\\times 3$的卷积层，输出通道数量与AlexNet中的相同。\n",
    "每个NiN块后有一个最大汇聚层，汇聚窗口形状为$3\\times 3$，步幅为2。\n",
    "\n",
    "NiN和AlexNet之间的一个显著区别是NiN完全取消了全连接层。\n",
    "相反，NiN使用一个NiN块，其输出通道数等于标签类别的数量。最后放一个*全局平均汇聚层*（global average pooling layer），生成一个对数几率\t（logits）。NiN设计的一个优点是，它显著减少了模型所需参数的数量。然而，在实践中，这种设计有时会增加训练模型的时间。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba4ca30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:19:52.246912Z",
     "iopub.status.busy": "2023-08-18T07:19:52.246237Z",
     "iopub.status.idle": "2023-08-18T07:19:52.294688Z",
     "shell.execute_reply": "2023-08-18T07:19:52.293840Z"
    },
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nin_block(1, 96, kernel_size=11, strides=4, padding=0),\n",
    "    nn.MaxPool2d(3, stride=2),\n",
    "    nin_block(96, 256, kernel_size=5, strides=1, padding=2),\n",
    "    nn.MaxPool2d(3, stride=2),\n",
    "    nin_block(256, 384, kernel_size=3, strides=1, padding=1),\n",
    "    nn.MaxPool2d(3, stride=2),\n",
    "    nn.Dropout(0.5),\n",
    "    # 标签类别数是10\n",
    "    nin_block(384, 10, kernel_size=3, strides=1, padding=1),\n",
    "    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    # 将四维的输出转成二维的输出，其形状为(批量大小,10)\n",
    "    nn.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c9e9ac",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "我们创建一个数据样本来[**查看每个块的输出形状**]。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2917eb46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:19:52.298946Z",
     "iopub.status.busy": "2023-08-18T07:19:52.298371Z",
     "iopub.status.idle": "2023-08-18T07:19:52.325792Z",
     "shell.execute_reply": "2023-08-18T07:19:52.324591Z"
    },
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 96, 54, 54])\n",
      "MaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\n",
      "Sequential output shape:\t torch.Size([1, 256, 26, 26])\n",
      "MaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\n",
      "Sequential output shape:\t torch.Size([1, 384, 12, 12])\n",
      "MaxPool2d output shape:\t torch.Size([1, 384, 5, 5])\n",
      "Dropout output shape:\t torch.Size([1, 384, 5, 5])\n",
      "Sequential output shape:\t torch.Size([1, 10, 5, 5])\n",
      "AdaptiveAvgPool2d output shape:\t torch.Size([1, 10, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 1, 224, 224))\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a4c245",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "## [**训练模型**]\n",
    "\n",
    "和以前一样，我们使用Fashion-MNIST来训练模型。训练NiN与训练AlexNet、VGG时相似。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25caafad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "def get_dataloader_workers():  #@save\n",
    "    \"\"\"使用4个进程来读取数据\"\"\"\n",
    "    return 4\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None):  #@save\n",
    "    \"\"\"下载Fashion-MNIST数据集，然后将其加载到内存中\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=True, transform=trans, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                            num_workers=get_dataloader_workers()),\n",
    "            data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "                            num_workers=get_dataloader_workers()))\n",
    "\n",
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "\n",
    "class Accumulator:  #@save\n",
    "    \"\"\"在n个变量上累加\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def accuracy(y_hat, y):  #@save\n",
    "    \"\"\"计算预测正确的数量\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "\n",
    "def evaluate_accuracy_gpu(net, data_iter, device=None): #@save\n",
    "    \"\"\"使用GPU计算模型在数据集上的精度\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()  # 设置为评估模式\n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    # 正确预测的数量，总预测的数量\n",
    "    metric = Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(X, list):\n",
    "                # BERT微调所需的（之后将介绍）\n",
    "                X = [x.to(device) for x in X]\n",
    "            else:\n",
    "                X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            metric.add(accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "class Animator:  #@save\n",
    "    \"\"\"在动画中绘制数据\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        # 增量地绘制多条线\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        \n",
    "        # 替换 d2l.use_svg_display()\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.rcParams['figure.figsize'] = figsize\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # 替换 d2l.set_axes，使用 lambda 函数直接配置参数\n",
    "        self.config_axes = lambda: self._set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def _set_axes(self, ax, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "        \"\"\"设置坐标轴标签、范围和比例\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        # 设置坐标轴标签\n",
    "        if xlabel:\n",
    "            ax.set_xlabel(xlabel)\n",
    "        if ylabel:\n",
    "            ax.set_ylabel(ylabel)\n",
    "        # 设置坐标轴范围\n",
    "        if xlim:\n",
    "            ax.set_xlim(xlim)\n",
    "        if ylim:\n",
    "            ax.set_ylim(ylim)\n",
    "        # 设置坐标轴比例\n",
    "        if xscale:\n",
    "            ax.set_xscale(xscale)\n",
    "        if yscale:\n",
    "            ax.set_yscale(yscale)\n",
    "        # 设置图例\n",
    "        if legend:\n",
    "            ax.legend(legend)\n",
    "        # 添加网格\n",
    "        ax.grid(True)\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # 向图表中添加多个数据点\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "class Timer:  #@save\n",
    "    \"\"\"记录多次运行时间\"\"\"\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"启动计时器\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"停止计时器并将时间记录在列表中\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"返回平均时间\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"返回时间总和\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"返回累计时间\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()\n",
    "\n",
    "#@save\n",
    "def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):\n",
    "    \"\"\"用GPU训练模型(在第六章定义)\"\"\"\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                            legend=['train loss', 'train acc', 'test acc'])\n",
    "    timer, num_batches = Timer(), len(train_iter)\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练损失之和，训练准确率之和，样本数\n",
    "        metric = Accumulator(3)\n",
    "        net.train()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            optimizer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l * X.shape[0], accuracy(y_hat, y), X.shape[0])\n",
    "            timer.stop()\n",
    "            train_l = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (train_l, train_acc, None))\n",
    "        test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "        animator.add(epoch + 1, (None, None, test_acc))\n",
    "    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
    "          f'test acc {test_acc:.3f}')\n",
    "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n",
    "          f'on {str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c362e21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:19:52.329626Z",
     "iopub.status.busy": "2023-08-18T07:19:52.329083Z",
     "iopub.status.idle": "2023-08-18T07:24:13.568006Z",
     "shell.execute_reply": "2023-08-18T07:24:13.566948Z"
    },
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m lr, num_epochs, batch_size = \u001b[32m0.1\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m128\u001b[39m\n\u001b[32m      2\u001b[39m train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=\u001b[32m224\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtrain_ch6\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 191\u001b[39m, in \u001b[36mtrain_ch6\u001b[39m\u001b[34m(net, train_iter, test_iter, num_epochs, lr, device)\u001b[39m\n\u001b[32m    189\u001b[39m optimizer.zero_grad()\n\u001b[32m    190\u001b[39m X, y = X.to(device), y.to(device)\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m y_hat = \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m l = loss(y_hat, y)\n\u001b[32m    193\u001b[39m l.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\d2l-pytorch-notes\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\d2l-pytorch-notes\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\d2l-pytorch-notes\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\d2l-pytorch-notes\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\d2l-pytorch-notes\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\d2l-pytorch-notes\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\d2l-pytorch-notes\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\d2l-pytorch-notes\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\d2l-pytorch-notes\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\d2l-pytorch-notes\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr, num_epochs, batch_size = 0.1, 10, 128\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)\n",
    "train_ch6(net, train_iter, test_iter, num_epochs, lr, try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847261ce",
   "metadata": {
    "origin_pos": 17
   },
   "source": [
    "## 小结\n",
    "\n",
    "* NiN使用由一个卷积层和多个$1\\times 1$卷积层组成的块。该块可以在卷积神经网络中使用，以允许更多的每像素非线性。\n",
    "* NiN去除了容易造成过拟合的全连接层，将它们替换为全局平均汇聚层（即在所有位置上进行求和）。该汇聚层通道数量为所需的输出数量（例如，Fashion-MNIST的输出为10）。\n",
    "* 移除全连接层可减少过拟合，同时显著减少NiN的参数。\n",
    "* NiN的设计影响了许多后续卷积神经网络的设计。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 调整NiN的超参数，以提高分类准确性。\n",
    "1. 为什么NiN块中有两个$1\\times 1$卷积层？删除其中一个，然后观察和分析实验现象。\n",
    "1. 计算NiN的资源使用情况。\n",
    "    1. 参数的数量是多少？\n",
    "    1. 计算量是多少？\n",
    "    1. 训练期间需要多少显存？\n",
    "    1. 预测期间需要多少显存？\n",
    "1. 一次性直接将$384 \\times 5 \\times 5$的表示缩减为$10 \\times 5 \\times 5$的表示，会存在哪些问题？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288616e8",
   "metadata": {
    "origin_pos": 19,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/1869)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
